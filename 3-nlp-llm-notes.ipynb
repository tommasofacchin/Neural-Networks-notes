{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ed2de2",
   "metadata": {
    "papermill": {
     "duration": 0.003703,
     "end_time": "2025-11-26T16:33:39.787458",
     "exception": false,
     "start_time": "2025-11-26T16:33:39.783755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Natural Language Processing (NLP) and Large Language Models (LLMs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601a0e9",
   "metadata": {
    "papermill": {
     "duration": 0.002448,
     "end_time": "2025-11-26T16:33:39.793176",
     "exception": false,
     "start_time": "2025-11-26T16:33:39.790728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **NLP**\n",
    "\n",
    "*Natural Language Processing (NLP)* is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language in a meaningful way. It combines linguistics, computer science, and machine learning to connect human communication and digital systems.  \n",
    "\n",
    "At its core, NLP allows machines to process text and speech just as humans do‚Äîanalyzing syntax, semantics, and context to extract useful information or produce coherent responses. Applications of NLP range from simple spell checkers and chatbots to complex systems such as automatic translation, sentiment analysis, and large language models that can reason, summarize, and converse naturally.  \n",
    "\n",
    "In essence, NLP is what allows computers to *read*, *listen*, *speak*, and *understand* human language‚Äîturning unstructured linguistic data into structured, actionable knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "## **History and Evolution**\n",
    "\n",
    "The history of NLP is deeply intertwined with the broader development of artificial intelligence.  \n",
    "It began in the 1950s, when researchers first attempted to make machines understand language through symbolic rules and logic. One of the earliest milestones was the **Georgetown‚ÄìIBM experiment (1954)**, which demonstrated automatic translation of Russian sentences into English. Although limited, it marked the start of computational linguistics.\n",
    "\n",
    "During the **1960s and 1970s**, systems such as **ELIZA** and **SHRDLU** explored rule-based interactions with users. These programs relied heavily on handcrafted linguistic rules, which worked only within narrow, predefined domains. While impressive at the time, they lacked scalability and real understanding.\n",
    "\n",
    "The **1980s and 1990s** introduced the statistical revolution: researchers began using probability and data-driven methods instead of fixed rules. Large text corpora and statistical models (like Hidden Markov Models) enabled tasks such as speech recognition and part-of-speech tagging to become far more robust and adaptable.\n",
    "\n",
    "In the **2000s**, the field shifted again toward **machine learning**, using algorithms such as support vector machines and conditional random fields. This era also saw the rise of NLP toolkits like **NLTK** and **spaCy**, which made NLP research and application more accessible.\n",
    "\n",
    "The **2010s** marked a turning point with the rise of **deep learning**. Neural networks‚Äîespecially recurrent architectures such as **LSTMs** and later **Transformers**‚Äîdramatically improved performance in translation, summarization, and text generation. The introduction of **word embeddings** (Word2Vec, GloVe) and **contextual models** (ELMo, BERT, GPT) brought NLP closer to human-level language understanding.\n",
    "\n",
    "Today, NLP has evolved into a foundation of modern AI. With the emergence of **Large Language Models (LLMs)** trained on massive datasets, NLP systems can not only process language but also reason, write, and interact in ways that resemble human communication. The journey from handcrafted rules to self-learning models reflects the broader evolution of AI‚Äîfrom explicit instruction to emergent intelligence.\n",
    "\n",
    "---\n",
    "\n",
    "## **NLP Tasks**\n",
    "\n",
    "Natural Language Processing includes a wide range of tasks that allow computers to understand and produce human language. Each task focuses on a specific aspect of language, from identifying emotions in text to generating complete sentences. Below are some of the main NLP tasks used in research and real-world applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **Text Classification**\n",
    "\n",
    "Text classification is about assigning predefined categories or labels to text.  \n",
    "It can be used for spam detection in emails, topic identification in articles, or sorting customer feedback into categories.  \n",
    "\n",
    "For example:\n",
    "- \"You won a free iPhone!\" ‚Üí *Spam*\n",
    "- \"Meeting at 10 AM tomorrow\" ‚Üí *Work*\n",
    "\n",
    "Machine learning models learn to recognize patterns in words and phrases to make these predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sentiment Analysis**\n",
    "\n",
    "Sentiment analysis determines whether a piece of text expresses a positive, negative, or neutral opinion.  \n",
    "It‚Äôs widely used in social media monitoring, brand analysis, and customer reviews.  \n",
    "\n",
    "Example:\n",
    "- \"This phone is amazing!\" ‚Üí *Positive*\n",
    "- \"Battery life is terrible.\" ‚Üí *Negative*\n",
    "\n",
    "It helps companies understand public opinion and emotional tone in large volumes of text.\n",
    "\n",
    "---\n",
    "\n",
    "### **Named Entity Recognition (NER)**\n",
    "\n",
    "Named Entity Recognition identifies and classifies key information in text such as names of people, organizations, locations, dates, or products.  \n",
    "\n",
    "Example:\n",
    "> \"Apple released the iPhone in California in 2007.\"  \n",
    "Here, **Apple** ‚Üí *Organization*, **iPhone** ‚Üí *Product*, **California** ‚Üí *Location*, **2007** ‚Üí *Date*.\n",
    "\n",
    "NER is essential for information extraction, question answering, and search engines.\n",
    "\n",
    "---\n",
    "\n",
    "### **Part-of-Speech Tagging**\n",
    "\n",
    "Part-of-Speech (POS) tagging assigns grammatical roles to each word in a sentence, such as noun, verb, adjective, or adverb.  \n",
    "\n",
    "Example:\n",
    "> \"The cat sleeps on the mat.\"  \n",
    "Tags might be: *The (det)*, *cat (noun)*, *sleeps (verb)*, *on (prep)*, *the (det)*, *mat (noun)*.\n",
    "\n",
    "POS tagging helps systems understand the structure and meaning of sentences.\n",
    "\n",
    "---\n",
    "\n",
    "### **Machine Translation**\n",
    "\n",
    "Machine translation automatically converts text from one language to another.  \n",
    "Modern translation systems like Google Translate use deep learning models that consider not only individual words but also context and sentence structure.  \n",
    "\n",
    "Example:\n",
    "> English: ‚ÄúGood morning.‚Äù  \n",
    "> French: ‚ÄúBonjour.‚Äù\n",
    "\n",
    "Today‚Äôs models, such as those based on Transformers, produce much more natural and accurate translations than older rule-based systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Text Summarization**\n",
    "\n",
    "Text summarization condenses long passages into shorter versions while keeping the main ideas intact.  \n",
    "There are two main types:\n",
    "- **Extractive summarization**: selects the most important sentences from the text.\n",
    "- **Abstractive summarization**: generates new sentences that capture the key meaning.\n",
    "\n",
    "This task is useful for news aggregation, legal document review, and research papers.\n",
    "\n",
    "---\n",
    "\n",
    "### **Question Answering**\n",
    "\n",
    "Question Answering (QA) systems take a question in natural language and try to find the most accurate answer from a text or database.  \n",
    "\n",
    "Example:\n",
    "> Question: \"Who wrote *Pride and Prejudice*?\"  \n",
    "> Answer: \"Jane Austen.\"\n",
    "\n",
    "Modern QA models combine reading comprehension and retrieval abilities, often using large language models like BERT or GPT.\n",
    "\n",
    "---\n",
    "\n",
    "### **Text Generation**\n",
    "\n",
    "Text generation focuses on producing coherent and meaningful text automatically.  \n",
    "It can be used for story writing, chatbots, email drafting, or code generation.  \n",
    "\n",
    "The model predicts the next word based on previous ones, gradually forming complete sentences and paragraphs.  \n",
    "Controlling parameters like *temperature*, *top-k*, and *top-p* allows adjustment of creativity and randomness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba3709",
   "metadata": {
    "papermill": {
     "duration": 0.002722,
     "end_time": "2025-11-26T16:33:39.798659",
     "exception": false,
     "start_time": "2025-11-26T16:33:39.795937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Core Concepts in NLP**\n",
    "\n",
    "Natural Language Processing (NLP) enables machines to process and generate human language. Several core concepts form the backbone of modern NLP systems.\n",
    "\n",
    "---\n",
    "\n",
    "## **Tokenization**\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units called **tokens**. These tokens can be words, subwords, or characters. The choice of tokenization affects how models handle rare or unknown words, vocabulary size, and training efficiency.\n",
    "\n",
    "- **Word-level:**  \n",
    "  Each word separated by whitespace or punctuation becomes a token.  \n",
    "  Example:  \n",
    "  `\"I bought a new laptop\"` ‚Üí `[\"I\", \"bought\", \"a\", \"new\", \"laptop\"]`  \n",
    "\n",
    "  **Pros:** simple.  \n",
    "  **Cons:** cannot handle rare or unseen words. For instance, `\"Chromebook\"` may be out-of-vocabulary if not present in the training data.\n",
    "\n",
    "\n",
    "- **Subword-level:**  \n",
    "  Words are broken into smaller units (subwords). This helps handle rare words while keeping the vocabulary size manageable. Two widely used algorithms are **WordPiece** and **Byte Pair Encoding (BPE)**.\n",
    "\n",
    "  1. **WordPiece:**  \n",
    "     - Starts with a base vocabulary of single characters or common words.  \n",
    "     - Iteratively builds subwords by selecting splits that maximize the likelihood of the training data.  \n",
    "     - Unknown words are broken into known subwords to avoid `<unk>` tokens.  \n",
    "     - Example:  \n",
    "       `\"unhappiness\"` ‚Üí `[\"un\", \"##happiness\"]`  \n",
    "       `\"##\"` indicates the subword continues from the previous token.  \n",
    "     - How it works in practice:  \n",
    "       1. Start with a basic vocabulary (letters and common words).  \n",
    "       2. Scan the text to find the longest sequence of known subwords.  \n",
    "       3. If a word isn‚Äôt fully covered, split it into smaller known pieces.  \n",
    "     - Used in BERT and other transformers.\n",
    "\n",
    "  2. **Byte Pair Encoding (BPE):**  \n",
    "     - Begins with all words split into characters.  \n",
    "     - Repeatedly merges the **most frequent adjacent symbol pairs** to form new subwords until reaching the desired vocabulary size.  \n",
    "     - Example:  \n",
    "       `\"unhappiness\"` ‚Üí `[\"un\", \"hap\", \"pi\", \"ness\"]`  \n",
    "     - How it works step by step:  \n",
    "       1. Count all character pairs in the training corpus.  \n",
    "       2. Merge the pair with the highest frequency into a new token.  \n",
    "       3. Repeat until the vocabulary reaches the target size.\n",
    "     - Often preserve spaces as part of the token. For example, some use a special character like `‚ñÅ` to indicate a space at the start of a token.\n",
    "     - Pros: efficiently handles rare words and maintains a limited vocabulary.  \n",
    "     - Cons: may create subwords that are not full words, making them harder to interpret.\n",
    "       \n",
    "  3. **Unigram:**  \n",
    "     - Starts with a large set of candidate subwords (including characters, common words, and subwords).  \n",
    "     - Uses a probabilistic model to select a subset of subwords that maximize the likelihood of the training corpus.  \n",
    "     - Words are tokenized by finding the most likely sequence of subwords according to the model.  \n",
    "     - Example:  \n",
    "       `\"unhappiness\"` ‚Üí `[\"un\", \"hap\", \"piness\"]`  \n",
    "     - How it works step by step:  \n",
    "       1. Initialize a large vocabulary of candidate subwords.  \n",
    "       2. Compute probabilities for each subword based on corpus frequency.  \n",
    "       3. Iteratively remove low-probability subwords to reduce vocabulary size while maintaining high likelihood.  \n",
    "     - Pros: flexible probabilistic splitting, handles rare words well.  \n",
    "     - Cons: training is slightly more complex than BPE and WordPiece. \n",
    "\n",
    "- **Character-level:**  \n",
    "  Each character is treated as a token.  \n",
    "  Example:  \n",
    "  `\"laptop\"` ‚Üí `[\"l\", \"a\", \"p\", \"t\", \"o\", \"p\"]`  \n",
    "\n",
    "  **Pros:** no unknown tokens, works for any language or word.  \n",
    "  **Cons:** sequences are longer and the model must learn meaning from finer-grained units, which can increase training complexity.\n",
    "\n",
    "### Special Tokens\n",
    "\n",
    "Special tokens are predefined tokens added to the vocabulary to handle specific purposes in NLP models. Common ones include:\n",
    "\n",
    "- `<pad>`: Used to pad sequences to the same length for batching.  \n",
    "- `<unk>`: Represents unknown or out-of-vocabulary words.  \n",
    "- `<sos>` (Start of Sequence): Indicates the start of a sentence or input sequence.  \n",
    "- `<eos>` (End of Sequence): Marks the end of a sentence or output sequence.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Text Segmentation**\n",
    "\n",
    "Text segmentation is the process of dividing a continuous piece of text into meaningful units such as sentences, words, or topics. It is a foundational step in many NLP pipelines because most language tasks rely on well-defined text boundaries.\n",
    "\n",
    "There are several types of segmentation:\n",
    "\n",
    "- **Sentence Segmentation:**  \n",
    "  Splits text into individual sentences. This is crucial for downstream tasks like machine translation, summarization, or sentiment analysis.  \n",
    "  Example:  \n",
    "  > \"Hello! How are you?\" ‚Üí [\"Hello!\", \"How are you?\"]\n",
    "\n",
    "- **Word Segmentation:**  \n",
    "  Especially important for languages that do not use spaces (like Chinese, Japanese, or Thai). The goal is to correctly identify word boundaries within a continuous string of characters.  \n",
    "  Example (Chinese):  \n",
    "  > \"ÊàëÂñúÊ¨¢Â≠¶‰π†‰∫∫Â∑•Êô∫ËÉΩ\" ‚Üí [\"Êàë\", \"ÂñúÊ¨¢\", \"Â≠¶‰π†\", \"‰∫∫Â∑•Êô∫ËÉΩ\"]\n",
    "\n",
    "- **Topic Segmentation:**  \n",
    "  Divides longer documents into sections or paragraphs based on topic shifts or semantic boundaries. This helps in tasks like document summarization, discourse analysis, and information retrieval.  \n",
    "\n",
    "Early text segmentation used *simple rules* or *punctuation marks* to decide where sentences or topics begin and end. These methods were straightforward but could easily fail with abbreviations, numbers, or languages that don‚Äôt use clear punctuation. Modern systems, instead, use *machine learning* and *neural networks*. They learn from large amounts of text how to split sentences, words, or topics more accurately. Models that use contextual embeddings, like BERT, can even detect subtle changes in meaning or topic that simple rules would miss. This makes segmentation more flexible and reliable across different languages and types of text.\n",
    "\n",
    "---\n",
    "\n",
    "## **Bag of Words (BoW)**\n",
    "\n",
    "Before embeddings became popular, one of the simplest ways to represent text was the **Bag of Words (BoW)** model. Despite its simplicity, it is still widely used for basic text processing and classical machine learning models.\n",
    "The Bag of Words model represents a text as a collection of its words, ignoring grammar, word order, and context. Essentially, a sentence or document is treated as a \"bag\" containing all its words. Each unique word in the corpus becomes a feature in a vector, and the value of that feature is usually the count of how many times the word appears in the document.\n",
    "\n",
    "For example, consider these two sentences:\n",
    "\n",
    "1. `\"I love natural language processing\"`  \n",
    "2. `\"Language processing is fun\"`\n",
    "\n",
    "First, we build a vocabulary of all unique words:\n",
    "\\[\"I\", \"love\", \"natural\", \"language\", \"processing\", \"is\", \"fun\"]\n",
    "\n",
    "Then we represent each sentence as a vector of word counts:\n",
    "\n",
    "- Sentence 1 ‚Üí `[1, 1, 1, 1, 1, 0, 0]`  \n",
    "- Sentence 2 ‚Üí `[0, 0, 0, 1, 1, 1, 1]`\n",
    "\n",
    "Here, each position corresponds to a word in the vocabulary, and the number indicates how many times the word appears in that sentence.\n",
    "\n",
    "BoW is simple and easy to implement, works well for small datasets and classical ML algorithms (Naive Bayes, SVM) and the vocabulary can be extended or restricted.\n",
    "\n",
    "Some variants are :\n",
    "- **Binary BoW:** Instead of counts, each entry is 1 if the word appears, 0 otherwise.  \n",
    "- **TF (Term Frequency):** Counts how often a word appears relative to the total number of words in the document.  \n",
    "- **TF-IDF (Term Frequency‚ÄìInverse Document Frequency):** Adjusts word counts based on how common the word is across all documents. Rare words get higher weight, common words get lower weight. This helps highlight words that are more informative.\n",
    "\n",
    "BoW also have some limitations :\n",
    "- Ignores word order, syntax, and context. `\"I love NLP\"` and `\"NLP love I\"` are treated the same.  \n",
    "- High-dimensional sparse vectors for large vocabularies.  \n",
    "- Cannot capture semantic similarity between words (e.g., `\"car\"` vs `\"automobile\"` are unrelated in BoW).\n",
    "\n",
    "---\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "Word embeddings are a way to represent words as dense numerical vectors in a continuous space. Unlike one-hot encoding, which represents each word as a sparse vector, embeddings allow words to have meaningful relationships. Words that are semantically or syntactically similar will be close together, while unrelated words will be farther apart. This helps models understand language patterns more effectively.\n",
    "\n",
    "*Static embeddings* assign a fixed vector to each word, regardless of its context.\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "Word2Vec is one of the most popular methods for generating static embeddings. It has two main approaches:\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words):** predicts a target word from surrounding words.\n",
    "2. **Skip-gram:** predicts surrounding words given a target word.\n",
    "\n",
    "The intuition is simple: words that appear in similar contexts tend to have similar meanings. For example, \"king\" and \"queen\" often appear near words like \"royal\", \"palace\", or \"monarch\", so their vectors are close in the embedding space.\n",
    "\n",
    "Word2Vec also captures semantic relationships. Vector arithmetic can show analogies, such as the relationship between gender or hierarchy, by adding and subtracting word vectors.\n",
    "\n",
    "### GloVe\n",
    "\n",
    "GloVe (Global Vectors) creates embeddings using co-occurrence statistics from a large corpus. Instead of only looking at nearby words like Word2Vec, GloVe considers how often words appear together across the entire dataset. This produces embeddings that capture both syntactic and semantic relationships.\n",
    "\n",
    "### Measuring Similarity Between Embeddings\n",
    "\n",
    "Once words are represented as vectors, we often want to know how similar or related they are. The most common method is **cosine similarity**:\n",
    "\n",
    "$$\n",
    "d(u, v) = \\frac{u \\cdot v}{||u|| \\, ||v||}\n",
    "$$\n",
    "\n",
    "Cosine similarity measures the angle between two vectors. Vectors pointing in the same direction have a similarity close to 1, vectors at right angles have similarity near 0, and vectors pointing in opposite directions have similarity -1. This works well for high-dimensional embeddings because it focuses on direction rather than magnitude.\n",
    "\n",
    "Other similarity or distance measures include:\n",
    "\n",
    "- **Euclidean distance:**  \n",
    "  Measures the straight-line distance between two vectors in space. Smaller distances mean more similar words. Useful for clustering or nearest-neighbor searches:\n",
    "  $$\n",
    "  d(u, v) = ||u - v||_2\n",
    "  $$\n",
    "\n",
    "- **Manhattan distance (L1 norm):**  \n",
    "  Sum of absolute differences between vector components. Sometimes preferred for sparse or high-dimensional data:\n",
    "  $$\n",
    "  d(u, v) = \\sum_i |u_i - v_i|\n",
    "  $$\n",
    "  \n",
    "- **Dot product:**  \n",
    "  Measures how much two vectors point in the same general direction. Often used in attention mechanisms and neural networks:\n",
    "  $$\n",
    "  u \\cdot v = \\sum_i u_i v_i\n",
    "  $$\n",
    "\n",
    "*Cosine* and *dot product* similarity can be used to compute *numerical similarity*, embeddings that are *numerically similar* are also *semantically similar*.\n",
    "\n",
    "*sentence embeddings* associate every sentence with a vector of numbers, similar sentences are assigned to similar vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## **Language Modeling**\n",
    "\n",
    "Language models predict word sequences, key for text generation, translation, and speech recognition.\n",
    "\n",
    "- **Statistical models:** N-grams estimate probabilities from observed sequences.  \n",
    "- **Neural models:** LSTMs, GRUs, Transformers capture complex patterns beyond simple co-occurrences.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Attention Mechanisms**\n",
    "\n",
    "The attention mechanism is a way for sequence models to dynamically focus on different parts of the input sequence when generating each output token. Instead of treating all input tokens equally, the model learns to assign different weights to different tokens based on their relevance to the current step of the output. This allows the model to capture long-range dependencies and relationships that might be far apart in the sequence, which traditional RNNs often struggle with.\n",
    "\n",
    "In practice, attention computes a weighted combination of all encoder hidden states, where the weights reflect the importance of each input token for generating the current output token. This lets the decoder \"attend\" to the most relevant pieces of information from the input, producing more accurate and contextually coherent outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Encoder-Decoder / Seq2Seq Models**\n",
    "\n",
    "Encoder-decoder architectures handle input and output sequences of different lengths. Common in translation, summarization, and chatbots.\n",
    "\n",
    "**Example:**  \n",
    "- Input: `\"Hello, how are you?\"`  \n",
    "- Output: `\"Ciao, come stai?\"`\n",
    "\n",
    "An **encoder** is a neural network component that functions like a ‚Äúreader‚Äù or ‚Äúcompressor‚Äù of information. It takes the input sequence and transforms it into a dense, numerical representation that captures the meaning, context, and dependencies of the entire sequence. It is not a separate physical object‚Äîit is part of a model, a series of mathematical operations and learned parameters that encode sequences into vectors. The encoder‚Äôs output is often called a **context vector** or a set of hidden states, which summarize all the relevant information from the input in a form that the decoder can understand.\n",
    "The process of updating hidden states at each step can be expressed as:\n",
    "$$\n",
    "h_t = f(W_{hh} h_{t-1} + W_{xh} x_t)\n",
    "$$\n",
    "where:  \n",
    "  - $h_t$ = hidden state at time $t$  \n",
    "  - $x_t$ = input at time $t$  \n",
    "  - $f$ = activation function (tanh or ReLU)  \n",
    "\n",
    "A **decoder** is the complementary neural network component that functions like a ‚Äúwriter‚Äù or ‚Äútranslator.‚Äù It receives the encoded representation from the encoder and generates the output sequence step by step. At each step, the decoder uses the current hidden state, the previous token, and the encoder‚Äôs context to decide the next token. The decoder is effectively a conditional generator: it produces outputs conditioned on the information encoded by the encoder, allowing it to handle sequences of different lengths and to produce coherent text that corresponds to the input.\n",
    "The generation process can be expressed as:\n",
    "$$\n",
    "y_t = g(W_{hy} h_t)\n",
    "$$\n",
    "where:  \n",
    "  - $y_t$ = output at time $t$  \n",
    "  - $h_t$ = decoder hidden state at time $t$  \n",
    "  - $g$ = output activation function  \n",
    "\n",
    "**Implementation:**  \n",
    "Usually RNNs, LSTMs, or GRUs. Teacher forcing feeds correct previous tokens during training. Input/output vectors may differ in size.\n",
    "\n",
    "**Tokenization for Seq2Seq:**  \n",
    "Tokenization is the process of converting text into a format that a sequence-to-sequence model can understand. The input text is first split into smaller units, called tokens, which can be words, subwords, or characters depending on the tokenizer used. Each token is then mapped to a unique numerical ID according to a predefined vocabulary. Special tokens, such as the start-of-sequence (SOS) and end-of-sequence (EOS), are added to indicate the beginning and the end of the sequence. This numerical representation allows the encoder-decoder model to process the text efficiently and generate coherent output sequences.\n",
    "\n",
    "Example: `\"Transformers improve summarization.\"` ‚Üí `[\"transformers\", \"improve\", \"summarization\", \".\"]` ‚Üí `[201, 57, 1342, 4]`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47bdab",
   "metadata": {
    "papermill": {
     "duration": 0.002534,
     "end_time": "2025-11-26T16:33:39.803962",
     "exception": false,
     "start_time": "2025-11-26T16:33:39.801428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Introduction to Large Language Models (LLMs)**\n",
    "\n",
    "## **What Are LLMs?**\n",
    "Large Language Models (LLMs) are a type of AI built to work with human language. Instead of just analyzing text, they can also generate it‚Äîanswering questions, writing paragraphs, translating, summarizing, or even helping with code. They learn by being trained on huge collections of text from books, articles, websites, and many other sources.\n",
    "\n",
    "These models don‚Äôt follow hand-written rules. They learn how language works by seeing millions of real examples. When you ask something, the model predicts one word at a time based on what it has learned, which allows it to produce surprisingly natural and well-structured responses.\n",
    "\n",
    "Modern LLMs can be extremely large, with billions (or even trillions) of parameters. More parameters generally mean a better ability to understand context and produce useful answers. Some well-known examples include: \n",
    "- **GPT-4 (OpenAI)** ‚Äì estimated around 1.7T parameters\n",
    "- **GPT-3 (OpenAI)** ‚Äì 175B parameters\n",
    "- **LLaMA 2 (Meta)** ‚Äì available in 7B, 13B, and 70B parameter versions\n",
    "\n",
    "Today, LLMs are at the core of many everyday tools‚Äîchatbots, writing assistants, customer support systems, and coding helpers‚Äîmaking them one of the most important developments in generative AI.\n",
    "\n",
    "---\n",
    "\n",
    "## **How LLMs Work**\n",
    "\n",
    "Large Language Models (LLMs) process and generate text by predicting one token at a time based on the context provided. They are trained on massive datasets containing diverse text sources, allowing them to learn patterns, grammar, semantics, and even some reasoning abilities. \n",
    "\n",
    "The process works roughly as follows:\n",
    "\n",
    "1. **Input Encoding**: The input text is first tokenized into smaller units (words, subwords, or characters) and converted into numerical embeddings that the model can process.  \n",
    "\n",
    "2. **Contextual Processing**: These embeddings are passed through multiple layers of a neural network (typically a Transformer), where the model captures relationships between tokens and builds a contextual representation of the input.  \n",
    "\n",
    "3. **Prediction**: For each position in the input sequence, the model predicts the most probable next token, using the learned probability distributions over the vocabulary.  \n",
    "\n",
    "4. **Output Generation**: Tokens are generated sequentially, with each new token fed back into the model to inform the next prediction. This continues until a stopping criterion is met (e.g., end-of-sequence token or maximum length).  \n",
    "\n",
    "5. **Optional Fine-tuning**: LLMs can be further fine-tuned on specific tasks or datasets to improve performance on domain-specific language understanding or generation.\n",
    "\n",
    "Through this token-by-token prediction mechanism and deep contextual understanding, LLMs are able to produce coherent and contextually relevant text across a wide range of tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **Transformer Architecture**\n",
    "\n",
    "The Transformer architecture, introduced in 2017, fundamentally changed how natural language processing works. Unlike earlier models like RNNs or LSTMs, which process text sequentially, Transformers can look at an entire sequence of words at once. This allows them to capture long-range dependencies and complex relationships between words more effectively, and to do so much faster during training.\n",
    "\n",
    "Here‚Äôs how a Transformer works in more detail:\n",
    "\n",
    "**Input Embeddings and Positional Encoding**  \n",
    "Every word (or subword) in a sentence is first converted into a numerical vector called an embedding, which represents the word‚Äôs meaning in a high-dimensional space. Since Transformers process all tokens simultaneously, the model has no inherent sense of word order. To solve this, positional encodings are added to the embeddings, giving the model information about the position of each word in the sequence.\n",
    "\n",
    "**Self-Attention Mechanism**  \n",
    "The self-attention mechanism is the core idea of the Transformer. For each word in the input, the model calculates attention scores with every other word in the sequence. This tells the model which words are most relevant for understanding the current word. For example, in the sentence ‚ÄúThe cat sat on the mat,‚Äù when processing ‚Äúsat,‚Äù the model can pay attention to ‚Äúcat‚Äù to understand what is performing the action.\n",
    "\n",
    "**Multi-Head Attention**  \n",
    "Instead of computing a single set of attention scores, the model uses multiple ‚Äúheads,‚Äù each learning different types of relationships. One head might focus on grammatical relationships, another on semantic meaning, and another on contextual patterns. The outputs of all heads are combined to form a richer representation of each word in context.\n",
    "\n",
    "**Feed-Forward Networks**  \n",
    "After attention, each token‚Äôs representation is passed through a small feed-forward neural network. This network applies transformations independently to each token, adding non-linearity and enabling the model to learn more complex patterns.\n",
    "\n",
    "**Residual Connections and Layer Normalization**  \n",
    "To make training deeper networks feasible, residual connections skip layers and add the input back to the output of the layer. Layer normalization ensures that the activations remain stable, preventing problems like vanishing or exploding gradients. Together, these techniques allow very deep Transformer models to train efficiently.\n",
    "\n",
    "**Encoder and Decoder Stacks**  \n",
    "   - The **encoder** reads the input sequence and produces a set of contextual representations for each token.  \n",
    "   - The **decoder** generates output sequences token by token, attending to both the encoder outputs and previously generated tokens.  \n",
    "   - In models focused on understanding rather than generation, only the encoder may be used (as in BERT). In generative models (like GPT), the decoder stack predicts the next token based on prior tokens.\n",
    "\n",
    "**Scalability and Parallelization**  \n",
    "Because the Transformer processes all tokens at once, it can take advantage of modern GPU hardware for parallel computation. This is one reason why Transformers can be scaled to billions of parameters, making them the backbone of today‚Äôs large language models.\n",
    "\n",
    "---\n",
    "\n",
    "## **Pretraining vs Fine-tuning**\n",
    "\n",
    "Large Language Models (LLMs) rely on a two-step process to achieve their impressive performance: **pretraining** and **fine-tuning**. Each step serves a distinct purpose in building the model‚Äôs ability to understand and generate language.\n",
    "\n",
    "### **1. Pretraining**\n",
    "Pretraining is the initial phase where the model learns general patterns of language from a massive dataset. This dataset typically includes books, articles, websites, and other forms of text. The model is trained in a self-supervised way, meaning it learns from the data itself without needing labeled examples. Common pretraining tasks include:\n",
    "\n",
    "- **Language Modeling**: The model predicts the next word in a sentence given the previous words. For example, in ‚ÄúThe cat sat on the ___,‚Äù it predicts ‚Äúmat.‚Äù  \n",
    "- **Masked Language Modeling**: Some words in a sentence are masked, and the model learns to predict them based on the surrounding context. This is used in models like BERT.  \n",
    "- **Next Sentence Prediction / Sentence Order Prediction**: The model learns relationships between sentences to understand context over longer spans of text.\n",
    "\n",
    "Pretraining equips the model with a broad understanding of grammar, syntax, semantics, and even some world knowledge. However, the model at this stage is general-purpose‚Äîit knows how language works but isn‚Äôt specialized for any specific task.\n",
    "\n",
    "### **2. Fine-tuning**\n",
    "Fine-tuning is the second phase, where the pretrained model is adapted to a specific task or domain. This involves training the model on a smaller, task-specific dataset. For example:\n",
    "\n",
    "- **Sentiment Analysis**: Fine-tuning the model to classify reviews as positive or negative.  \n",
    "- **Question Answering**: Adapting the model to select correct answers given a context.  \n",
    "- **Code Generation**: Specializing the model on programming datasets to generate functional code.\n",
    "\n",
    "Fine-tuning can be done in several ways:\n",
    "\n",
    "- **Full Fine-tuning**: Updating all the model‚Äôs parameters. This can be computationally expensive for very large models.  \n",
    "- **Parameter-Efficient Fine-tuning**: Techniques like adapters, LoRA, or prefix tuning update only a small subset of parameters, reducing cost and memory usage.  \n",
    "- **Reinforcement Learning from Human Feedback (RLHF)**: The model is fine-tuned to align with human preferences, improving outputs in chatbots or assistant-like applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **Zero-shot, One-shot, Few-shot Learning**\n",
    "\n",
    "Large Language Models (LLMs) are capable of performing tasks without traditional task-specific training, thanks to their ability to generalize from language patterns learned during pretraining. This ability is often described using **zero-shot, one-shot, and few-shot learning**.\n",
    "\n",
    "### **Zero-shot Learning**\n",
    "In zero-shot learning, the model is asked to perform a task **without seeing any examples**. The instructions are provided in natural language, and the model relies solely on its pretrained knowledge to generate a response.  \n",
    "\n",
    "**Example:**  \n",
    "Prompt: ‚ÄúTranslate the following sentence to French: ‚ÄòThe weather is nice today.‚Äô‚Äù  \n",
    "Even if the model has never been specifically fine-tuned on translation for this sentence, it can produce: ‚ÄúIl fait beau aujourd‚Äôhui.‚Äù\n",
    "\n",
    "Zero-shot learning demonstrates the model‚Äôs ability to generalize and understand instructions directly from the prompt.\n",
    "\n",
    "### **One-shot Learning**\n",
    "In one-shot learning, the model is given **a single example** to understand the task before producing an output for a new input. This helps the model infer the expected format or style.  \n",
    "\n",
    "**Example:**  \n",
    "Prompt:  \n",
    "- Example: ‚ÄúHello ‚Üí Bonjour‚Äù  \n",
    "- New input: ‚ÄúGood morning ‚Üí ?‚Äù  \n",
    "The model can output: ‚ÄúBonjour‚Äù or ‚ÄúBon matin,‚Äù depending on context.\n",
    "\n",
    "### **Few-shot Learning**\n",
    "Few-shot learning provides the model with **a small number of examples** to improve its understanding of the task. This is particularly useful when the task requires a specific format or when the model struggles with zero-shot instructions.  \n",
    "\n",
    "**Example:**  \n",
    "Prompt:  \n",
    "- Example 1: ‚ÄúDog ‚Üí Chien‚Äù  \n",
    "- Example 2: ‚ÄúCat ‚Üí Chat‚Äù  \n",
    "- New input: ‚ÄúBird ‚Üí ?‚Äù  \n",
    "The model can correctly output: ‚ÄúOiseau.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# **Challenges and Limitations**\n",
    "\n",
    "While Large Language Models (LLMs) and NLP technologies have made incredible advances, they are not without significant challenges. Understanding these limitations is crucial for anyone working with these systems, whether in research, industry, or product development.\n",
    "\n",
    "## **Tokenization Limitations**\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units‚Äîtokens‚Äîthat a model can understand. While it seems straightforward, it comes with several challenges:\n",
    "\n",
    "1. **Subword vs Word-level Tokenization**  \n",
    "   - Early NLP models often used word-level tokenization, but this struggled with rare or unseen words (out-of-vocabulary, OOV).  \n",
    "   - Modern LLMs use subword tokenization (like BPE or SentencePiece), which can break uncommon words into smaller, known pieces.  \n",
    "   - **Example:** The word ‚Äúunbelievably‚Äù might be split into ‚Äúun‚Äù, ‚Äúbeliev‚Äù, and ‚Äúably.‚Äù While this helps cover rare words, it can sometimes lead to unnatural splits or loss of semantic nuance.\n",
    "\n",
    "2. **Multilingual Challenges**  \n",
    "   - Tokenizers trained on English may poorly handle other languages, especially those with complex scripts like Chinese, Japanese, or Arabic.  \n",
    "   - **Example:** In Japanese, a single character can carry meaning, and tokenizing incorrectly can fragment the meaning.\n",
    "\n",
    "3. **Impact on Downstream Tasks**  \n",
    "   - Poor tokenization can affect sentiment analysis, named entity recognition, or translation.  \n",
    "   - **Example:** If ‚ÄúNew York-based company‚Äù is incorrectly tokenized as separate fragments, the model might fail to recognize it as a single entity.\n",
    "\n",
    "4. **Handling Special Characters and Emojis**  \n",
    "   - Modern text often includes emojis, hashtags, or URLs. These can confuse tokenizers.  \n",
    "   - **Example:** ‚ÄúI love pizza üçï!‚Äù might be split into ‚ÄúI‚Äù, ‚Äúlove‚Äù, ‚Äúpizza‚Äù, ‚ÄúÔøΩ‚Äù, ‚Äú!‚Äù, where the pizza emoji is incorrectly handled.\n",
    "\n",
    "---\n",
    "\n",
    "## **Compute and Energy Costs**\n",
    "\n",
    "Training and deploying LLMs requires enormous computational resources, which raises both practical and ethical concerns:\n",
    "\n",
    "1. **High GPU/TPU Requirements**  \n",
    "   - State-of-the-art models often require hundreds or thousands of GPUs for weeks or months.  \n",
    "   - **Example:** Training GPT-3 reportedly used several thousand petaflop/s-days of compute.\n",
    "\n",
    "2. **Energy Consumption and Carbon Footprint**  \n",
    "   - Large models consume megawatts of electricity.  \n",
    "   - **Example:** A single full training run of GPT-3 could emit as much CO‚ÇÇ as multiple cars over their lifetime. Organizations are now considering renewable energy sources and model efficiency.\n",
    "\n",
    "3. **Inference Costs**  \n",
    "   - Even after training, running LLMs for real-time applications can be expensive.  \n",
    "   - **Example:** Generating long text outputs in a chatbot with GPT-4 can cost significant cloud computing resources.\n",
    "\n",
    "4. **Accessibility**  \n",
    "   - High compute requirements make cutting-edge models available mostly to well-funded companies and institutions, limiting democratization.\n",
    "\n",
    "---\n",
    "\n",
    "## **Security and Misuse Concerns**\n",
    "\n",
    "LLMs are powerful, but this power can be misused in ways that have serious consequences:\n",
    "\n",
    "1. **Misinformation and Hallucinations**  \n",
    "   - LLMs can produce text that is syntactically correct but factually incorrect.  \n",
    "   - **Example:** A model might generate a fake historical fact: ‚ÄúThe Eiffel Tower was completed in 1895‚Äù (correct year is 1889).\n",
    "\n",
    "2. **Malicious Content Generation**  \n",
    "   - Models can be prompted to create phishing emails, spam, malware code, or hate speech.  \n",
    "   - **Example:** An attacker could use an LLM to generate highly convincing fraudulent emails targeting employees of a company.\n",
    "\n",
    "3. **Bias and Discrimination**  \n",
    "   - LLMs reflect biases present in training data.  \n",
    "   - **Example:** A model might associate certain professions disproportionately with one gender or ethnicity, reinforcing stereotypes.\n",
    "\n",
    "4. **Privacy Risks**  \n",
    "   - Models trained on sensitive or personal data may inadvertently generate confidential information.  \n",
    "   - **Example:** A fine-tuned model could output a snippet of someone‚Äôs private email if it appeared in the training data.\n",
    "\n",
    "5. **Prompt Injection and Security Vulnerabilities**  \n",
    "   - Malicious actors can craft inputs that manipulate the model‚Äôs behavior, bypassing safety filters.  \n",
    "   - **Example:** A user could ask a chatbot to ignore safety guidelines and reveal restricted information.\n",
    "\n",
    "6. **Dual-use Concerns**  \n",
    "   - Technologies designed for research or productivity can also be used for harmful purposes.  \n",
    "   - **Example:** Text generation models can be used for legitimate content creation or propaganda campaigns depending on intent.\n",
    "\n",
    "\n",
    "# **Tools and Frameworks**\n",
    "\n",
    "LLMs and NLP models rely on a variety of tools and frameworks to be trained, deployed, and used effectively. These tools range from specialized libraries for text processing to large-scale machine learning frameworks and APIs for accessing prebuilt models.\n",
    "\n",
    "---\n",
    "\n",
    "## **Specialized Libraries (spaCy, NLTK, etc.)**\n",
    "\n",
    "These libraries provide prebuilt functions and pipelines for core NLP tasks such as tokenization, part-of-speech tagging, named entity recognition, and dependency parsing.\n",
    "\n",
    "- **NLTK (Natural Language Toolkit)**: One of the oldest and most comprehensive NLP libraries, useful for text preprocessing, tokenization, stemming, and evaluation of NLP algorithms.  \n",
    "- **spaCy**: Modern, high-performance NLP library designed for production. Offers fast tokenization, word vectors, and pretrained pipelines for tasks like NER, POS tagging, and dependency parsing.  \n",
    "- **Other Libraries**: TextBlob, Gensim, and CoreNLP provide additional functionality for sentiment analysis, topic modeling, and semantic similarity.\n",
    "\n",
    "These libraries are essential for traditional NLP workflows and smaller-scale experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## **TensorFlow and PyTorch**\n",
    "\n",
    "TensorFlow and PyTorch are general-purpose deep learning frameworks that serve as the backbone for training large neural networks, including LLMs.\n",
    "\n",
    "- **TensorFlow**: Developed by Google, it offers strong support for distributed training, deployment on mobile and cloud, and high-level APIs like Keras.  \n",
    "- **PyTorch**: Developed by Facebook/Meta, it is widely used in research for its flexibility and dynamic computation graph, which simplifies experimentation with novel architectures.  \n",
    "\n",
    "Both frameworks provide automatic differentiation, GPU acceleration, and modular design, making them essential for modern NLP and generative AI.\n",
    "\n",
    "---\n",
    "\n",
    "## **Hugging Face Transformers**\n",
    "\n",
    "Hugging Face provides a high-level interface to a vast collection of pretrained Transformer models. Its library allows users to quickly load models like BERT, GPT, RoBERTa, and more for a wide range of NLP tasks.\n",
    "\n",
    "- Supports tokenization, model training, fine-tuning, and inference.  \n",
    "- Offers pipelines for classification, question answering, text generation, summarization, and translation.  \n",
    "- Integrates easily with TensorFlow and PyTorch, enabling seamless experimentation and deployment.  \n",
    "\n",
    "Hugging Face has democratized access to LLMs, allowing developers and researchers to use state-of-the-art models without training from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "## **OpenAI API**\n",
    "\n",
    "The OpenAI API provides access to pretrained large language models, including GPT-3, GPT-4, and other specialized models, through a simple RESTful interface.\n",
    "\n",
    "- Enables text generation, summarization, translation, code generation, and more.  \n",
    "- Allows developers to integrate LLM capabilities into applications without managing infrastructure or model training.  \n",
    "- Supports advanced features like few-shot prompting, controlling creativity (temperature), and fine-tuning for custom tasks.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638c056",
   "metadata": {
    "papermill": {
     "duration": 0.00244,
     "end_time": "2025-11-26T16:33:39.809146",
     "exception": false,
     "start_time": "2025-11-26T16:33:39.806706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Useful resources\n",
    "\n",
    "### **Youtube**\n",
    "\n",
    "*TensorFlow*: Natural Language Processing (NLP) Zero to Hero\n",
    "https://www.youtube.com/playlist?list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S\n",
    "\n",
    "*Stanford Online*: Stanford CS224N Natural Language Processing (NLP)\n",
    "https://www.youtube.com/playlist?list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S\n",
    "\n",
    "\n",
    "### **Websites**\n",
    "\n",
    "### **Courses**\n",
    "\n",
    "*Oracle*: OCI Generative AI Professional (2025)\n",
    "https://mylearn.oracle.com/ou/learning-path/become-an-oci-generative-ai-professional-2025/147863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9fbf8",
   "metadata": {
    "papermill": {
     "duration": 0.002423,
     "end_time": "2025-11-26T16:33:39.814171",
     "exception": false,
     "start_time": "2025-11-26T16:33:39.811748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.540433,
   "end_time": "2025-11-26T16:33:43.104383",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-26T16:33:34.563950",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
