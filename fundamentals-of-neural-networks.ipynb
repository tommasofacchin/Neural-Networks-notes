{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tommasofacchin/fundamentals-of-neural-networks?scriptVersionId=266197821\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"075c0944","metadata":{"papermill":{"duration":0.005856,"end_time":"2025-10-06T21:09:38.572542","exception":false,"start_time":"2025-10-06T21:09:38.566686","status":"completed"},"tags":[]},"source":["Personal notes from my studies, sources are university lectures, YouTube, books and chatGPT."]},{"cell_type":"markdown","id":"dc835b46","metadata":{"papermill":{"duration":0.004535,"end_time":"2025-10-06T21:09:38.582513","exception":false,"start_time":"2025-10-06T21:09:38.577978","status":"completed"},"tags":[]},"source":["# **NEURAL NETWORK ARCHITECTURE**\n","A **Neural Network** is a machine learning model inspired by the human brain. It consists of layers of interconnected neurons that process and learn from data.\n","<br>Neural networks are typically structured into **three types of layers**.\n","\n","## **1. Input Layer**\n","The **input layer** is where the network receives raw data. It does NOT perform any computations—its only job is to pass the input values to the next layer.\n","<br>The number of input layer is equal to the number of features in the dataset.\n","<br>Example Inputs:\n","* Image Processing: Pixel values from an image\n","* Text Processing: Word embeddings (numerical representation of words)\n","* Tabular Data: Age, salary, temperature, etc.\n","\n","If you're building a neural network for handwritten digit recognition, considering 28×28 pixels the input layer might have 784 neurons.\n","\n","## **2. Hidden Layers**\n","Hidden layers are where the real learning happens, these layers perform transformations using **weights**, biases, and activation functions to extract important patterns.\n","<br>Each neuron in a hidden layer does the following:\n","1. **Weighted Sum Calculation**\n","   * Each neuron receives inputs, multiplies them by weights, and adds a bias term:\n","$$\n","     z=Wx+b\n","$$\n","    * $W$ = Weights, $x$ = Input values, $b$ = Bias\n","2. **Activation function**\n","   * The result, $z$, is passed through an activation function to introduce non-linearity, allowing the network to learn complex patterns.\n","\n","<br>The number of hidden layers and neurons is a key design choice, more hidden layers mean more powerfull network, but also risk of **overfitting**.\n","<br>A deep neural network might have **multiple hidden layers**, where the first learns edges in an image, the next learns textures, and deeper layers recognize objects like faces or digits.\n","\n","## **3. Output Layer**\n","The output layer produces the **final prediction**. It contains neurons equal to the number of possible outputs.\n","<br>If we are classifying digits (0–9), the output layer will have **10 neurons**, each representing the probability of a specific digit.\n","\n","## **Fully Connected (Dense) Layers vs. Convolutional Layers**\n","**Fully Connected (Dense) Layers:**\n","* Every neuron in one layer is connected to **every** neuron in the next layer.\n","* Used in standard **feedforward neural networks (FNNs)**.\n","* Good for structured/tabular data but **inefficient for images** due to large weight matrices.\n","\n","**Convolutional Layers (CNNs):**\n","* Used in **Convolutional Neural Networks (CNNs)** for image processing.\n","* Instead of connecting every neuron, it applies **kernels** that scan small parts of the image to detect edges, textures, and objects.\n","* More efficient for vision tasks.\n","\n","\n"," ## **Kernel**\n","**A kernel** (also called a filter) is a small square matrix used in CNNs to detect patterns such as **edges**, **textures**, and **objects** in images, it slides over an image (or feature map) and performs **convolution** to extract features.\n","* The kernel is applied to small regions of the image at a time.\n","* Multiply each pixel value by the corresponding kernel value.\n","* Sum up all the multiplied values to get a new pixel value.\n","* Move to the next region and repeat the process.\n","\n","If we apply a **blur kernel**, it smoothens the image. If we apply an **edge-detection kernel**, it highlights edges.\n","<br>**CNNs don’t use predefined kernels**, instead, they learn the best kernels during training."]},{"cell_type":"markdown","id":"b81532f3","metadata":{"papermill":{"duration":0.004169,"end_time":"2025-10-06T21:09:38.591249","exception":false,"start_time":"2025-10-06T21:09:38.58708","status":"completed"},"tags":[]},"source":["# **PERCEPTRON & MULTI-LAYER PERCEPTRON (MLP)**\n","A Perceptron is the simplest type of artificial neuron, introduced by Frank Rosenblatt in 1958. It mimics a biological neuron, performing the following steps:\n","1. Takes multiple inputs $x_1,x_2,...,x_n$.\n","2. Each input is multiplied by a **weight** $w_1,w_2,...,w_n$.\n","3. The weighted sum is passed through an **activation function**.\n","4. Produces an **output** (either 0 or 1 for binary classification).\n","$$\n"," z = w_1x_1 + w_2x_2 + ... + w_nx_n + b\n","$$\n","$$ \n"," y=f(z)\n","$$\n","where :\n","* $x_i$ = input\n","* $w_i$ = weight\n","* $b$ = bias\n","* $f(z)$ = activation function (usually a step function in Perceptrons)\n","\n","Works well for **linearly separable** problems (e.g. AND, OR gates), but cannot solve **non linearly separable** problems like **XOR** or hidden layers.\n","\n","## **Multi-Layer Perceptron (MLP)**\n","A **MLP** is a **neural network** with one or more **hidden layers** that perform transformations using weights and activation functions.\n","<br>For each layer:\n","$$\n"," z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}\n","$$\n","$$ \n"," a^{(l)}=f(z^{(l)})\n","$$\n","where:\n","* **$W^{(l)}$** = weights of layer **l**\n","* **$a^{(l)}$** = activations of layer **l**\n","* **$f(z)$** = activation function\n","\n","## **Why is MLP Powerful?**\n","* Solves Non-Linear Problems (e.g., XOR).\n","* Multiple Layers Enable Feature Extraction.\n","* Works for Classification & Regression."]},{"cell_type":"markdown","id":"c2be8a3d","metadata":{"papermill":{"duration":0.004267,"end_time":"2025-10-06T21:09:38.600211","exception":false,"start_time":"2025-10-06T21:09:38.595944","status":"completed"},"tags":[]},"source":["# **ACTIVATION FUNCTIONS**\n","\n","An **activation function** is a mathematical function that determines whether a neuron in a neural network should be activated or not, **Activation Unit** refers to a single neuron in a neural network that applies an activation function to the weighted sum of its inputs.\n","\n","## **Why Are Activation Functions Important?**\n","Neural networks rely on activation functions to:\n","* Introduce **non-linearity** - Without activation functions, deep networks behave like a simple linear model.\n","* Control **neuron outputs** - They decide how much information passes through each neuron.\n","* Help with **backpropagation** - Some functions improve gradient-based learning (e.g., ReLU, sigmoid).\n","\n","## **Types of Activation Functions**\n","* **Threshold Function:** outputs 1 if input is above a threshold, otherwise 0.\n","  <br>Not used because it's not differentiable, making backpropagation impossible, and only works for binary classification.\n","  $$\n","f(x) =\n","\\begin{cases} \n","1 & \\text{if } x \\geq 0 \\\\\n","0 & \\text{if } x < 0\n","\\end{cases}\n","$$\n","* **Sigmoid Function:** Converts inputs into a probability-like value between 0 and 1.\n","  <br>Used in binary classification, can cause **vanishing gradients** in deep networks (very small gradients slow learning).\n","  $$\n","f(x) = \\frac{1}{1 + e^{-x}}\n","$$\n","* **Tanh Function:** Similar to sigmoid, but outputs values between -1 and 1.\n","  <br>it's zero-centered, helping optimization, but still suffers from vanishing gradients.\n","  $$\n","f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n","$$\n","* **ReLU (Rectified Linear Unit):** Outputs x if positive, otherwise 0.\n","  <br>Solves vanishing gradients (gradient is 1 for x>0) and works well in deep networks, but can suffer from dying neurons (if x < 0, gradient = 0).\n","  $$\n","f(x) = \\max(0, x)\n","$$\n","* **Leaky ReLU:** Has a small slope for negative x instead of zero.\n","  $$\n","f(x) =\n","\\begin{cases}\n","x & \\text{if } x > 0 \\\\\n","\\alpha x & \\text{if } x \\leq 0\n","\\end{cases}\n","$$\n","* **Softmax (For Multi-Class Classification):** Converts scores into probabilities summing to 1.\n","  <br>Used in final layer of multi-class classifiers, ensure outputs sum to 1(like probabilities).\n","  $$\n","f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n","$$\n","\n","## **Vanishing Gradient Problem**\n","The vanishing gradient problem occurs when gradients **become too small** during backpropagation, causing **slow or no learning** in deep neural network.\n","\n","**Why does it happen?**\n","<br>During backpropagation, gradients are computed using the **chain rule**, if each layer's gradient is less than 1, mulpitplying many small numbers **shrinks** the gradient exponentially, meaning that **earlier layers stop learning** (since their weight updates are almost zero).\n","\n","**When does it happen?**\n","* **Sigmoid & Tanh Activation Functions:** These functions squash outputs between small ranges (Sigmoid: 0 to 1, Tanh: -1 to 1), their **derivatives are small** for large or small inputs, this results in **tiny gradients**, slowing down learning.\n","* **Deep Networks (Many Layers):** The more layers, the more times we multiply gradients, small gradients **vanish** as they backpropagate through layers.\n","* **Poor Weight Initialization:** If weights are poorly initialized (too small), activations **saturate**, worsening the problem.\n","\n","  \n","**Solutions to Vanishing Gradients**\n","* **Use ReLU Instead of Sigmoid/Tanh:** ReLU has a gradient of 1 for positive inputs, avoiding vanishing gradients. However, ReLU can cause dying neurons (where gradients become zero for negative inputs).\n","* **Batch Normalization (BN):** BN normalizes activations, preventing them from shrinking. Helps stabilize learning and allows higher learning rates.\n","* **Use Advanced Optimizers:** Adam, RMSprop dynamically adjust learning rates. Help gradients stay large enough to continue learning."]},{"cell_type":"markdown","id":"4b10eacf","metadata":{"papermill":{"duration":0.00406,"end_time":"2025-10-06T21:09:38.60864","exception":false,"start_time":"2025-10-06T21:09:38.60458","status":"completed"},"tags":[]},"source":["# **BACKPROPAGATION**\n","Backpropagation is the **core learning algorithm** for neural networks. It efficiently updates weights to minimize the **loss function** by propagating errors **backward** through the network using **gradient descent** and the **chain rule**.\n","\n","## **Why Do We Need Backpropagation?**\n","When training a neural network, we want to find the optimal weights $𝑊$ and biases $b$ that minimize the error (loss function). However, computing how much each weight contributes to the total error is not straightforward in deep networks.\n","\n","- **Forward Pass:** Data flows from input → hidden layers → output.\n","- **Compute Loss:** Compare predicted output to actual output.\n","- **Backward Pass:** Adjust weights proportionally to their impact on the loss using gradients.\n","\n","The goal is to find the gradients of the loss function with respect to each weight efficiently.\n","\n","## **Step-by-Step Backpropagation**\n","**1. Forward Pass (Compute Predictions)**\n","   Each neuron in the network performs:\n","   - **Linear transformation:** Compute weighted sum of inputs:\n","   $$\n","     z=Wx+b\n","   $$\n","* **Apply activation function**\n","   $$\n","     a=f(z)\n","    $$\n","* The process repeats layer by layer until we get the final output $\\hat{y}$.\n","     \n","**2. Compute Loss**\n","<br>The loss function $L$ measures how different the predicted output $\\hat{y}$ is from the actual output $y$.\n","   <br> For example, using **MSE** for regression or **Cross-Entropy Loss** for classification. \n","\n","**3. Backward Pass (Compute Gradients)**\n","<br>To update weights, we need to compute how much each weight contributed to the error. This is done using partial derivatives of the loss function.\n","<br>Backpropagation applies the **chain rule** to compute gradients step by step from output to input.\n","\n","Gradient of Loss for the **output layer**:\n","$$\n","\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n","$$\n","Gradient of Loss for **hidden layers**:\n","$$\n","\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n","$$\n","where:\n","* $\\frac{\\partial L}{\\partial a}$ is the **gradient from the next layer** (recursively computed).\n","* $\\frac{\\partial a}{\\partial z}$ is the **derivative of the activation function**.\n","* $\\frac{\\partial z}{\\partial W} = x$, since $z = Wx + b$.\n","\n","Instead of manually computing gradients, frameworks like **TensorFlow** & **PyTorch** use **automatic differentiation** to handle backprop efficiently.\n","\n","**4. Update Weights (Gradient Descent)**\n","<br>After computing gradients, we update weights using gradient descent:\n","$$\n","W = W - \\eta \\frac{\\partial L}{\\partial W}\n","$$\n","where:\n","* $\\eta$ = learning rate (controls step size).\n","* $\\frac{\\partial L}{\\partial W}$ = gradient of loss w.r.t weights.\n","\n","This process repeats for multiple epochs (an epoch is one complete pass through the entire training dataset) until the loss is minimized.\n","<br>Backpropagation is repeated for multiple epochs, but training can stop early if:\n","* Loss stops decreasing significantly.\n","* Validation loss increases (to prevent overfitting).\n","* A predefined epoch limit is reached."]},{"cell_type":"markdown","id":"38391784","metadata":{"papermill":{"duration":0.004398,"end_time":"2025-10-06T21:09:38.617441","exception":false,"start_time":"2025-10-06T21:09:38.613043","status":"completed"},"tags":[]},"source":["# **GRADIENT DESCENT**\n","Gradient Descent is an optimization algorithm used to minimize a function, typically a **loss function** (a loss function measures the difference between the predicted and actual values) in ML.\n","<br> It does this by iteratively adjusting parameters (weights) in diretions of **steepest descend** (negative gradient).\n","\n","## **Why do we need Gradient Descent**\n","* Neural networks **learn by adjusting weights** to minimize error.\n","* The error is **measured by a loss function** (e.g. MSE for regression or cross-entropy for classification).\n","* Gradient Descent **helps find the optimal weights** that minimize the loss function.\n","\n","## **How it works**\n","Gradient Descent follows this update rule : \n","$$ \\theta = \\theta -\\alpha\\times\\nabla J(\\theta)$$\n","Where:\n","* $\\theta$ : Model parameters (Weights, biases).\n","* $J(\\theta)$ : Loss function.\n","* $\\nabla J(\\theta)$ : Gradient of the loss function.\n","* $\\alpha$ : Learning rate.\n","\n","**Step-by-step Process**\n","1. **Compute the Loss** (Difference between predictions & actual values).\n","2. **Compute the Gradient** (Partial derivatives of loss w.r.t. each parameter).\n","3. **Update Weights** (Move in the negative gradient direction).\n","4. **Repeat until convergence** (or stop when loss stops decreasing).\n","\n","## **Types of Gradient Descent**\n","* **Batch Gradient Descent (BGD)**\n","  <br>Uses **entire dataset** to compute gradients in each step.\n","  <br>More stable updates but **slow for large datasets**. \n","  <br>Used when dataset is small and computational power is high.\n","  \n","* **Stochastic Gradient Descent (SGD)**\n","  <br>Updates **after each data point** instead of the entire dataset.\n","  <br>Faster but more noisy (fluctuations in loss).\n","  <br>Helps escape local minima.\n","\n","  \n","* **Mini-Batch Gradient Descent**\n","  <br>Uses **small batches** of data for updates.\n","  <br>**Balances** between BGD (stable) and SGD (fast).\n","  <br>Most commonly used in deep learning.\n","\n","  \n","## **Learning Rate & Convergence**\n","The learning rate $\\alpha$ determines step size:\n","* **Too High :** May overshoot and never converge.\n","* **Too Low :** Slow converge, stuck in local minima.\n","* **Optimal :** Reaches the minimum efficiently.\n","  \n","<br> **Adaptive methods** (like Adam or RMSprop) dynamically adjest learning rates for better training.\n","\n","## **Types of Loss Functions**\n","1. **Regression Loss Function**\n","   * **Mean Squared Error (MSE):**\n","    $$\n","     MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","    $$\n","     Penalizes large errors more than smaller ones, making it sensitive to outliers.\n","   * **Mean Absolute Error (MAE):**\n","     $$\n","     MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n","     $$\n","     Less sensitive to outliers but may be harder to optimize\n","   * **Huber Loss:**\n","     A combination of MSE and MAE that is robust to outliers.\n","\n","2. **Classification Loss Function**\n","     * **Binary Cross-Entropy (Log Loss)** (For binary classification):\n","    $$\n","     BCE = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n","    $$\n","   * **Categorical Cross-Entropy** (For multi-class classification):\n","     $$\n","     CCE = -\\sum_{i=1}^{n} \\sum_{j=1}^{k} y_{ij} \\log (\\hat{y}_{ij})\n","     $$\n","     Used when output probabilities sum to 1 (Softmax activation).\n","   * **Sparse Categorical Cross-Entropy:** Similar to categorical cross-entropy but used when class labels are integers instead of one-hot encoded vectors."]},{"cell_type":"markdown","id":"1ff9f812","metadata":{"papermill":{"duration":0.00461,"end_time":"2025-10-06T21:09:38.626814","exception":false,"start_time":"2025-10-06T21:09:38.622204","status":"completed"},"tags":[]},"source":["# **ADVANCED OPTIMIZATION ALGORITHMS**\n","\n","Gradient Descent is the foundation for training neural networks, but there are several **advanced optimizers** that improve convergence speed, stability, and performance. These optimizers adapt the learning process to the model and dataset.\n","\n","\n","## **Momentum**\n","\n","Momentum helps accelerate gradient descent by **accumulating past gradients** to build velocity.\n","\n","### How It Works\n","Instead of updating weights purely based on the current gradient, momentum uses a fraction of the previous update:\n","\n","$$\n","v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla_\\theta J(\\theta)\n","$$\n","$$\n","\\theta = \\theta - \\alpha v_t\n","$$\n","\n","Where:  \n","* $v_t$ = velocity (momentum term)  \n","* $\\beta$ = momentum coefficient (usually 0.9)  \n","* $\\alpha$ = learning rate  \n","* $\\nabla_\\theta J(\\theta)$ = current gradient  \n","\n","### Benefits\n","* Accelerates convergence in **consistent gradient directions**.  \n","* Reduces oscillations in **ravines** of the loss surface.  \n","\n","\n","## **Nesterov Accelerated Gradient (NAG)**\n","\n","NAG is a **look-ahead version of momentum**, checking the future position before calculating the gradient.\n","\n","$$\n","v_t = \\beta v_{t-1} + \\alpha \\nabla_\\theta J(\\theta - \\beta v_{t-1})\n","$$\n","$$\n","\\theta = \\theta - v_t\n","$$\n","\n","### Benefits\n","* Provides **faster convergence** than standard momentum.  \n","* Reduces **overshooting** in high-curvature areas.  \n","\n","\n","## **Adagrad**\n","\n","Adagrad adapts the learning rate **per parameter**, scaling it inversely with the sum of squared past gradients.\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} \\nabla_\\theta J(\\theta_t)\n","$$\n","\n","Where:  \n","* $G_t$ = sum of squares of gradients for each parameter  \n","* $\\epsilon$ = small number for numerical stability  \n","\n","### Benefits\n","* Automatically adjusts learning rates.  \n","* Works well for **sparse data**.  \n","\n","### Limitation\n","* Learning rate can **shrink too much** over time, causing slow convergence.\n","\n","\n","## **RMSprop**\n","\n","RMSprop modifies Adagrad to **avoid the diminishing learning rate problem** by using an exponential moving average of squared gradients:\n","\n","$$\n","E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta) (\\nabla_\\theta J(\\theta_t))^2\n","$$\n","$$\n","\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla_\\theta J(\\theta_t)\n","$$\n","\n","### Benefits\n","* Works well for **non-stationary objectives** (like RNNs).  \n","* Popular in deep learning due to **fast convergence**.  \n","\n","\n","## **Adam (Adaptive Moment Estimation)**\n","\n","Adam combines **momentum** and **RMSprop**, tracking both first (mean) and second (variance) moments of gradients.\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_\\theta J(\\theta_t)\n","$$\n","$$\n","v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_\\theta J(\\theta_t))^2\n","$$\n","Bias-corrected estimates:\n","$$\n","\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n","$$\n","Update rule:\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n","$$\n","\n","### Default Hyperparameters\n","* $\\alpha = 0.001$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$\n","\n","### Benefits\n","* Fast convergence.  \n","* Robust to **sparse gradients**.  \n","* Often the **default choice** in deep learning.  \n","\n","\n","## **AdamW (Adam with Weight Decay)**\n","\n","AdamW is a modification of Adam that **decouples weight decay from the gradient update**, improving regularization and generalization.\n","\n","### Motivation\n","In standard Adam, applying L2 regularization is equivalent to **adding weight decay to the gradients**, which interacts poorly with Adam's adaptive learning rates. AdamW separates these two concepts:\n","\n","* Weight decay is applied **directly to the weights**, independent of the gradient moments.\n","* Leads to more **consistent regularization**, especially in deep networks.\n","\n","### Update Rule\n","AdamW modifies the parameter update as follows:\n","\n","1. Compute Adam updates (momentum + RMSprop):\n","$$\n","m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_\\theta J(\\theta_t)\n","$$\n","$$\n","v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_\\theta J(\\theta_t))^2\n","$$\n","$$\n","\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n","$$\n","\n","2. Apply weight decay **directly** to parameters:\n","$$\n","\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\alpha \\lambda \\theta_t\n","$$\n","\n","Where:  \n","* $\\lambda$ = weight decay coefficient  \n","* $\\alpha, \\beta_1, \\beta_2, \\epsilon$ = same as Adam  \n","\n","### Benefits\n","* Improves **generalization** compared to standard Adam with L2 regularization.  \n","* Widely used in **large-scale deep learning** (e.g., Transformers).  \n","* More stable training for very deep networks.  \n","\n","### Notes\n","* Often used with **learning rate schedules** like cosine decay or linear warmup.  \n","* Default hyperparameters are usually similar to Adam: $\\alpha=0.001$, $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=10^{-8}$.\n","\n","\n","## **Adam Variants**\n","\n","* **Adamax**: Uses infinity norm instead of $L_2$ norm, more stable in some cases.  \n","* **Nadam**: Adam + Nesterov momentum. Can improve convergence speed.  \n","* **AMSGrad**: Modifies Adam to guarantee **convergence**, solving some theoretical issues.\n"]},{"cell_type":"markdown","id":"bcc1d94f","metadata":{"papermill":{"duration":0.004997,"end_time":"2025-10-06T21:09:38.636486","exception":false,"start_time":"2025-10-06T21:09:38.631489","status":"completed"},"tags":[]},"source":["# Learning Rate Schedules\n","\n","A fixed learning rate ($\\alpha$) may not be optimal during training: all layers and training phases may benefit from different learning rates. **Learning rate schedules** adjust $\\alpha$ over time to improve convergence and stability. They help avoid getting stuck in local minima, reducing oscillations as training converges.They are often combined with optimizers like Adam, RMSprop, or SGD.\n","\n","### Common Strategies\n","\n","**Step Decay**\n","   - Reduces the learning rate by a factor every few epochs.\n","   $$\n","   \\alpha_t = \\alpha_0 \\cdot \\text{decay\\_factor}^{\\lfloor t / \\text{step\\_size} \\rfloor}\n","   $$\n","   - Simple and widely used, especially for CNNs.\n","\n","**Exponential Decay**\n","   - Learning rate decreases exponentially over epochs:\n","   $$\n","   \\alpha_t = \\alpha_0 \\cdot e^{-k t}\n","   $$\n","   - $k$ = decay rate.\n","\n","**Polynomial Decay**\n","   - Learning rate decreases following a polynomial:\n","   $$\n","   \\alpha_t = \\alpha_0 \\cdot \\left(1 - \\frac{t}{T}\\right)^p\n","   $$\n","   - $T$ = total steps, $p$ = power.\n","\n","**Cosine Annealing**\n","   - Smoothly decreases the learning rate using a cosine function:\n","   $$\n","   \\alpha_t = \\alpha_{\\min} + \\frac{1}{2} (\\alpha_0 - \\alpha_{\\min}) \\left(1 + \\cos\\frac{\\pi t}{T}\\right)\n","   $$\n","   - Often used with **warm restarts** (SGDR) to escape local minima.\n","\n","**Warmup**\n","   - Starts with a very small learning rate, gradually increasing to $\\alpha_0$.\n","   - Helps prevent instability at the beginning of training, especially in Transformers and very deep networks.\n","\n","**Cyclical Learning Rate (CLR)**\n","   - Learning rate oscillates between $\\alpha_{\\min}$ and $\\alpha_{\\max}$ periodically.\n","   - Can improve generalization and escape shallow local minima.\n"]},{"cell_type":"markdown","id":"24a56ac8","metadata":{"papermill":{"duration":0.004282,"end_time":"2025-10-06T21:09:38.645506","exception":false,"start_time":"2025-10-06T21:09:38.641224","status":"completed"},"tags":[]},"source":["# **WEIGHT INITALIZATION**\n","\n","Proper weight initialization is crucial for training deep neural networks, as poor initialization can lead to problems like **vanishing gradients** or **exploding gradients**.\n","\n","## **Random Initialization**\n","Weights are initialized randomly, usually from a normal or uniform distribution. However, this can cause:\n","- **Vanishing gradients** (if weights are too small).\n","- **Exploding gradients** (if weights are too large).\n","$$\n","W \\sim \\mathcal{N} \\left( 0, 1\\right)\n","$$\n","this approach often fails in deep networks due to unstable gradients.\n","## **Xavier (Glorot) Initialization**\n","Designed for **sigmoid** and **tanh** activations, Xavier initialization ensures that the variance of activations remains stable across layers. \n","<br>The main idea is to set the initial weights of the network in a way that allows the activations and gradients to flow effectively during both forward and backpropagation. It considers the number of input and output units of each layer to determine the scale of the random initialization, the higher the number of outputs, the higher the need to spread weights.\n","$$\n","W \\sim \\mathcal{N} \\left( 0, \\frac{1}{n_{in} + n_{out}} \\right) \\quad \\text{or} \\quad W \\sim U \\left( -\\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}} \\right)\n","$$\n","- **$n_{in}$** = number of input neurons.\n","- **$n_{out}$** = number of output neurons.\n","\n","\n","## **He Initialization**\n","Designed for **ReLU and Leaky ReLU** activations. Since ReLU only activates half of the neurons (zeroing out negative values), it scales variance to avoid vanishing gradients.\n","$$\n","W \\sim \\mathcal{N} \\left( 0, \\frac{2}{n_{in}} \\right) \\quad \\text{or} \\quad W \\sim U \\left( -\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}} \\right)\n","$$"]},{"cell_type":"markdown","id":"08c0a3c7","metadata":{"papermill":{"duration":0.004437,"end_time":"2025-10-06T21:09:38.654631","exception":false,"start_time":"2025-10-06T21:09:38.650194","status":"completed"},"tags":[]},"source":[" # **BATCH NORMALIZATION (BN)**\n","Batch Normalization (BN) is a technique used to improve the training of deep neural networks by **normalizing the inputs** to each layer. It helps address issues like vanishing/exploding gradients, which can slow down training or make the network hard to optimize. It also allows the network to train faster and potentially improve generalization.\n","<br> BN is typically applied after the linear transformation of the layer and before the activation function (e.g., ReLU). This is because we want the activations to have normalized distributions.\n","\n","## **How Batch Normalization Works**\n","In deep networks, the distribution of each layer's inputs can change during training, causing instability and slow learning. This is known as **internal covariate shift**. Batch Normalization addresses this by normalizing the activations of each layer so they have a mean of 0 and a standard deviation of 1.\n","1. **Normalize** the output of a layer (before applying the activation function) by subtracting the mean and dividing by the standard deviation of the batch. This ensures the activations are centered around zero and have a unit variance.\n","$$\n","\\hat{x} = \\frac{x - \\mu_B}{\\sigma_B}\n","$$\n","Where:\n","* $\\mu_B$ is the mean of the batch.\n","* $\\sigma_B$ is the standard deviation of the batch.\n","\n","2. **Scale and shift** the normalized values using two learnable parameters, $\\gamma$ (scale) and $\\beta$ (shift), which allow the model to undo normalization if necessary. This helps retain the network's capacity to model complex relationships.\n","$$\n","y = \\gamma \\cdot \\hat{x} + \\beta\n","$$\n","Where:\n","* $\\hat{x}$ is the normalized value.\n","* $\\gamma$ and $\\beta$ are the learnable parameters that allow the model to scale and shift the normalized output.\n","* $y$ is the final output after applying the scaling and shifting.\n","\n","3. The **batch statistics** (mean and variance) are computed for each mini-batch during training, but during inference, fixed values are used (the running averages of the batch mean and variance from training).\n","## **Benefits of Batch Normalization**\n","1. **Stabilizes Training:** By normalizing the inputs to each layer, BN reduces the effect of outliers and helps the model converge faster.\n","2. **Faster Convergence:** BN allows for higher learning rates since the network is less likely to diverge during training.\n","3. **Reduced Need for Initialization:** BN reduces the sensitivity to weight initialization, which can help mitigate the risk of poor initializations.\n","4. **Improved Regularization:** It introduces some noise into the learning process, acting as a form of regularization. This can help reduce overfitting, though it is not a replacement for other regularization methods like dropout."]},{"cell_type":"markdown","id":"1bbb58d4","metadata":{"papermill":{"duration":0.004342,"end_time":"2025-10-06T21:09:38.663605","exception":false,"start_time":"2025-10-06T21:09:38.659263","status":"completed"},"tags":[]},"source":["# **DROPOUT**\n","**Dropout** is a regularization technique used in training deep neural networks to prevent overfitting and improve generalization. It works by randomly \"dropping out\" or deactivating a fraction of neurons during each forward and backward pass in training.\n","\n","**How Dropout Works**\n","1. **Randomly Drop Neurons:** During training, dropout randomly sets the output of each neuron to zero with a given probability, typically referred to as the **dropout rate** (denoted as $p$, where $p$ is the probability of dropping a neuron, a higher dropout rate might be used for very large networks or more complex tasks.). For example, if $p=0.2$ , then 20% of the neurons in the layer will be dropped out (set to zero).\n","2. **Scaling Neuron Outputs:** The remaining neurons that are not dropped out are scaled by $\\frac{1}{1-p}$. This is done to maintain the overall scale of the activations, ensuring that the expected value of the outputs during training remains the same as during inference (when no neurons are dropped out).\n","3. **Training vs Inference:** During training, dropout is applied to the network, but during inference (when making predictions), no neurons are dropped out. Instead, all neurons are used, and their outputs are scaled down by $1-p$ to account for the fact that dropout was applied during training.\n","\n","## **Dropout in Practice**\n","* **Training Phase:** Randomly deactivate a fraction of neurons.\n","* **Inference Phase:** Use all neurons, but scale their outputs to match the training behavior.\n","  \n","## **Why Dropout Helps**\n","* **Prevents Overfitting:** Dropout forces the network to not rely too heavily on any one neuron or connection, as different random subsets of neurons are activated during each training step. This prevents the model from becoming too specialized to the training data, improving its ability to generalize to new, unseen data.\n","* **Reduces Co-adaptation of Neurons:** In the absence of dropout, neurons may co-adapt, meaning they might rely on each other too much. Dropout ensures that each neuron must independently contribute to learning, which leads to a more robust model.\n","* **Improves Model Robustness:** By forcing the model to make predictions without certain features during training, it forces the network to learn more robust features and not memorize specific patterns in the training data."]},{"cell_type":"markdown","id":"ebe2a699","metadata":{"papermill":{"duration":0.004184,"end_time":"2025-10-06T21:09:38.672337","exception":false,"start_time":"2025-10-06T21:09:38.668153","status":"completed"},"tags":[]},"source":["# **REGULARIZATION**\n","\n","Regularization is a set of techniques used to **prevent overfitting** and improve the ability of a neural network to generalize on unseen data. While Dropout and Batch Normalization are common methods, there are additional strategies that can help your model perform better.\n","\n","\n","## **L1 and L2 Regularization**\n","\n","L1 and L2 are two common forms of regularization that **penalize large weights** in the network. By adding a penalty term to the loss function, the network is encouraged to keep weights small and simple, reducing overfitting.\n","\n","### **L2 Regularization (Ridge)**\n","L2 regularization adds the **sum of squared weights** to the loss function:\n","$$\n","L_{new} = L_{original} + \\lambda \\sum_{i} W_i^2\n","$$\n","* $L_{original}$ = original loss (e.g., MSE or Cross-Entropy)  \n","* $\\lambda$ = regularization strength (hyperparameter)  \n","* $W_i$ = weights of the network  \n","\n","**Effect:**  \n","* Penalizes large weights more strongly.  \n","* Encourages weights to be small but **rarely exactly zero**.  \n","\n","### **L1 Regularization (Lasso)**\n","L1 regularization adds the **sum of absolute weights** to the loss function:\n","$$\n","L_{new} = L_{original} + \\lambda \\sum_{i} |W_i|\n","$$\n","\n","**Effect:**  \n","* Encourages sparsity in weights (some weights become exactly zero).  \n","* Can be used for **feature selection**, removing irrelevant inputs automatically.  \n","\n","**Practical Tip:**  \n","- Combine L1 and L2 in what’s called **Elastic Net** to get benefits of both.\n","\n","\n","## **Early Stopping**\n","\n","Early stopping is a **training strategy** where the network **stops training when performance on a validation set stops improving**.  \n","\n","### How It Works:\n","1. Split your data into **training** and **validation** sets.  \n","2. Train the model and monitor **validation loss** at each epoch.  \n","3. Stop training when the validation loss **does not decrease** for a set number of epochs (patience).  \n","\n","**Benefits:**  \n","* Prevents overfitting by not letting the network memorize the training data.  \n","* Often works well with high-capacity networks.  \n","\n","**Example in practice:**  \n","If after 10 epochs your validation loss hasn’t improved, stop training and use the model from the epoch with the **lowest validation loss**.\n","\n","\n","## **Data Augmentation**\n","\n","Data augmentation artificially **increases the size of your dataset** by applying random transformations to existing data. This helps the model generalize better by **seeing more diverse examples**.  \n","\n","### Common Techniques for Images:\n","* Rotation  \n","* Flipping (horizontal/vertical)  \n","* Zooming or scaling  \n","* Color jitter  \n","* Random cropping  \n","\n","### Common Techniques for Text:\n","* Synonym replacement  \n","* Random insertion or deletion of words  \n","* Back-translation (translate to another language and back)  \n","\n","**Effect:**  \n","* Reduces overfitting by making the model **less dependent on specific features** in the training data.  \n","* Especially useful when training data is limited.\n","\n","\n","## **Noise Injection**\n","\n","Adding small amounts of **noise** to inputs, weights, or activations during training can act as a regularizer.  \n","\n","### How it works:\n","* Add Gaussian noise to input data or hidden layer activations.  \n","* Slightly perturb the weights during training.  \n","\n","**Effect:**  \n","* Forces the network to be robust to small variations in the data.  \n","* Helps generalization, similar to Dropout, but can be applied in different ways.\n","\n","\n","## **Combining Techniques**\n","\n","In practice, the most powerful networks **combine multiple regularization methods**:\n","\n","| Technique | Best Use Case |\n","|-----------|---------------|\n","| Dropout | Deep networks, CNNs, RNNs |\n","| Batch Normalization | Stabilize training, allows higher learning rate |\n","| L1 / L2 | Any neural network, especially fully connected layers |\n","| Early Stopping | When training for many epochs |\n","| Data Augmentation | When dataset is small or overfitting is strong |\n","| Noise Injection | Adds robustness in sensitive tasks |\n","\n","> **Rule of thumb:** Start simple (Dropout + L2), then add data augmentation or early stopping as needed.\n"," \n"]},{"cell_type":"markdown","id":"f46dd397","metadata":{"papermill":{"duration":0.004157,"end_time":"2025-10-06T21:09:38.680955","exception":false,"start_time":"2025-10-06T21:09:38.676798","status":"completed"},"tags":[]},"source":["# **OVERFITTING AND UNDERFITTING**\n","**Overfitting** and **underfitting** are two common problems in machine learning and statistical modeling, both of which can negatively affect the model’s ability to generalize to new, unseen data.\n","\n","## **Overfitting**\n","**Overfitting** occurs when a model learns not only the underlying patterns in the training data but also the noise or random fluctuations. As a result, the model performs very well on the training set but fails to generalize to new data.\n","* **High training accuracy, low test accuracy:** The model memorizes the training data, so it achieves very low error on the training set but performs poorly on the validation or test set.\n","* **Model is too complex:** The model has too many parameters relative to the amount of data available. This can happen if the model is too flexible or has too many layers, features, or nodes (in the case of neural networks).\n","* **Noise fitting:** The model may start fitting noise or irrelevant patterns in the training data that don't generalize to unseen data.\n","\n","**Causes of Overfitting**\n","* **Too many features:** When there are more features than necessary, the model may find patterns that don't exist in the real world.\n","* **Too complex a model:** High-capacity models, such as deep neural networks with many layers or high-degree polynomial regression, are more prone to overfitting, especially when the dataset is small or noisy.\n","* **Insufficient training data:** When there's not enough data, the model has fewer examples to learn from and may start to memorize rather than generalize.\n","  \n","**How to Combat Overfitting**\n","* **Cross-validation:** Use techniques like k-fold cross-validation to evaluate the model's performance on different subsets of data and avoid bias.\n","* **Simplify the model:** Reduce the model’s complexity, such as using fewer features, fewer layers, or regularization techniques.\n","* **Regularization:** Methods like L1 (Lasso), L2 (Ridge) regularization, or Dropout (in deep learning) can help penalize the model for overly large weights, which encourages simpler models.\n","* **Early stopping:** Stop training before the model starts to memorize the data. For neural networks, this can be done by monitoring the validation loss and halting training when it starts to increase.\n","* **More data:** Collecting more data can help the model better generalize and avoid overfitting to noise.\n","\n","## **Underfitting**\n","**Underfitting** occurs when the model is too simple to capture the underlying patterns in the data. The model does not perform well on either the training data or the test data because it has not learned the relevant features or relationships.\n","* **High bias:** The model makes strong assumptions that are too simplistic, resulting in poor performance on both the training and test sets.\n","* **Poor training accuracy:** The model may have high error on the training data itself, indicating that it has not learned the patterns in the data properly.\n","* **Model is too simple:** The model lacks sufficient complexity to capture the underlying structure in the data. For example, using a linear regression model for data that follows a non-linear pattern can result in underfitting.\n","\n","**Causes of Overfitting**\n","* **Too simple a model:** If the model is too basic (e.g., using a linear model for non-linear data), it will not have enough capacity to learn the complexities of the data.\n","* **Not enough features:** The model may not have enough relevant features to represent the data well.\n","* **Insufficient training:** If the model hasn’t been trained long enough or the learning rate is too high, it may not have had time to capture the patterns in the data.\n","\n","**How to Combat Overfitting**\n","* **Increase model complexity:** Use more sophisticated models or algorithms that have the capacity to capture more complex patterns, such as switching from linear regression to polynomial regression, or using more layers in a neural network.\n","* **Use more features:** Include more relevant features to give the model more information to work with.\n","* **Improve training:** Train the model for a longer period or adjust the learning rate to help the model converge to a better solution.\n","\n","## **Finding the Balance**\n","The goal in machine learning is to find a model that strikes a balance between overfitting and underfitting. This balance is often referred to as the **bias-variance trade-off**:\n","* **High bias (underfitting):** The model is too simple and cannot capture the underlying patterns, leading to poor performance on both training and test data.\n","* **High variance (overfitting):** The model is too complex and learns the noise in the training data, leading to great performance on the training set but poor generalization to new data.\n","\n","<br>The optimal model will have:\n","* **Low bias:** It captures the underlying patterns in the data.\n","* **Low variance:** It generalizes well to unseen data, not overfitting to the noise.\n","\n","To achieve the best performance, you want to find a model with **just the right complexity**, complex enough to capture the underlying patterns, but simple enough to avoid memorizing the data."]},{"cell_type":"markdown","id":"727ef0e9","metadata":{"papermill":{"duration":0.004441,"end_time":"2025-10-06T21:09:38.689938","exception":false,"start_time":"2025-10-06T21:09:38.685497","status":"completed"},"tags":[]},"source":["# **EVALUATION METRICS**\n","\n","Evaluation metrics are tools used to **measure the performance** of a machine learning model. Choosing the right metric is crucial, because it tells you whether your model is actually doing what you want.\n","\n","Metrics vary depending on the **type of problem**: regression, classification, or ranking tasks.\n","\n","\n","## **Metrics for Regression**\n","\n","Regression models predict **continuous values**, so metrics focus on measuring the **difference between predicted and actual values**.\n","\n","### **Mean Squared Error (MSE)**\n","$$\n","MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","$$`\n","* Measures **average squared difference** between predictions and actual values.  \n","* Penalizes **large errors more than small errors**.  \n","* Good for emphasizing big mistakes.\n","\n","### **Root Mean Squared Error (RMSE)**\n","$$\n","RMSE = \\sqrt{MSE}\n","$$\n","* Expressed in the **same units as the target**, making it easier to interpret.  \n","\n","### **Mean Absolute Error (MAE)**\n","$$\n","MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n","$$\n","* Measures the **average absolute difference** between predictions and actual values.  \n","* Less sensitive to **outliers** than MSE.\n","\n","### **R-Squared ($R^2$)**\n","$$\n","R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}\n","$$\n","* Measures how well the model explains the **variance of the target**.  \n","* $R^2 = 1$ → perfect prediction, $R^2 = 0$ → model predicts as well as the mean.  \n","\n","\n","## **Metrics for Classification**\n","\n","Classification models predict **discrete categories**, so metrics focus on **how often predictions match the true labels**.\n","\n","### **Confusion Matrix**\n","A table that summarizes **true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)**.  \n","\n","| Actual \\ Predicted | Positive | Negative |\n","|------------------|---------|---------|\n","| Positive          | TP      | FN      |\n","| Negative          | FP      | TN      |\n","\n","**Useful for calculating other metrics.**\n","\n","\n","### **Accuracy**\n","$$\n","Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n","$$\n","* Percentage of **correct predictions**.  \n","* Works well for **balanced datasets**.  \n","* Misleading if classes are **imbalanced**.\n","\n","### **Precision**\n","$$\n","Precision = \\frac{TP}{TP + FP}\n","$$\n","* Measures **how many predicted positives are actually positive**.  \n","* Important when **false positives are costly** (e.g., spam detection).\n","\n","### **Recall (Sensitivity)**\n","$$\n","Recall = \\frac{TP}{TP + FN}\n","$$\n","* Measures **how many actual positives were correctly predicted**.  \n","* Important when **missing a positive is costly** (e.g., disease detection).\n","\n","### **F1 Score**\n","$$\n","F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n","$$\n","* Harmonic mean of precision and recall.  \n","* Good for **imbalanced datasets**.  \n","\n","\n","### **ROC Curve & AUC**\n","* **ROC Curve:** Plots **True Positive Rate (Recall)** vs **False Positive Rate (FPR)** at different thresholds.  \n","* **AUC (Area Under the Curve):** Measures **overall ability to distinguish classes**.  \n","  * AUC = 1 → perfect classifier  \n","  * AUC = 0.5 → random guessing  \n","\n","\n","## **Metrics for Multi-Class Classification**\n","\n","For multi-class tasks (more than 2 classes), metrics are extended:\n","\n","* **Macro Average:** Compute metric per class, then average (treats all classes equally).  \n","* **Weighted Average:** Compute metric per class, then average weighted by class frequency.  \n","* **Confusion Matrix:** Expanded to **n x n** table for n classes.  \n","\n","\n","## **Metrics for Ranking & Probabilistic Models**\n","\n","Some models output **scores or probabilities** rather than exact classes. Metrics include:\n","\n","* **Log Loss (Cross-Entropy Loss):**\n","$$\n","L = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} y_{ij} \\log(\\hat{y}_{ij})\n","$$\n","* Measures **difference between predicted probabilities and true labels**.  \n","* Lower is better.  \n","\n","* **Precision / Recall:** Used in **recommendation systems**, measures accuracy for top k predictions.  \n","* **Mean Average Precision (MAP):** Average precision across all queries or samples.  "]},{"cell_type":"markdown","id":"e8833bba","metadata":{"papermill":{"duration":0.004173,"end_time":"2025-10-06T21:09:38.698617","exception":false,"start_time":"2025-10-06T21:09:38.694444","status":"completed"},"tags":[]},"source":["# Useful resources\n","\n","### **Youtube**\n","*3blue1brow*: Neural networks Course \n","www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n","\n","*Normalized Nerd*: Why do we need Cross Entropy Loss? (Visualized)\n","www.youtube.com/watch?v=gIx974WtVb4\n","\n","*StatQuest with Josh Starmer*: Gradient Descent, Step-by-Step\n","www.youtube.com/watch?v=sDv4f4s2SB8\n","\n","*Emergent Garden*: Watching Neural Networks Learn\n","www.youtube.com/watch?v=TkwXa7Cvfr8\n","\n","*Emergent Garden*: Why Neural Networks can learn (almost) anything\n","www.youtube.com/watch?v=0QczhVg5HaI\n","\n","*Artem Kirsanov*: The Most Important Algorithm in Machine Learning\n","www.youtube.com/watch?v=SmZmBKc7Lrs\n","\n","*Artem Kirsanov*: What Textbooks Don't Tell You About Curve Fitting\n","www.youtube.com/watch?v=q7seckj1hwM\n","\n","*Artem Kirsanov*: The Key Equation Behind Probability\n","www.youtube.com/watch?v=KHVR587oW8I\n","\n","*Samson Zhang*: Building a neural network FROM SCRATCH\n","www.youtube.com/watch?v=w8yWXqWQYmU\n","\n","*Layerwise Lectures*: Hopfield network: How are memories stored in neural networks? \n","www.youtube.com/watch?v=piF6D6CQxUw"]},{"cell_type":"code","execution_count":null,"id":"e3896133","metadata":{"papermill":{"duration":0.004334,"end_time":"2025-10-06T21:09:38.707428","exception":false,"start_time":"2025-10-06T21:09:38.703094","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":4.436657,"end_time":"2025-10-06T21:09:39.232604","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-06T21:09:34.795947","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}