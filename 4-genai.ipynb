{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc7afca",
   "metadata": {
    "papermill": {
     "duration": 0.002378,
     "end_time": "2025-10-28T13:17:51.733037",
     "exception": false,
     "start_time": "2025-10-28T13:17:51.730659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Generative AI Personal Notes**\n",
    "\n",
    "# **Introduction to Generative AI**\n",
    "\n",
    "## **Definition of Generative AI**\n",
    "Generative AI is basically any kind of AI that can *create* new content rather than just analyzing existing data. This could be text, images, music, code, or even video. Unlike traditional AI that mostly classifies, predicts, or detects patterns, generative AI tries to *imitate* the process of human creativity. Think of it as teaching a machine to “imagine” something new based on what it has learned. The cool part is that it doesn’t just copy what it’s seen—it generates something original, though often influenced by the training data.  \n",
    "\n",
    "## **Brief History and Evolution**\n",
    "Generative AI isn’t exactly brand new, but it has exploded in the past decade. Some key milestones:  \n",
    "- Early work: Back in the 1990s and 2000s, we had basic probabilistic models and some neural networks that could generate simple patterns.  \n",
    "- GANs: In 2014, Generative Adversarial Networks (GANs) came out, which was a huge game-changer for generating realistic images. The “adversarial” part means two networks compete—the generator tries to create content, the discriminator tries to spot fakes. This tug-of-war leads to surprisingly realistic outputs.  \n",
    "- Transformers & LLMs: Around 2017, the Transformer architecture arrived, making it much easier to handle sequential data like text. That gave rise to models like GPT, which can write paragraphs, answer questions, or even code.  \n",
    "- Multi-modal models: More recently, AI started handling multiple types of data at once—images, text, audio—blurring the lines between creative fields.  \n",
    "\n",
    "So basically, the field went from basic pattern generation → convincing images → sophisticated language → multi-modal creativity. The pace has been insane, and every year there’s something new that feels “sci-fi level.”  \n",
    "\n",
    "## **Main Application Areas**\n",
    "Generative AI is everywhere now, even if you don’t notice it. Some big categories:  \n",
    "- **Text & Language**: Chatbots, content creation, summarization, translation, coding assistants. Basically anything involving language can be augmented.  \n",
    "- **Images & Art**: AI-generated art, photo editing, meme creation, even fashion design. Tools like DALL·E or MidJourney fall here.  \n",
    "- **Audio & Music**: AI can compose music, create realistic voiceovers, or generate sound effects.  \n",
    "- **Video & Animation**: Generating short clips, animating characters, deepfake-style video editing. Still harder than text or images, but improving fast.  \n",
    "- **Science & Research**: Drug discovery, protein folding, chemical simulation, generating hypotheses or datasets.  \n",
    "- **Business & Productivity**: Automated reports, personalized marketing content, document drafting, and customer support.  \n",
    "\n",
    "In short, any field where creativity, simulation, or content production is important is being touched by generative AI. It’s not perfect, but the rate of improvement is crazy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe882c",
   "metadata": {
    "papermill": {
     "duration": 0.00164,
     "end_time": "2025-10-28T13:17:51.737368",
     "exception": false,
     "start_time": "2025-10-28T13:17:51.735728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Technical Fundamentals**\n",
    "## **Types of Generative Models**\n",
    "### **Autoregressive Models**\n",
    "### **Variational Autoencoders (VAE)**\n",
    "### **Generative Adversarial Networks (GAN)**\n",
    "### **Diffusion Models**\n",
    "- For text-to-image and multimodal generation\n",
    "- Produces high-quality outputs by iteratively denoising\n",
    "\n",
    "## **Neural Networks and Deep Learning**\n",
    "## **Tokenization and Embeddings**\n",
    "- Semantic embeddings\n",
    "- Chunking strategies for long documents\n",
    "- Handling token limits and truncation\n",
    "- Vector stores and similarity search\n",
    "\n",
    "## **Training and Fine-tuning**\n",
    "- Supervised fine-tuning\n",
    "- Few-shot / zero-shot learning\n",
    "- Dataset preparation and validation\n",
    "- Error-driven fine-tuning (hard examples)\n",
    "- Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "---\n",
    "\n",
    "# **Generative Language Models**\n",
    "## **Introduction to Large Language Models (LLMs)**\n",
    "## **Transformer Architecture**\n",
    "## **Training and Fine-tuning**\n",
    "- Pre-training vs fine-tuning vs parameter-efficient tuning\n",
    "## **Prompting and Model Control**\n",
    "- Prompt engineering\n",
    "- Template placeholders and runtime variables\n",
    "- Emphasizing specific vocabulary vs general knowledge\n",
    "## **Text Generation Parameters**\n",
    "temperature, tok-p, max tokens, stop sequences, penalties\n",
    "## **Retrieval-Augmented Generation (RAG)**\n",
    "- Difference from prompt engineering and fine-tuning\n",
    "- Indexing, embedding, retrieval pipelines\n",
    "- Advantages for grounding and reducing hallucinations\n",
    "## **LLM Frameworks and Tools**\n",
    "LangChain, LlamaIndex, Haystack\n",
    "## **Advanced Prompting Techniques**\n",
    "- Preamble/context for style and tone\n",
    "- Self-consistency checks and iterative reasoning\n",
    "## **Model Safety and Bias Mitigation**\n",
    "- Groundedness / factuality\n",
    "- Prompt injection mitigation\n",
    "- Bias sources and mitigation strategies\n",
    "- Fact-checking and auditing pipelines\n",
    "## **Memory and State Handling**\n",
    "- Session management / context retention\n",
    "- Session timeout and ephemeral context\n",
    "## **Multi-modal LLMs**\n",
    "- Text + Image\n",
    "- Text + Audio\n",
    "- Text + video\n",
    "\n",
    "---\n",
    "\n",
    "# **Generative AI for Images and Multimedia**\n",
    "## **Image Generation Models (DALL·E, Stable Diffusion, etc.)**\n",
    "## **Audio and Music Generation**\n",
    "## **Video Generation**\n",
    "\n",
    "---\n",
    "\n",
    "# **Tools and Platforms**\n",
    "## **Libraries and Frameworks (Hugging Face, OpenAI, TensorFlow, etc.)**\n",
    "## **Deployment Tools and APIs**\n",
    "- Vector stores and database integration\n",
    "- On-demand vs batch inference\n",
    "## **No-code / Low-code Interfaces**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620310b",
   "metadata": {
    "papermill": {
     "duration": 0.001545,
     "end_time": "2025-10-28T13:17:51.740771",
     "exception": false,
     "start_time": "2025-10-28T13:17:51.739226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **OCI Generative AI**\n",
    "\n",
    "**Oracle Cloud Infrastructure (OCI)** is a comprehensive cloud platform that provides computing, storage, networking, and data management services for enterprises and developers. It offers a range of infrastructure-as-a-service (IaaS), platform-as-a-service (PaaS), and software-as-a-service (SaaS) solutions, designed to support high-performance applications and scalable workloads.  \n",
    "\n",
    "OCI emphasizes security, reliability, and performance, with features such as dedicated compute clusters, encrypted storage, and advanced networking options. The platform also includes specialized services for artificial intelligence and machine learning, allowing developers to build, train, and deploy AI models with integrated tools and resources.\n",
    "\n",
    "\n",
    "## OCI Generative AI Service\n",
    "\n",
    "OCI Generative AI Service provides pre-trained foundation models, primarily chat models that can be used in on-demand serving mode. Model endpoints act as the designated points where user requests are sent and responses are received. When using on-demand inference, customers are charged per character processed, which makes it suitable for applications with variable or moderate usage.  \n",
    "\n",
    "Session-enabled endpoints allow the chat context to be retained during the session, helping maintain continuity in conversations. These sessions automatically end after a specified timeout period.  \n",
    "\n",
    "The service also offers a citation option, which displays the source details for each chat response, providing transparency and traceability. Content moderation can be enabled during endpoint creation to ensure that generated content complies with safety and policy requirements.\n",
    "\n",
    "### Types of Pre-trained Models\n",
    "- **Chat models:** For interactive text-based conversations.\n",
    "- **Embedding models:** For semantic search, similarity, or vector-based retrieval.\n",
    "- **Image generation models:** For text-to-image tasks (if enabled in OCI).  \n",
    "- **Other specialized models:** Depending on service updates, e.g., summarization or code generation.\n",
    "\n",
    "\n",
    "## Fine-Tuning and Custom Models\n",
    "\n",
    "OCI allows fine-tuning of AI models using TCU methods, which are particularly suitable for large datasets ranging from hundreds of thousands to millions of samples.  \n",
    "\n",
    "**Training Compute Unit (TCU)** is Oracle's unit of measurement for the computational resources allocated to model training and fine-tuning. Each TCU represents a portion of GPU or TPU resources, memory, and compute capacity, allowing predictable scaling of training workloads. Using TCUs, developers can estimate training time, optimize resource usage, and manage costs effectively.  \n",
    "\n",
    "When preparing a custom dataset for fine-tuning, it is important to organize the data into separate files for training and validation to ensure proper evaluation and prevent data leakage.\n",
    "\n",
    "Once a model has been fine-tuned, it is stored securely in OCI Object Storage, with encryption enabled by default to protect customer data. Dedicated AI clusters provide predictable pricing that does not fluctuate with demand, and they help minimize GPU or TPU memory overhead by sharing base model weights across multiple fine-tuned models. These clusters also enable the deployment of several fine-tuned models within a single infrastructure, improving efficiency and resource utilization.\n",
    "\n",
    "## Prompting and RAG in OCI\n",
    "\n",
    "In OCI, deciding between prompting and training depends on the goal. Prompting is best used when you want the model to emphasize specific vocabulary or phrases, such as product names, during responses. Fine-tuning, on the other hand, is more suitable for improving the model's understanding of broader domain-specific terminology across multiple tasks.  \n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** offers a way to enhance model responses without the costs of full model training. Unlike traditional fine-tuning, RAG does not modify the model weights; instead, it augments generation with relevant information retrieved from external documents. This approach reduces hallucinations and ensures that responses are grounded in factual sources.  \n",
    "\n",
    "The typical RAG pipeline consists of three main stages:\n",
    "\n",
    "1. **Document Embedding:** External documents are converted into vector representations (embeddings) that capture their semantic meaning.  \n",
    "2. **Indexing and Retrieval:** The embeddings are stored in a vector database or index. When a query is issued, the system retrieves the most relevant documents by comparing the query embedding with the stored vectors.  \n",
    "3. **Generation:** The LLM uses the retrieved documents as context to generate responses, combining its language understanding with factual information from the sources.  \n",
    "\n",
    "**Key points about RAG:**\n",
    "- It simplifies setup compared to full fine-tuning, avoiding the need for large-scale training datasets and TCU usage.  \n",
    "- Operational complexity still exists: the retrieval and embedding pipeline must be carefully configured, including document preprocessing, vector indexing, and relevance tuning.  \n",
    "- RAG works well with prompting: prompts can still guide style, emphasis, or vocabulary, while retrieval provides grounded content.  \n",
    "- This method allows fine control over sources used, enabling auditing, bias mitigation, and transparent content generation.  \n",
    "\n",
    "Overall, RAG strikes a balance between **flexibility, grounding, and practical implementation**, making it a preferred choice when you want reliable, fact-based outputs without the overhead of fine-tuning the model itself.\n",
    "\n",
    "\n",
    "\n",
    "## Vector Search and Database Integration\n",
    "\n",
    "Before executing any code related to vector search in OCI, embeddings must first be created and stored in the database. These embeddings serve as the foundation for building vector stores, which allow the model to efficiently search and retrieve relevant information from large datasets. Using vector store code makes it possible to create these stores directly from database tables of embeddings, simplifying integration with Oracle Database AI services.  \n",
    "\n",
    "### Vector Store Creation and Management\n",
    "- **Creating a vector store:** Embeddings generated from documents or data are indexed in a vector database (e.g., OCI Object Storage + indexing service) to enable fast similarity search.  \n",
    "- **Updating and maintaining:** When new data is added or documents change, embeddings need to be updated in the vector store to maintain search accuracy.  \n",
    "- **Querying:** When a user query is issued, it is converted into an embedding and compared against the stored vectors to retrieve the most relevant documents.\n",
    "\n",
    "### Importance for RAG\n",
    "- Vector stores are a critical component of **Retrieval-Augmented Generation (RAG)** pipelines. The retrieved vectors provide the LLM with grounded, contextually relevant information to generate accurate responses.  \n",
    "- Proper vector store management ensures minimal retrieval errors, reduces hallucinations, and allows fine control over the documents and sources used during generation.\n",
    "\n",
    "When integrating vector search functionality with an Oracle Database:\n",
    "- The **core field** in the vector search results represents the distance between the query vector and the corresponding body vector, indicating similarity or relevance.  \n",
    "- **Network configuration:** Subnet ingress rules must be correctly set, and the source type for the Oracle database should be CIDR to ensure secure access.\n",
    "\n",
    "## Knowledge Bases\n",
    "\n",
    "OCI Generative AI Agents can connect to knowledge bases, which are structured collections of documents or data used to provide contextually accurate responses.  \n",
    "\n",
    "- **Ingestion:** Documents are ingested into the knowledge base and optionally converted into embeddings for efficient retrieval.  \n",
    "- **Failure handling:** If an ingestion job fails partially, only the failed and updated files are re-ingested.  \n",
    "- **Deletion:** Deleting a knowledge base is permanent and cannot be undone.  \n",
    "- **Integration with vector search:** Knowledge bases can leverage embeddings and vector stores to enhance retrieval, which is critical for RAG pipelines.  \n",
    "- **Endpoint connection:** Agents can query knowledge bases through model endpoints to provide grounded, fact-based responses.\n",
    "\n",
    "## Session Management in Agents\n",
    "\n",
    "OCI Generative AI Agents include session management features that track the conversational history, including both user prompts and model responses. This allows the agent to maintain context across interactions, which is essential for creating coherent and continuous conversations.  \n",
    "\n",
    "When session-enabled endpoints are used, the context is retained for the duration of the session. However, sessions have a timeout, after which the context is automatically cleared. Additionally, if an ingestion job processes multiple files and some fail, only the failed and subsequently updated files are re-ingested when the job is restarted, ensuring efficient data processing without duplicating work.\n",
    "\n",
    "\n",
    "## Configuration and Code\n",
    "\n",
    "Configuring OCI Generative AI services requires understanding key code settings as well as operational features of endpoints and session handling.\n",
    "\n",
    "- **OCI Configuration File:**  \n",
    "  The line `oci.config.from_file()` loads OCI configuration details from a local file, allowing the system to authenticate and connect to the cloud environment.\n",
    "\n",
    "- **Serving Mode:**  \n",
    "  For chat models, setting `ChatDetail.ServingMode = OnDemandServingMode` ensures that the model operates in on-demand inference mode, generating responses only when requested and providing flexibility in resource usage.\n",
    "\n",
    "- **Embeddings Truncation:**  \n",
    "  When working with embeddings, the setting `EmbedTextDetail.truncate = none` disables truncation of the input text, ensuring the full content is used for generating embeddings. This is particularly important for long texts to preserve context and information for vector search or retrieval tasks.\n",
    "\n",
    "- **Model Endpoints:**  \n",
    "  Endpoints are the designated points where user requests are sent and model responses are received.  \n",
    "  - **Session-enabled endpoints:** Retain the conversation context for the duration of the session.  \n",
    "  - **Session timeout:** After the specified timeout, the context is cleared automatically.  \n",
    "  - **Immutable session option:** Once enabled, the session option cannot be changed.\n",
    "\n",
    "- **Citation Option:**  \n",
    "  Enabling citation displays source details for each chat response, improving transparency and traceability of the model's outputs.\n",
    "\n",
    "- **Content Moderation:**  \n",
    "  Content moderation ensures that generated outputs comply with safety policies and regulations. It must be enabled when creating the endpoint.\n",
    "\n",
    "This configuration and operational setup ensures that developers can manage authentication, model behavior, session context, and output transparency effectively while using OCI Generative AI.\n",
    "\n",
    "\n",
    "\n",
    "## Text Processing and Embeddings\n",
    "\n",
    "When working with text in OCI Generative AI, processing it properly is essential for generating accurate embeddings and enabling efficient retrieval. Text is typically divided into paragraphs, which are then split into sequences and further broken down into tokens until the desired chunk size is reached. This ensures that each piece of text can be processed by the model without exceeding token limits.  \n",
    "\n",
    "Handling token limits is particularly important when dealing with long documents, as exceeding the model's capacity can result in incomplete embeddings. In cases where only certain parts of a document are most relevant, a truncation strategy can be applied. For example, if the most important information is at the beginning, the end of the text may be truncated to fit within the token limit while preserving key content.\n",
    "\n",
    "\n",
    "## Multi-modal and Advanced Models\n",
    "\n",
    "OCI Generative AI supports advanced multi-modal capabilities, allowing models to handle different types of input and output beyond plain text. Diffusion models, for example, are used to generate complex outputs such as converting text descriptions into images. These models iteratively refine the output, producing high-quality and detailed results suitable for applications in design, marketing, and creative content generation.  \n",
    "\n",
    "The **Cohere Embed V3** model represents an improvement over its predecessors in several ways. It provides higher-dimensional embeddings that capture semantic meaning more accurately, which improves performance in tasks like semantic search, clustering, and retrieval. This enhanced precision allows the model to better understand textual nuances and relationships, making similarity searches and document retrieval more reliable. Compared to previous versions, Cohere Embed V3 is optimized for **speed, accuracy, and generalization**, enabling developers to build applications that require high-quality semantic understanding across large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f188c0",
   "metadata": {
    "papermill": {
     "duration": 0.001494,
     "end_time": "2025-10-28T13:17:51.744017",
     "exception": false,
     "start_time": "2025-10-28T13:17:51.742523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Useful resources\n",
    "\n",
    "### **Youtube**\n",
    "\n",
    "### **Websites**\n",
    "\n",
    "### **Courses**\n",
    "\n",
    "*Oracle*: OCI Generative AI Professional (2025)\n",
    "https://mylearn.oracle.com/ou/learning-path/become-an-oci-generative-ai-professional-2025/147863"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.402932,
   "end_time": "2025-10-28T13:17:52.165687",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-28T13:17:46.762755",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
