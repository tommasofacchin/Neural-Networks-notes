{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b97623d",
   "metadata": {
    "papermill": {
     "duration": 0.003016,
     "end_time": "2025-11-20T22:56:49.956963",
     "exception": false,
     "start_time": "2025-11-20T22:56:49.953947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Generative AI Personal Notes**\n",
    "\n",
    "# **Introduction to Generative AI**\n",
    "\n",
    "## **Definition of Generative AI**\n",
    "Generative AI is basically any kind of AI that can *create* new content rather than just analyzing existing data. This could be text, images, music, code, or even video. Unlike traditional AI that mostly classifies, predicts, or detects patterns, generative AI tries to *imitate* the process of human creativity. Think of it as teaching a machine to “imagine” something new based on what it has learned. The cool part is that it doesn’t just copy what it’s seen—it generates something original, though often influenced by the training data.  \n",
    "\n",
    "## **Brief History and Evolution**\n",
    "Generative AI isn’t exactly brand new, but it has exploded in the past decade. Some key milestones:  \n",
    "- Early work: Back in the 1990s and 2000s, we had basic probabilistic models and some neural networks that could generate simple patterns.  \n",
    "- GANs: In 2014, Generative Adversarial Networks (GANs) came out, which was a huge game-changer for generating realistic images. The “adversarial” part means two networks compete—the generator tries to create content, the discriminator tries to spot fakes. This tug-of-war leads to surprisingly realistic outputs.  \n",
    "- Transformers & LLMs: Around 2017, the Transformer architecture arrived, making it much easier to handle sequential data like text. That gave rise to models like GPT, which can write paragraphs, answer questions, or even code.  \n",
    "- Multi-modal models: More recently, AI started handling multiple types of data at once—images, text, audio—blurring the lines between creative fields.  \n",
    "\n",
    "So basically, the field went from basic pattern generation → convincing images → sophisticated language → multi-modal creativity. The pace has been insane, and every year there’s something new that feels “sci-fi level.”  \n",
    "\n",
    "## **Main Application Areas**\n",
    "Generative AI is everywhere now, even if you don’t notice it. Some big categories:  \n",
    "- **Text & Language**: Chatbots, content creation, summarization, translation, coding assistants. Basically anything involving language can be augmented.  \n",
    "- **Images & Art**: AI-generated art, photo editing, meme creation, even fashion design. Tools like DALL·E or MidJourney fall here.  \n",
    "- **Audio & Music**: AI can compose music, create realistic voiceovers, or generate sound effects.  \n",
    "- **Video & Animation**: Generating short clips, animating characters, deepfake-style video editing. Still harder than text or images, but improving fast.  \n",
    "- **Science & Research**: Drug discovery, protein folding, chemical simulation, generating hypotheses or datasets.  \n",
    "- **Business & Productivity**: Automated reports, personalized marketing content, document drafting, and customer support.  \n",
    "\n",
    "In short, any field where creativity, simulation, or content production is important is being touched by generative AI. It’s not perfect, but the rate of improvement is crazy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf8fec9",
   "metadata": {
    "papermill": {
     "duration": 0.002022,
     "end_time": "2025-11-20T22:56:49.961712",
     "exception": false,
     "start_time": "2025-11-20T22:56:49.959690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Technical Fundamentals**\n",
    "## **Types of Generative Models**\n",
    "### **Autoregressive Models**\n",
    "### **Variational Autoencoders (VAE)**\n",
    "### **Generative Adversarial Networks (GAN)**\n",
    "### **Diffusion Models**\n",
    "- For text-to-image and multimodal generation\n",
    "- Produces high-quality outputs by iteratively denoising\n",
    "\n",
    "---\n",
    "\n",
    "## **Fine-tuning**\n",
    "\n",
    "Fine-tuning is the process of taking a pretrained Large Language Model (LLM) and adapting it to a **specific task or domain**. While pretraining gives the model a broad understanding of language, fine-tuning focuses it on particular use cases, improving accuracy and relevance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Fine-tuning is Needed**\n",
    "Pretrained LLMs are general-purpose—they know grammar, syntax, and general facts, but they may not perform optimally for specialized tasks like legal document analysis, medical question answering, or customer support automation. Fine-tuning helps the model learn task-specific patterns and terminology.\n",
    "\n",
    "---\n",
    "\n",
    "### **Methods of Fine-tuning**\n",
    "\n",
    "1. **Full Fine-tuning**  \n",
    "   - All model parameters are updated using task-specific data.  \n",
    "   - Effective for specialized domains but computationally expensive, especially for very large models.  \n",
    "   - Requires **high-end GPUs or TPUs** for training, and the process can take hours or days depending on model size.  \n",
    "   - **Example:** Training GPT on a legal corpus to generate legal contracts.\n",
    "\n",
    "2. **Parameter-Efficient Fine-tuning**  \n",
    "   - Only a subset of the model parameters are updated, using methods such as **adapters**, **LoRA (Low-Rank Adaptation)**, or **prefix tuning**.  \n",
    "   - Reduces compute costs and memory requirements while maintaining strong performance.  \n",
    "   - **Example:** Fine-tuning a 70B-parameter LLaMA model for medical question answering without retraining all parameters.\n",
    "\n",
    "3. **Reinforcement Learning from Human Feedback (RLHF)**  \n",
    "   - Combines supervised fine-tuning with human preferences.  \n",
    "   - The model learns to generate outputs aligned with desired behaviors, improving safety, helpfulness, and alignment.  \n",
    "   - **Example:** ChatGPT is fine-tuned using RLHF to provide more useful, polite, and factual responses.\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges and Considerations**\n",
    "\n",
    "- **Compute Requirements**: Full fine-tuning of large models can be extremely resource-intensive, requiring **powerful GPUs or TPUs**, large memory, and long training times.  \n",
    "- **Data Sensitivity**: Fine-tuned models are only as good as the data they were trained on. If the task or domain data changes, the model may need to be **re-fine-tuned**, which can be costly and time-consuming.  \n",
    "- **Maintenance Overhead**: Continually updating or expanding the model for new data or tasks can become a recurring effort.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps in Fine-tuning**\n",
    "1. **Dataset Preparation**: Curate a high-quality dataset with examples relevant to the task.  \n",
    "2. **Training**: Update model parameters using the dataset, adjusting learning rates and regularization.  \n",
    "3. **Validation**: Evaluate performance on unseen examples to prevent overfitting.  \n",
    "4. **Deployment**: Integrate the fine-tuned model into applications for inference.\n",
    "\n",
    "---\n",
    "\n",
    "# **Generative Language Models**\n",
    "\n",
    "# **LLM Applications**\n",
    "## **Chatbots and Virtual Assistants**\n",
    "## **Content Creation**\n",
    "## **Code Generation**\n",
    "## **Summarization and Translation**\n",
    "## **Data Analysis and Insights**\n",
    "## **Prompt Engineering**\n",
    "### **Basics of Prompt Design**\n",
    "### **Temperature and Creativity**\n",
    "## **Decoding / Generation Strategies**\n",
    "### **Greedy Search**\n",
    "### **Beam Search**\n",
    "### **Top-k Sampling**\n",
    "### **Top-p (Nucleus Sampling)**\n",
    "### **Frequency and Presence Penalties**\n",
    "### **Max Tokens**\n",
    "\n",
    "## **Prompting and Model Control**\n",
    "- Prompt engineering\n",
    "- Template placeholders and runtime variables\n",
    "- Emphasizing specific vocabulary vs general knowledge\n",
    "## **Text Generation Parameters**\n",
    "temperature, tok-p, max tokens, stop sequences, penalties\n",
    "\n",
    "---\n",
    "\n",
    "## **Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a method that combines the generative capabilities of Large Language Models (LLMs) with external knowledge retrieval. This allows the model to provide answers that are more accurate, grounded, and relevant, using up-to-date or domain-specific information at inference time. Unlike standard LLM outputs, which rely solely on what the model learned during pretraining, RAG dynamically incorporates external knowledge without retraining.\n",
    "\n",
    "---\n",
    "\n",
    "### **How RAG Differs from Prompt Engineering and Fine-Tuning**\n",
    "\n",
    "- **Prompt Engineering**: Adjusts the input text to guide the model’s responses, but the model is still limited to its pretrained knowledge.  \n",
    "- **Fine-Tuning**: Updates model parameters on task-specific data, improving performance for specialized tasks, but requires compute resources and retraining.  \n",
    "- **RAG**: Queries an external knowledge base in real time, feeding the retrieved information into the model. This makes the system flexible, scalable, and able to use current knowledge without retraining.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Components of RAG**\n",
    "\n",
    "1. **Indexing**  \n",
    "   - Documents or knowledge sources are preprocessed and organized into an index for efficient retrieval.  \n",
    "   - The index acts as a map of the content, storing embeddings, metadata, or keywords for quick access.  \n",
    "   - **Example:** Research papers, FAQs, or company manuals organized for fast lookup.\n",
    "\n",
    "2. **Embedding Representations**  \n",
    "   - Queries and document chunks are converted into numerical vectors that capture their meaning.  \n",
    "   - Similarity between embeddings identifies which documents are most relevant to the query.  \n",
    "   - **Example:** Using cosine similarity to retrieve the five most relevant articles for a user question.\n",
    "\n",
    "3. **Vector Storage & Similarity Search**  \n",
    "   - Embeddings are stored in a vector database optimized for fast semantic retrieval.  \n",
    "   - Similarity search compares the query embedding against stored embeddings to find the closest matches.  \n",
    "   - Popular tools include **FAISS**, **Milvus**, and **Weaviate**.  \n",
    "   - **Example:** A query about “climate change effects” retrieves relevant papers even if the wording differs from the documents.\n",
    "\n",
    "4. **Chunking**  \n",
    "   - Long documents are split into smaller pieces, or “chunks,” to handle token limits and improve retrieval accuracy.  \n",
    "   - Overlapping chunks can be used to maintain context.  \n",
    "   - Each chunk is embedded and stored in the vector database.  \n",
    "   - **Example:** A 200-page report is divided into 500 smaller chunks; only the most relevant ones are retrieved for answering a question.\n",
    "\n",
    "5. **Retrieval Pipeline**  \n",
    "   - The system uses similarity search on the vector database to retrieve the most relevant chunks for a given query.  \n",
    "   - Retrieved content is passed to the LLM as context for generating a response.  \n",
    "   - **Example:** A query about “X disease symptoms” pulls relevant research abstracts before the model generates an answer.\n",
    "\n",
    "6. **Generation Step**  \n",
    "   - The LLM produces a response combining its internal knowledge and the retrieved context.  \n",
    "   - This approach reduces hallucinations and improves factual accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of RAG**\n",
    "\n",
    "- **Grounded Responses**: Outputs are based on external sources, improving accuracy.  \n",
    "- **Domain Adaptability**: Works well for specialized topics without retraining the model.  \n",
    "- **Reduced Hallucinations**: By using real documents, the model is less likely to invent information.  \n",
    "- **Dynamic Knowledge Updating**: Knowledge bases can be updated independently of the model.  \n",
    "- **Resource Efficiency**: Avoids full model retraining, saving time and compute.  \n",
    "- **Semantic Search**: Retrieves information based on meaning rather than exact words, improving relevance.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **LLM Frameworks and Tools**\n",
    "LangChain, LlamaIndex, Haystack\n",
    "## **Advanced Prompting Techniques**\n",
    "- Preamble/context for style and tone\n",
    "- Self-consistency checks and iterative reasoning\n",
    "## **Model Safety and Bias Mitigation**\n",
    "- Groundedness / factuality\n",
    "- Prompt injection mitigation\n",
    "- Bias sources and mitigation strategies\n",
    "- Fact-checking and auditing pipelines\n",
    "## **Memory and State Handling**\n",
    "- Session management / context retention\n",
    "- Session timeout and ephemeral context\n",
    "## **Multi-modal LLMs**\n",
    "- Text + Image\n",
    "- Text + Audio\n",
    "- Text + video\n",
    "\n",
    "---\n",
    "\n",
    "## **Bias and Fairness**\n",
    "## **Hallucinations and Incorrect Outputs**\n",
    "\n",
    "# **Generative AI for Images and Multimedia**\n",
    "## **Image Generation Models (DALL·E, Stable Diffusion, etc.)**\n",
    "## **Audio and Music Generation**\n",
    "## **Video Generation**\n",
    "\n",
    "---\n",
    "\n",
    "# **Tools and Platforms**\n",
    "## **Libraries and Frameworks (Hugging Face, OpenAI, TensorFlow, etc.)**\n",
    "## **Deployment Tools and APIs**\n",
    "- Vector stores and database integration\n",
    "- On-demand vs batch inference\n",
    "## **No-code / Low-code Interfaces**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006cab4",
   "metadata": {
    "papermill": {
     "duration": 0.001973,
     "end_time": "2025-11-20T22:56:49.965916",
     "exception": false,
     "start_time": "2025-11-20T22:56:49.963943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **OCI Generative AI**\n",
    "\n",
    "**Oracle Cloud Infrastructure (OCI)** is a comprehensive cloud platform that provides computing, storage, networking, and data management services for enterprises and developers. It offers a range of infrastructure-as-a-service (IaaS), platform-as-a-service (PaaS), and software-as-a-service (SaaS) solutions, designed to support high-performance applications and scalable workloads.  \n",
    "\n",
    "OCI emphasizes security, reliability, and performance, with features such as dedicated compute clusters, encrypted storage, and advanced networking options. The platform also includes specialized services for artificial intelligence and machine learning, allowing developers to build, train, and deploy AI models with integrated tools and resources.\n",
    "\n",
    "---\n",
    "\n",
    "## OCI Generative AI Service\n",
    "\n",
    "OCI Generative AI Service provides pre-trained foundation models, primarily chat models that can be used in on-demand serving mode. Model endpoints act as the designated points where user requests are sent and responses are received. When using on-demand inference, customers are charged per character processed, which makes it suitable for applications with variable or moderate usage.  \n",
    "\n",
    "Session-enabled endpoints allow the chat context to be retained during the session, helping maintain continuity in conversations. These sessions automatically end after a specified timeout period.  \n",
    "\n",
    "The service also offers a citation option, which displays the source details for each chat response, providing transparency and traceability. Content moderation can be enabled during endpoint creation to ensure that generated content complies with safety and policy requirements.\n",
    "\n",
    "### Types of Pre-trained Models\n",
    "- **Chat models:** For interactive text-based conversations.\n",
    "- **Embedding models:** For semantic search, similarity, or vector-based retrieval.\n",
    "- **Image generation models:** For text-to-image tasks (if enabled in OCI).  \n",
    "- **Other specialized models:** Depending on service updates, e.g., summarization or code generation.\n",
    "- \n",
    "---\n",
    "\n",
    "## Fine-Tuning and Custom Models\n",
    "\n",
    "OCI allows fine-tuning of AI models using TCU methods, which are particularly suitable for large datasets ranging from hundreds of thousands to millions of samples.  \n",
    "\n",
    "**Training Compute Unit (TCU)** is Oracle's unit of measurement for the computational resources allocated to model training and fine-tuning. Each TCU represents a portion of GPU or TPU resources, memory, and compute capacity, allowing predictable scaling of training workloads. Using TCUs, developers can estimate training time, optimize resource usage, and manage costs effectively.  \n",
    "\n",
    "When preparing a custom dataset for fine-tuning, it is important to organize the data into separate files for training and validation to ensure proper evaluation and prevent data leakage.\n",
    "\n",
    "Once a model has been fine-tuned, it is stored securely in OCI Object Storage, with encryption enabled by default to protect customer data. Dedicated AI clusters provide predictable pricing that does not fluctuate with demand, and they help minimize GPU or TPU memory overhead by sharing base model weights across multiple fine-tuned models. These clusters also enable the deployment of several fine-tuned models within a single infrastructure, improving efficiency and resource utilization.\n",
    "\n",
    "---\n",
    "\n",
    "## Prompting and RAG in OCI\n",
    "\n",
    "In OCI, deciding between prompting and training depends on the goal. Prompting is best used when you want the model to emphasize specific vocabulary or phrases, such as product names, during responses. Fine-tuning, on the other hand, is more suitable for improving the model's understanding of broader domain-specific terminology across multiple tasks.  \n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** offers a way to enhance model responses without the costs of full model training. Unlike traditional fine-tuning, RAG does not modify the model weights; instead, it augments generation with relevant information retrieved from external documents. This approach reduces hallucinations and ensures that responses are grounded in factual sources.  \n",
    "\n",
    "The typical RAG pipeline consists of three main stages:\n",
    "\n",
    "1. **Document Embedding:** External documents are converted into vector representations (embeddings) that capture their semantic meaning.  \n",
    "2. **Indexing and Retrieval:** The embeddings are stored in a vector database or index. When a query is issued, the system retrieves the most relevant documents by comparing the query embedding with the stored vectors.  \n",
    "3. **Generation:** The LLM uses the retrieved documents as context to generate responses, combining its language understanding with factual information from the sources.  \n",
    "\n",
    "**Key points about RAG:**\n",
    "- It simplifies setup compared to full fine-tuning, avoiding the need for large-scale training datasets and TCU usage.  \n",
    "- Operational complexity still exists: the retrieval and embedding pipeline must be carefully configured, including document preprocessing, vector indexing, and relevance tuning.  \n",
    "- RAG works well with prompting: prompts can still guide style, emphasis, or vocabulary, while retrieval provides grounded content.  \n",
    "- This method allows fine control over sources used, enabling auditing, bias mitigation, and transparent content generation.  \n",
    "\n",
    "Overall, RAG strikes a balance between **flexibility, grounding, and practical implementation**, making it a preferred choice when you want reliable, fact-based outputs without the overhead of fine-tuning the model itself.\n",
    "\n",
    "---\n",
    "\n",
    "## Vector Search and Database Integration\n",
    "\n",
    "Before executing any code related to vector search in OCI, embeddings must first be created and stored in the database. These embeddings serve as the foundation for building vector stores, which allow the model to efficiently search and retrieve relevant information from large datasets. Using vector store code makes it possible to create these stores directly from database tables of embeddings, simplifying integration with Oracle Database AI services.  \n",
    "\n",
    "### Vector Store Creation and Management\n",
    "- **Creating a vector store:** Embeddings generated from documents or data are indexed in a vector database (e.g., OCI Object Storage + indexing service) to enable fast similarity search.  \n",
    "- **Updating and maintaining:** When new data is added or documents change, embeddings need to be updated in the vector store to maintain search accuracy.  \n",
    "- **Querying:** When a user query is issued, it is converted into an embedding and compared against the stored vectors to retrieve the most relevant documents.\n",
    "\n",
    "### Importance for RAG\n",
    "- Vector stores are a critical component of **Retrieval-Augmented Generation (RAG)** pipelines. The retrieved vectors provide the LLM with grounded, contextually relevant information to generate accurate responses.  \n",
    "- Proper vector store management ensures minimal retrieval errors, reduces hallucinations, and allows fine control over the documents and sources used during generation.\n",
    "\n",
    "When integrating vector search functionality with an Oracle Database:\n",
    "- The **core field** in the vector search results represents the distance between the query vector and the corresponding body vector, indicating similarity or relevance.  \n",
    "- **Network configuration:** Subnet ingress rules must be correctly set, and the source type for the Oracle database should be CIDR to ensure secure access.\n",
    "\n",
    "---\n",
    "\n",
    "## Knowledge Bases\n",
    "\n",
    "OCI Generative AI Agents can connect to knowledge bases, which are structured collections of documents or data used to provide contextually accurate responses.  \n",
    "\n",
    "- **Ingestion:** Documents are ingested into the knowledge base and optionally converted into embeddings for efficient retrieval.  \n",
    "- **Failure handling:** If an ingestion job fails partially, only the failed and updated files are re-ingested.  \n",
    "- **Deletion:** Deleting a knowledge base is permanent and cannot be undone.  \n",
    "- **Integration with vector search:** Knowledge bases can leverage embeddings and vector stores to enhance retrieval, which is critical for RAG pipelines.  \n",
    "- **Endpoint connection:** Agents can query knowledge bases through model endpoints to provide grounded, fact-based responses.\n",
    "\n",
    "---\n",
    "\n",
    "## Session Management in Agents\n",
    "\n",
    "OCI Generative AI Agents include session management features that track the conversational history, including both user prompts and model responses. This allows the agent to maintain context across interactions, which is essential for creating coherent and continuous conversations.  \n",
    "\n",
    "When session-enabled endpoints are used, the context is retained for the duration of the session. However, sessions have a timeout, after which the context is automatically cleared. Additionally, if an ingestion job processes multiple files and some fail, only the failed and subsequently updated files are re-ingested when the job is restarted, ensuring efficient data processing without duplicating work.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration and Code\n",
    "\n",
    "Configuring OCI Generative AI services requires understanding key code settings as well as operational features of endpoints and session handling.\n",
    "\n",
    "- **OCI Configuration File:**  \n",
    "  The line `oci.config.from_file()` loads OCI configuration details from a local file, allowing the system to authenticate and connect to the cloud environment.\n",
    "\n",
    "- **Serving Mode:**  \n",
    "  For chat models, setting `ChatDetail.ServingMode = OnDemandServingMode` ensures that the model operates in on-demand inference mode, generating responses only when requested and providing flexibility in resource usage.\n",
    "\n",
    "- **Embeddings Truncation:**  \n",
    "  When working with embeddings, the setting `EmbedTextDetail.truncate = none` disables truncation of the input text, ensuring the full content is used for generating embeddings. This is particularly important for long texts to preserve context and information for vector search or retrieval tasks.\n",
    "\n",
    "- **Model Endpoints:**  \n",
    "  Endpoints are the designated points where user requests are sent and model responses are received.  \n",
    "  - **Session-enabled endpoints:** Retain the conversation context for the duration of the session.  \n",
    "  - **Session timeout:** After the specified timeout, the context is cleared automatically.  \n",
    "  - **Immutable session option:** Once enabled, the session option cannot be changed.\n",
    "\n",
    "- **Citation Option:**  \n",
    "  Enabling citation displays source details for each chat response, improving transparency and traceability of the model's outputs.\n",
    "\n",
    "- **Content Moderation:**  \n",
    "  Content moderation ensures that generated outputs comply with safety policies and regulations. It must be enabled when creating the endpoint.\n",
    "\n",
    "This configuration and operational setup ensures that developers can manage authentication, model behavior, session context, and output transparency effectively while using OCI Generative AI.\n",
    "\n",
    "---\n",
    "\n",
    "## Text Processing and Embeddings\n",
    "\n",
    "When working with text in OCI Generative AI, processing it properly is essential for generating accurate embeddings and enabling efficient retrieval. Text is typically divided into paragraphs, which are then split into sequences and further broken down into tokens until the desired chunk size is reached. This ensures that each piece of text can be processed by the model without exceeding token limits.  \n",
    "\n",
    "Handling token limits is particularly important when dealing with long documents, as exceeding the model's capacity can result in incomplete embeddings. In cases where only certain parts of a document are most relevant, a truncation strategy can be applied. For example, if the most important information is at the beginning, the end of the text may be truncated to fit within the token limit while preserving key content.\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-modal and Advanced Models\n",
    "\n",
    "OCI Generative AI supports advanced multi-modal capabilities, allowing models to handle different types of input and output beyond plain text. Diffusion models, for example, are used to generate complex outputs such as converting text descriptions into images. These models iteratively refine the output, producing high-quality and detailed results suitable for applications in design, marketing, and creative content generation.  \n",
    "\n",
    "The **Cohere Embed V3** model represents an improvement over its predecessors in several ways. It provides higher-dimensional embeddings that capture semantic meaning more accurately, which improves performance in tasks like semantic search, clustering, and retrieval. This enhanced precision allows the model to better understand textual nuances and relationships, making similarity searches and document retrieval more reliable. Compared to previous versions, Cohere Embed V3 is optimized for **speed, accuracy, and generalization**, enabling developers to build applications that require high-quality semantic understanding across large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854420e6",
   "metadata": {
    "papermill": {
     "duration": 0.00188,
     "end_time": "2025-11-20T22:56:49.969921",
     "exception": false,
     "start_time": "2025-11-20T22:56:49.968041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Useful resources\n",
    "\n",
    "### **Youtube**\n",
    "\n",
    "*Builders Central*: Retrieval-augmented generation (RAG), Clearly Explained (Why it Matters)\n",
    "https://www.youtube.com/watch?v=VioF7v8Mikg\n",
    "\n",
    "### **Websites**\n",
    "\n",
    "### **Courses**\n",
    "\n",
    "*Oracle*: OCI Generative AI Professional (2025)\n",
    "https://mylearn.oracle.com/ou/learning-path/become-an-oci-generative-ai-professional-2025/147863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44622ae",
   "metadata": {
    "papermill": {
     "duration": 0.001951,
     "end_time": "2025-11-20T22:56:49.973928",
     "exception": false,
     "start_time": "2025-11-20T22:56:49.971977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.996289,
   "end_time": "2025-11-20T22:56:50.396809",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-20T22:56:44.400520",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
