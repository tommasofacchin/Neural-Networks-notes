{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed0b531",
   "metadata": {
    "papermill": {
     "duration": 0.002179,
     "end_time": "2025-10-27T13:37:02.063685",
     "exception": false,
     "start_time": "2025-10-27T13:37:02.061506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "## Introduction to Supervised Learning\n",
    "\n",
    "Supervised learning is a type of machine learning where we teach a model to map inputs to known outputs. The algorithm learns from examples (input-output pairs) and then predicts outcomes for new, unseen data.\n",
    "\n",
    "In practice, supervised learning uses **labeled data**—each example comes with a correct answer. The model adjusts its parameters as it sees more data, learning the relationship between features and labels. This explicit guidance helps the model make accurate predictions on new inputs.\n",
    "\n",
    "Supervised learning is widely used in real-world problems, such as:  \n",
    "- Email spam detection  \n",
    "- Stock price prediction  \n",
    "- Customer churn prediction  \n",
    "\n",
    "It is especially useful when you want to build models that are highly accurate and can generalize well to new data.\n",
    "\n",
    "Main points:  \n",
    "- Predict a target variable (y) from input features (X).  \n",
    "- Targets can be **continuous** (regression) or **categorical** (classification).  \n",
    "- Difference from other learning types:  \n",
    "  - **Unsupervised:** find patterns without labels  \n",
    "  - **Reinforcement:** learn by interacting with an environment\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Fundamental Concepts\n",
    "\n",
    "Before working with models, some basics to keep in mind:  \n",
    "\n",
    "- **Dataset:** collection of samples with features (X) and targets (y)  \n",
    "- **Train vs Test:** train to learn, test to evaluate generalization  \n",
    "- **Overfitting / Underfitting:** too complex vs too simple models  \n",
    "- **Bias-Variance tradeoff:** balancing simplicity and flexibility  \n",
    "- **Model evaluation:** use metrics like MSE, R², accuracy, or F1-score; cross-validation helps estimate performance reliably\n",
    "\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "Data preprocessing is a critical step in any machine learning workflow. Raw data is often messy, inconsistent, or incomplete, and models trained directly on such data can perform poorly or produce biased results. Preprocessing ensures that the dataset is clean, structured, and in a form suitable for the algorithms we want to use.  \n",
    "\n",
    "The first step is **data cleaning**, which involves identifying and correcting errors, inconsistencies, or duplicate entries. Missing values are common in real-world datasets, and handling them appropriately is crucial. Depending on the context, missing data can be removed, imputed using statistics like the mean or median, or predicted using more advanced methods such as k-nearest neighbors or regression models.  \n",
    "\n",
    "Another important consideration is **scaling and normalization**. Many algorithms, especially those based on distances (like KNN or SVM) or gradient-based optimization, are sensitive to the scale of input features. Standardization transforms features to have zero mean and unit variance, while normalization rescales data to a fixed range. Choosing the right scaling method can significantly affect model convergence and performance.  \n",
    "\n",
    "**Categorical variables** require special treatment because most machine learning algorithms only process numerical data. Common approaches include **label encoding**, which assigns an integer to each category, and **one-hot encoding**, which creates binary columns for each category. More sophisticated encodings, such as target encoding or frequency encoding, can capture additional information while avoiding introducing spurious orderings.  \n",
    "\n",
    "Finally, **feature engineering and selection** are essential for improving model accuracy and interpretability. Feature engineering involves creating new features or transforming existing ones to better capture underlying patterns. Feature selection helps reduce dimensionality, remove irrelevant or redundant features, and mitigate overfitting. Techniques range from simple correlation analysis to more advanced methods like recursive feature elimination or model-based importance scores.  \n",
    "\n",
    "Overall, careful and thoughtful data preprocessing forms the foundation of effective machine learning models. Without it, even the most sophisticated algorithms may fail to deliver reliable results.\n",
    "\n",
    "\n",
    "## Regression\n",
    "### Linear Regression\n",
    "\n",
    "Linear regression is one of the most fundamental and widely used techniques in supervised learning. It models the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data. The main idea is to predict the target as a weighted sum of the input features plus an intercept term.\n",
    "\n",
    "Mathematically, the model can be expressed as:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
    "$$\n",
    "\n",
    "Here, $\\beta_0$ is the intercept, $\\beta_1, \\dots, \\beta_n$ are the coefficients representing the influence of each feature on the target, and $\\epsilon$ is the error term capturing the difference between observed and predicted values. The coefficients can be interpreted as the expected change in the target variable for a one-unit change in the corresponding feature, assuming all other features remain constant.\n",
    "\n",
    "Parameter estimation is usually performed using **Ordinary Least Squares (OLS)**, which finds the coefficients that minimize the sum of squared differences between the predicted and actual target values:\n",
    "\n",
    "$$\n",
    "\\text{Minimize } \\sum_{i=1}^m (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "This ensures the best possible linear fit to the training data in terms of squared error.\n",
    "\n",
    "Linear regression relies on several key **assumptions** that ensure the model produces valid, interpretable, and reliable results. Understanding these assumptions is crucial because if they are violated, the predictions, inference, or coefficient interpretations can be misleading.\n",
    "\n",
    "- **Linearity**: This assumption states that the relationship between each feature and the target variable is linear. In other words, the expected change in the target is proportional to a change in the feature. If the true relationship is non-linear, linear regression may underfit the data, leading to biased predictions. Visualizing scatter plots of each feature against the target can help detect non-linearity. \n",
    "- **Independence**: Observations should be independent of each other. This means that the value of one observation does not influence another. Violations occur in time series data or grouped data where correlations exist between samples. Ignoring dependence can result in underestimated standard errors and misleading significance tests. \n",
    "- **Homoscedasticity**: The variance of residuals (errors) should be constant across all levels of the features. If residuals systematically increase or decrease with the predicted values (heteroscedasticity), the model may be less efficient and confidence intervals or hypothesis tests can be inaccurate. Plotting residuals versus predicted values is a common way to check for this.\n",
    "- **Normality of residuals**: The residuals should follow a normal distribution. This assumption is especially important for conducting inference, such as calculating confidence intervals or p-values for coefficients. Even if predictions can still be reasonably accurate, non-normal residuals can invalidate hypothesis testing. Histograms or Q-Q plots of residuals are often used to check normality. \n",
    "- **No multicollinearity**: Features should not be highly correlated with each other. When multicollinearity exists, it becomes difficult to isolate the individual effect of each feature on the target, coefficients can become unstable, and small changes in the data can lead to large changes in estimates. Correlation matrices or variance inflation factors (VIF) are commonly used to detect multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "Evaluating a linear regression model is essential to understand how well it predicts the target variable and how much of the underlying variability it captures. Unlike classification problems where metrics like accuracy or F1-score are used, regression requires measures that quantify the **difference between predicted and actual values**, as well as the overall explanatory power of the model.\n",
    "\n",
    "One of the most common metrics is the **Mean Squared Error (MSE)**, which calculates the average of the squared differences between predicted and actual values:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Here, $y_i$ represents the actual target value, $\\hat{y}_i$ is the predicted value, and $m$ is the number of samples. Squaring the errors penalizes larger deviations more heavily, making MSE sensitive to outliers. A lower MSE indicates better predictive performance.\n",
    "\n",
    "The **Root Mean Squared Error (RMSE)** is simply the square root of the MSE:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "RMSE is often preferred because it is expressed in the same units as the target variable, making it easier to interpret and compare with the scale of the data.\n",
    "\n",
    "Another important metric is the **Coefficient of Determination ($R^2$)**:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^m (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "$R^2$ measures the proportion of variance in the target variable that is explained by the model. A value of $R^2 = 1$ indicates a perfect fit, whereas $R^2 = 0$ suggests that the model does no better than predicting the mean of the target. Negative values can occur when the model performs worse than simply predicting the mean.\n",
    "\n",
    "Together, these metrics give a **comprehensive view** of model performance: MSE and RMSE focus on prediction errors, while $R^2$ provides insight into how well the model captures the underlying structure of the data. By analyzing these metrics, we can assess not only how accurate our predictions are but also how much of the variability in the target is being explained.\n",
    "\n",
    "In practice, these evaluation measures guide decisions on model selection, hyperparameter tuning, and whether further feature engineering or preprocessing is needed. Even though linear regression is conceptually simple, mastering these metrics is crucial for building a solid foundation before moving on to more complex supervised learning models.\n",
    "\n",
    "\n",
    "\n",
    "### Polynomial Regression\n",
    "- Using polynomials for non-linear models\n",
    "- Overfitting and regularization\n",
    "\n",
    "### Regularized Regression\n",
    "- Ridge regression (L2)\n",
    "- Lasso regression (L1)\n",
    "- Elastic Net\n",
    "\n",
    "### Other Regression Models\n",
    "- Support Vector Regression (SVR)\n",
    "- Decision Tree Regressor\n",
    "- Random Forest Regressor\n",
    "- Gradient Boosting Regressor\n",
    "\n",
    "## Classification\n",
    "### Binary Classification\n",
    "- Logistic Regression\n",
    "- Interpretation of coefficients\n",
    "- Sigmoid function\n",
    "- Decision boundary\n",
    "\n",
    "### Multiclass Classification\n",
    "- One-vs-Rest\n",
    "- Softmax\n",
    "- Multinomial Logistic Regression\n",
    "\n",
    "### Classification Algorithms\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Support Vector Machine (SVM)\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Gradient Boosting Classifier (XGBoost, LightGBM, CatBoost)\n",
    "- Naive Bayes\n",
    "- Neural Networks for classification\n",
    "\n",
    "## Model Evaluation\n",
    "### Regression Metrics\n",
    "- MSE, RMSE, MAE\n",
    "- R² (Coefficient of determination)\n",
    "- Adjusted R²\n",
    "\n",
    "### Classification Metrics\n",
    "- Accuracy\n",
    "- Precision, Recall, F1-score\n",
    "- Confusion Matrix\n",
    "- ROC curve and AUC\n",
    "- Log-loss\n",
    "- Matthews Correlation Coefficient (MCC)\n",
    "- Cohen’s Kappa\n",
    "- Balanced accuracy\n",
    "- F-beta score\n",
    "\n",
    "### Cross-validation\n",
    "- K-Fold Cross Validation\n",
    "- Leave-One-Out Cross Validation\n",
    "- Stratified K-Fold\n",
    "\n",
    "## Advanced Techniques\n",
    "- Ensemble methods\n",
    "  - Bagging (Random Forest)\n",
    "  - Boosting (AdaBoost, Gradient Boosting)\n",
    "  - Stacking\n",
    "- Feature importance\n",
    "- Hyperparameter tuning\n",
    "  - Grid Search\n",
    "  - Random Search\n",
    "  - Bayesian Optimization\n",
    "- Learning curves and validation curves\n",
    "\n",
    "## Advanced Topics / Algorithms\n",
    "- Probabilistic models:\n",
    "  - Bayesian regression\n",
    "  - Gaussian Naive Bayes\n",
    "- Distance-based methods beyond KNN:\n",
    "  - Metric learning concepts\n",
    "- Tree-based advanced techniques:\n",
    "  - Extra Trees\n",
    "  - Gradient Boosting variants (CatBoost specifics)\n",
    "- Neural networks for tabular data (basic MLP)\n",
    "- Calibration of probabilistic classifiers\n",
    "\n",
    "## Practical Data Handling\n",
    "- Feature transformation:\n",
    "  - Log transformation, polynomial features\n",
    "  - Interaction terms\n",
    "- Categorical variable encoding advanced:\n",
    "  - Target encoding\n",
    "  - Frequency encoding\n",
    "- Handling missing data advanced:\n",
    "  - Imputation techniques (mean, median, k-NN, MICE)\n",
    "- Data leakage prevention\n",
    "- Pipeline automation (scikit-learn pipelines)\n",
    "\n",
    "## Evaluation / Metrics\n",
    "- Learning curves (training vs validation performance)\n",
    "- Validation curves (hyperparameter impact)\n",
    "- Bootstrapping and Monte Carlo evaluation\n",
    "- Nested Cross-validation\n",
    "- Confounding variables and collinearity\n",
    "- Concept drift handling\n",
    "\n",
    "## Practical Considerations\n",
    "- Imbalanced datasets\n",
    "  - Oversampling (SMOTE)\n",
    "  - Undersampling\n",
    "- Handling outliers\n",
    "- Model interpretability\n",
    "- Model deployment\n",
    "- Scalability and optimization\n",
    "\n",
    "## Applications\n",
    "- Sales prediction (regression)\n",
    "- Image or text classification\n",
    "- Fraud detection\n",
    "- Churn prediction\n",
    "- Medical diagnosis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0609d09",
   "metadata": {
    "papermill": {
     "duration": 0.001497,
     "end_time": "2025-10-27T13:37:02.066937",
     "exception": false,
     "start_time": "2025-10-27T13:37:02.065440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "## Introduction to Unsupervised Learning\n",
    "- Definition of unsupervised learning\n",
    "- Difference between supervised, unsupervised, and reinforcement learning\n",
    "- Goal: find patterns, structures, or groupings in data\n",
    "- No target variable (y)\n",
    "- Common tasks:\n",
    "  - Clustering\n",
    "  - Dimensionality reduction\n",
    "  - Anomaly detection\n",
    "\n",
    "## Fundamental Concepts\n",
    "- Dataset and features\n",
    "- Distance and similarity measures\n",
    "  - Euclidean distance\n",
    "  - Manhattan distance\n",
    "  - Cosine similarity\n",
    "  - Correlation\n",
    "- Overfitting and underfitting in unsupervised learning\n",
    "- Evaluation challenges (lack of ground truth)\n",
    "\n",
    "## Data Preprocessing\n",
    "- Data cleaning\n",
    "- Handling missing values\n",
    "- Normalization and standardization\n",
    "- Encoding categorical variables\n",
    "- Feature scaling\n",
    "- Feature selection and feature engineering\n",
    "- Dimensionality reduction before clustering (optional)\n",
    "\n",
    "## Clustering\n",
    "### Partitioning Methods\n",
    "- K-Means\n",
    "  - Algorithm overview\n",
    "  - Choosing number of clusters (elbow method, silhouette score)\n",
    "  - Limitations: sensitive to initialization, outliers\n",
    "- K-Medoids / PAM\n",
    "- Mini-Batch K-Means\n",
    "\n",
    "### Hierarchical Clustering\n",
    "- Agglomerative clustering\n",
    "  - Linkage methods: single, complete, average, ward\n",
    "- Divisive clustering\n",
    "- Dendrogram visualization\n",
    "\n",
    "### Density-Based Clustering\n",
    "- DBSCAN\n",
    "- OPTICS\n",
    "- HDBSCAN\n",
    "\n",
    "### Model-Based Clustering\n",
    "- Gaussian Mixture Models (GMM)\n",
    "- Expectation-Maximization algorithm\n",
    "- Choosing number of components (BIC/AIC)\n",
    "\n",
    "### Other Clustering Techniques\n",
    "- Spectral Clustering\n",
    "- Self-Organizing Maps (SOM)\n",
    "- Mean-Shift clustering\n",
    "\n",
    "## Dimensionality Reduction\n",
    "- Principal Component Analysis (PCA)\n",
    "- Kernel PCA\n",
    "- Independent Component Analysis (ICA)\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- Uniform Manifold Approximation and Projection (UMAP)\n",
    "- Linear Discriminant Analysis (LDA, supervised variant)\n",
    "\n",
    "## Anomaly Detection\n",
    "- Z-score and statistical methods\n",
    "- Isolation Forest\n",
    "- One-Class SVM\n",
    "- Local Outlier Factor (LOF)\n",
    "- Autoencoder-based anomaly detection\n",
    "\n",
    "## Evaluation of Unsupervised Models\n",
    "- Internal metrics:\n",
    "  - Silhouette score\n",
    "  - Davies-Bouldin index\n",
    "  - Calinski-Harabasz index\n",
    "- External metrics (if ground truth available):\n",
    "  - Adjusted Rand Index (ARI)\n",
    "  - Normalized Mutual Information (NMI)\n",
    "  - Fowlkes-Mallows score\n",
    "- Visual inspection:\n",
    "  - Scatter plots, cluster plots\n",
    "  - Heatmaps\n",
    "\n",
    "## Advanced Techniques\n",
    "- Ensemble clustering\n",
    "- Consensus clustering\n",
    "- Subspace clustering\n",
    "- Feature learning with autoencoders\n",
    "- Self-supervised learning (modern approach)\n",
    "\n",
    "## Practical Considerations\n",
    "- Choosing the right number of clusters/components\n",
    "- Handling high-dimensional data\n",
    "- Handling categorical features\n",
    "- Handling outliers\n",
    "- Scaling for large datasets\n",
    "- Interpretability of clusters or latent features\n",
    "\n",
    "## Applications\n",
    "- Customer segmentation\n",
    "- Market basket analysis\n",
    "- Anomaly/fraud detection\n",
    "- Image compression or embedding\n",
    "- Topic modeling in text\n",
    "- Dimensionality reduction for visualization\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.0442,
   "end_time": "2025-10-27T13:37:02.486910",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-27T13:36:57.442710",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
