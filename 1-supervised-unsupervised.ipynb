{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec8579c",
   "metadata": {
    "papermill": {
     "duration": 0.002114,
     "end_time": "2025-10-27T13:14:12.272477",
     "exception": false,
     "start_time": "2025-10-27T13:14:12.270363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "## Introduction to Supervised Learning\n",
    "- Definition of supervised learning\n",
    "- Difference between supervised, unsupervised, and reinforcement learning\n",
    "- Goal: predict a target variable from input/features\n",
    "- Variables: features (X) and target (y)\n",
    "- Types of target:\n",
    "  - Continuous → regression\n",
    "  - Categorical → classification\n",
    "\n",
    "## Fundamental Concepts\n",
    "- Dataset and samples\n",
    "- Training set vs Test set\n",
    "- Overfitting and underfitting\n",
    "- Bias and Variance\n",
    "- Model evaluation\n",
    "\n",
    "## Data Preprocessing\n",
    "- Data cleaning\n",
    "- Handling missing values\n",
    "- Normalization and standardization\n",
    "- Encoding categorical variables:\n",
    "  - One-hot encoding\n",
    "  - Label encoding\n",
    "- Feature scaling\n",
    "- Feature selection and feature engineering\n",
    "\n",
    "## Regression\n",
    "### Linear Regression\n",
    "- Formula and interpretation of coefficients\n",
    "- Parameter estimation with OLS (Ordinary Least Squares)\n",
    "- Assumptions of linear regression\n",
    "- Evaluation metrics: MSE, RMSE, R²\n",
    "\n",
    "### Polynomial Regression\n",
    "- Using polynomials for non-linear models\n",
    "- Overfitting and regularization\n",
    "\n",
    "### Regularized Regression\n",
    "- Ridge regression (L2)\n",
    "- Lasso regression (L1)\n",
    "- Elastic Net\n",
    "\n",
    "### Other Regression Models\n",
    "- Support Vector Regression (SVR)\n",
    "- Decision Tree Regressor\n",
    "- Random Forest Regressor\n",
    "- Gradient Boosting Regressor\n",
    "\n",
    "## Classification\n",
    "### Binary Classification\n",
    "- Logistic Regression\n",
    "- Interpretation of coefficients\n",
    "- Sigmoid function\n",
    "- Decision boundary\n",
    "\n",
    "### Multiclass Classification\n",
    "- One-vs-Rest\n",
    "- Softmax\n",
    "- Multinomial Logistic Regression\n",
    "\n",
    "### Classification Algorithms\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Support Vector Machine (SVM)\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Gradient Boosting Classifier (XGBoost, LightGBM, CatBoost)\n",
    "- Naive Bayes\n",
    "- Neural Networks for classification\n",
    "\n",
    "## Model Evaluation\n",
    "### Regression Metrics\n",
    "- MSE, RMSE, MAE\n",
    "- R² (Coefficient of determination)\n",
    "- Adjusted R²\n",
    "\n",
    "### Classification Metrics\n",
    "- Accuracy\n",
    "- Precision, Recall, F1-score\n",
    "- Confusion Matrix\n",
    "- ROC curve and AUC\n",
    "- Log-loss\n",
    "- Matthews Correlation Coefficient (MCC)\n",
    "- Cohen’s Kappa\n",
    "- Balanced accuracy\n",
    "- F-beta score\n",
    "\n",
    "### Cross-validation\n",
    "- K-Fold Cross Validation\n",
    "- Leave-One-Out Cross Validation\n",
    "- Stratified K-Fold\n",
    "\n",
    "## Advanced Techniques\n",
    "- Ensemble methods\n",
    "  - Bagging (Random Forest)\n",
    "  - Boosting (AdaBoost, Gradient Boosting)\n",
    "  - Stacking\n",
    "- Feature importance\n",
    "- Hyperparameter tuning\n",
    "  - Grid Search\n",
    "  - Random Search\n",
    "  - Bayesian Optimization\n",
    "- Learning curves and validation curves\n",
    "\n",
    "## Advanced Topics / Algorithms\n",
    "- Probabilistic models:\n",
    "  - Bayesian regression\n",
    "  - Gaussian Naive Bayes\n",
    "- Distance-based methods beyond KNN:\n",
    "  - Metric learning concepts\n",
    "- Tree-based advanced techniques:\n",
    "  - Extra Trees\n",
    "  - Gradient Boosting variants (CatBoost specifics)\n",
    "- Neural networks for tabular data (basic MLP)\n",
    "- Calibration of probabilistic classifiers\n",
    "\n",
    "## Practical Data Handling\n",
    "- Feature transformation:\n",
    "  - Log transformation, polynomial features\n",
    "  - Interaction terms\n",
    "- Categorical variable encoding advanced:\n",
    "  - Target encoding\n",
    "  - Frequency encoding\n",
    "- Handling missing data advanced:\n",
    "  - Imputation techniques (mean, median, k-NN, MICE)\n",
    "- Data leakage prevention\n",
    "- Pipeline automation (scikit-learn pipelines)\n",
    "\n",
    "## Evaluation / Metrics\n",
    "- Learning curves (training vs validation performance)\n",
    "- Validation curves (hyperparameter impact)\n",
    "- Bootstrapping and Monte Carlo evaluation\n",
    "- Nested Cross-validation\n",
    "- Confounding variables and collinearity\n",
    "- Concept drift handling\n",
    "\n",
    "## Practical Considerations\n",
    "- Imbalanced datasets\n",
    "  - Oversampling (SMOTE)\n",
    "  - Undersampling\n",
    "- Handling outliers\n",
    "- Model interpretability\n",
    "- Model deployment\n",
    "- Scalability and optimization\n",
    "\n",
    "## Applications\n",
    "- Sales prediction (regression)\n",
    "- Image or text classification\n",
    "- Fraud detection\n",
    "- Churn prediction\n",
    "- Medical diagnosis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ea85d",
   "metadata": {
    "papermill": {
     "duration": 0.001626,
     "end_time": "2025-10-27T13:14:12.276075",
     "exception": false,
     "start_time": "2025-10-27T13:14:12.274449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "## Introduction to Unsupervised Learning\n",
    "- Definition of unsupervised learning\n",
    "- Difference between supervised, unsupervised, and reinforcement learning\n",
    "- Goal: find patterns, structures, or groupings in data\n",
    "- No target variable (y)\n",
    "- Common tasks:\n",
    "  - Clustering\n",
    "  - Dimensionality reduction\n",
    "  - Anomaly detection\n",
    "\n",
    "## Fundamental Concepts\n",
    "- Dataset and features\n",
    "- Distance and similarity measures\n",
    "  - Euclidean distance\n",
    "  - Manhattan distance\n",
    "  - Cosine similarity\n",
    "  - Correlation\n",
    "- Overfitting and underfitting in unsupervised learning\n",
    "- Evaluation challenges (lack of ground truth)\n",
    "\n",
    "## Data Preprocessing\n",
    "- Data cleaning\n",
    "- Handling missing values\n",
    "- Normalization and standardization\n",
    "- Encoding categorical variables\n",
    "- Feature scaling\n",
    "- Feature selection and feature engineering\n",
    "- Dimensionality reduction before clustering (optional)\n",
    "\n",
    "## Clustering\n",
    "### Partitioning Methods\n",
    "- K-Means\n",
    "  - Algorithm overview\n",
    "  - Choosing number of clusters (elbow method, silhouette score)\n",
    "  - Limitations: sensitive to initialization, outliers\n",
    "- K-Medoids / PAM\n",
    "- Mini-Batch K-Means\n",
    "\n",
    "### Hierarchical Clustering\n",
    "- Agglomerative clustering\n",
    "  - Linkage methods: single, complete, average, ward\n",
    "- Divisive clustering\n",
    "- Dendrogram visualization\n",
    "\n",
    "### Density-Based Clustering\n",
    "- DBSCAN\n",
    "- OPTICS\n",
    "- HDBSCAN\n",
    "\n",
    "### Model-Based Clustering\n",
    "- Gaussian Mixture Models (GMM)\n",
    "- Expectation-Maximization algorithm\n",
    "- Choosing number of components (BIC/AIC)\n",
    "\n",
    "### Other Clustering Techniques\n",
    "- Spectral Clustering\n",
    "- Self-Organizing Maps (SOM)\n",
    "- Mean-Shift clustering\n",
    "\n",
    "## Dimensionality Reduction\n",
    "- Principal Component Analysis (PCA)\n",
    "- Kernel PCA\n",
    "- Independent Component Analysis (ICA)\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- Uniform Manifold Approximation and Projection (UMAP)\n",
    "- Linear Discriminant Analysis (LDA, supervised variant)\n",
    "\n",
    "## Anomaly Detection\n",
    "- Z-score and statistical methods\n",
    "- Isolation Forest\n",
    "- One-Class SVM\n",
    "- Local Outlier Factor (LOF)\n",
    "- Autoencoder-based anomaly detection\n",
    "\n",
    "## Evaluation of Unsupervised Models\n",
    "- Internal metrics:\n",
    "  - Silhouette score\n",
    "  - Davies-Bouldin index\n",
    "  - Calinski-Harabasz index\n",
    "- External metrics (if ground truth available):\n",
    "  - Adjusted Rand Index (ARI)\n",
    "  - Normalized Mutual Information (NMI)\n",
    "  - Fowlkes-Mallows score\n",
    "- Visual inspection:\n",
    "  - Scatter plots, cluster plots\n",
    "  - Heatmaps\n",
    "\n",
    "## Advanced Techniques\n",
    "- Ensemble clustering\n",
    "- Consensus clustering\n",
    "- Subspace clustering\n",
    "- Feature learning with autoencoders\n",
    "- Self-supervised learning (modern approach)\n",
    "\n",
    "## Practical Considerations\n",
    "- Choosing the right number of clusters/components\n",
    "- Handling high-dimensional data\n",
    "- Handling categorical features\n",
    "- Handling outliers\n",
    "- Scaling for large datasets\n",
    "- Interpretability of clusters or latent features\n",
    "\n",
    "## Applications\n",
    "- Customer segmentation\n",
    "- Market basket analysis\n",
    "- Anomaly/fraud detection\n",
    "- Image compression or embedding\n",
    "- Topic modeling in text\n",
    "- Dimensionality reduction for visualization\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.406674,
   "end_time": "2025-10-27T13:14:15.326899",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-27T13:14:06.920225",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
