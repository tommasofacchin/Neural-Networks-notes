{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45765ca0",
   "metadata": {
    "papermill": {
     "duration": 0.00651,
     "end_time": "2025-12-17T12:20:17.548356",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.541846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "## Introduction to Supervised Learning\n",
    "\n",
    "Supervised learning is a type of machine learning where we teach a model to map inputs to known outputs. The algorithm learns from examples (input-output pairs) and then predicts outcomes for new, unseen data.\n",
    "\n",
    "In practice, supervised learning uses **labeled data**—each example comes with a correct answer. The model adjusts its parameters as it sees more data, learning the relationship between features and labels. This explicit guidance helps the model make accurate predictions on new inputs.\n",
    "\n",
    "Supervised learning is widely used in real-world problems, such as:  \n",
    "- Email spam detection  \n",
    "- Stock price prediction  \n",
    "- Customer churn prediction  \n",
    "\n",
    "It is especially useful when you want to build models that are highly accurate and can generalize well to new data.\n",
    "\n",
    "Main points:  \n",
    "- Predict a target variable (y) from input features (X).  \n",
    "- Targets can be **continuous** (regression) or **categorical** (classification).  \n",
    "- Difference from other learning types:  \n",
    "  - **Unsupervised:** find patterns without labels  \n",
    "  - **Reinforcement:** learn by interacting with an environment\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Fundamental Concepts\n",
    "\n",
    "Before working with models, some basics to keep in mind:  \n",
    "\n",
    "- **Dataset:** collection of samples with features (X) and targets (y)  \n",
    "- **Train vs Test:** train to learn, test to evaluate generalization  \n",
    "- **Overfitting / Underfitting:** too complex vs too simple models  \n",
    "- **Bias-Variance tradeoff:** balancing simplicity and flexibility  \n",
    "- **Model evaluation:** use metrics like MSE, R², accuracy, or F1-score; cross-validation helps estimate performance reliably\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785cc716",
   "metadata": {
    "papermill": {
     "duration": 0.004525,
     "end_time": "2025-12-17T12:20:17.557926",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.553401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Data preprocessing is a critical step in any machine learning workflow. Raw data is often messy, inconsistent, or incomplete, and models trained directly on such data can perform poorly or produce biased results. Preprocessing ensures that the dataset is clean, structured, and in a form suitable for the algorithms we want to use.  \n",
    "\n",
    "The first step is **data cleaning**, which involves identifying and correcting errors, inconsistencies, or duplicate entries. Missing values are common in real-world datasets, and handling them appropriately is crucial. Depending on the context, missing data can be removed, imputed using statistics like the mean or median, or predicted using more advanced methods such as k-nearest neighbors or regression models.  \n",
    "\n",
    "Another important consideration is **scaling and normalization**. Many algorithms, especially those based on distances (like KNN or SVM) or gradient-based optimization, are sensitive to the scale of input features. Standardization transforms features to have zero mean and unit variance, while normalization rescales data to a fixed range. Choosing the right scaling method can significantly affect model convergence and performance.  \n",
    "\n",
    "**Categorical variables** require special treatment because most machine learning algorithms only process numerical data. Common approaches include **label encoding**, which assigns an integer to each category, and **one-hot encoding**, which creates binary columns for each category. More sophisticated encodings, such as target encoding or frequency encoding, can capture additional information while avoiding introducing spurious orderings.  \n",
    "\n",
    "Finally, **feature engineering and selection** are essential for improving model accuracy and interpretability. Feature engineering involves creating new features or transforming existing ones to better capture underlying patterns. Feature selection helps reduce dimensionality, remove irrelevant or redundant features, and mitigate overfitting. Techniques range from simple correlation analysis to more advanced methods like recursive feature elimination or model-based importance scores.  \n",
    "\n",
    "Overall, careful and thoughtful data preprocessing forms the foundation of effective machine learning models. Without it, even the most sophisticated algorithms may fail to deliver reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d6b76",
   "metadata": {
    "papermill": {
     "duration": 0.006569,
     "end_time": "2025-12-17T12:20:17.569402",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.562833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Regression\n",
    "### Linear Regression\n",
    "\n",
    "Linear regression is one of the most fundamental and widely used techniques in supervised learning. It models the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data. The main idea is to predict the target as a weighted sum of the input features plus an intercept term.\n",
    "\n",
    "Mathematically, the model can be expressed as:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
    "$$\n",
    "\n",
    "Here, $\\beta_0$ is the intercept, $\\beta_1, \\dots, \\beta_n$ are the coefficients representing the influence of each feature on the target, and $\\epsilon$ is the error term capturing the difference between observed and predicted values. **Note that $\\epsilon_i$ represents the true (unobservable) random noise in the data-generating process, while in practice we work with the residuals $e_i = y_i - \\hat{y}_i$, which are the estimated errors after fitting the model.** The coefficients can be interpreted as the expected change in the target variable for a one-unit change in the corresponding feature, assuming all other features remain constant.\n",
    "\n",
    "Parameter estimation is usually performed using **Ordinary Least Squares (OLS)**, which finds the coefficients that minimize the sum of squared differences between the predicted and actual target values:\n",
    "\n",
    "$$\n",
    "\\text{Minimize } \\sum_{i=1}^m (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "This ensures the best possible linear fit to the training data in terms of squared error.\n",
    "\n",
    "Linear regression relies on several key **assumptions** that ensure the model produces valid, interpretable, and reliable results. Understanding these assumptions is crucial because if they are violated, the predictions, inference, or coefficient interpretations can be misleading.\n",
    "\n",
    "- **Linearity**: This assumption states that the relationship between each feature and the target variable is linear. In other words, the expected change in the target is proportional to a change in the feature. If the true relationship is non-linear, linear regression may underfit the data, leading to biased predictions. Visualizing scatter plots of each feature against the target can help detect non-linearity. \n",
    "- **Independence**: Observations should be independent of each other. This means that the value of one observation does not influence another. Violations occur in time series data or grouped data where correlations exist between samples. Ignoring dependence can result in underestimated standard errors and misleading significance tests. \n",
    "- **Homoscedasticity**: The variance of residuals (errors) should be constant across all levels of the features. If residuals systematically increase or decrease with the predicted values (heteroscedasticity), the model may be less efficient and confidence intervals or hypothesis tests can be inaccurate. Plotting residuals versus predicted values is a common way to check for this.\n",
    "- **Normality of residuals**: The residuals should follow a normal distribution. This assumption is especially important for conducting inference, such as calculating confidence intervals or p-values for coefficients. Even if predictions can still be reasonably accurate, non-normal residuals can invalidate hypothesis testing. Histograms or Q-Q plots of residuals are often used to check normality. \n",
    "- **No multicollinearity**: Features should not be highly correlated with each other. When multicollinearity exists, it becomes difficult to isolate the individual effect of each feature on the target, coefficients can become unstable, and small changes in the data can lead to large changes in estimates. Correlation matrices or variance inflation factors (VIF) are commonly used to detect multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "Evaluating a linear regression model is essential to understand how well it predicts the target variable and how much of the underlying variability it captures. Unlike classification problems where metrics like accuracy or F1-score are used, regression requires measures that quantify the **difference between predicted and actual values**, as well as the overall explanatory power of the model.\n",
    "\n",
    "One of the most common metrics is the **Mean Squared Error (MSE)**, which calculates the average of the squared differences between predicted and actual values:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Here, $y_i$ represents the actual target value, $\\hat{y}_i$ is the predicted value, and $m$ is the number of samples. Squaring the errors penalizes larger deviations more heavily, making MSE sensitive to outliers. A lower MSE indicates better predictive performance.\n",
    "\n",
    "The **Root Mean Squared Error (RMSE)** is simply the square root of the MSE:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "RMSE is often preferred because it is expressed in the same units as the target variable, making it easier to interpret and compare with the scale of the data.\n",
    "\n",
    "Another important metric is the **Coefficient of Determination ($R^2$)**:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^m (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^m (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "$R^2$ measures the proportion of variance in the target variable that is explained by the model. A value of $R^2 = 1$ indicates a perfect fit, whereas $R^2 = 0$ suggests that the model does no better than predicting the mean of the target. Negative values can occur when the model performs worse than simply predicting the mean.\n",
    "\n",
    "Together, these metrics give a **comprehensive view** of model performance: MSE and RMSE focus on prediction errors, while $R^2$ provides insight into how well the model captures the underlying structure of the data. By analyzing these metrics, we can assess not only how accurate our predictions are but also how much of the variability in the target is being explained.\n",
    "\n",
    "In practice, these evaluation measures guide decisions on model selection, hyperparameter tuning, and whether further feature engineering or preprocessing is needed. Even though linear regression is conceptually simple, mastering these metrics is crucial for building a solid foundation before moving on to more complex supervised learning models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32a0fd",
   "metadata": {
    "papermill": {
     "duration": 0.007042,
     "end_time": "2025-12-17T12:20:17.582944",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.575902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Multiple Linear Regression\n",
    "\n",
    "So far, the linear regression model has been introduced with one generic target variable and several predictors, but the structure is the same whether we have a single feature or many. Multiple linear regression explicitly considers the case where the target depends on several input variables simultaneously.\n",
    "\n",
    "Mathematically, the multiple linear regression model with $p - 1$ predictors can be written as:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_{p-1} x_{i,\\,p-1} + \\epsilon_i, \\quad i = 1, \\dots, m\n",
    "$$\n",
    "\n",
    "Here, $\\beta_0$ is the intercept, $\\beta_j$ for $j = 1, \\dots, p-1$ are the slope coefficients associated with each predictor $x_{ij}$, and $\\epsilon_i$ are the error terms. As before, $\\epsilon_i$ represent the unobservable random noise, while the residuals $e_i = y_i - \\hat{y}_i$ are their empirical estimates obtained after fitting the model.\n",
    "\n",
    "In matrix form, the model can be written compactly as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{y} \\in \\mathbb{R}^m$ is the vector of responses, $X \\in \\mathbb{R}^{m \\times p}$ is the design matrix (each row corresponds to an observation and each column to a predictor, with the first column typically equal to 1 for the intercept), $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the vector of coefficients, and $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^m$ is the vector of errors.\n",
    "\n",
    "Ordinary Least Squares (OLS) estimation in the multiple linear regression setting is obtained by minimizing the sum of squared residuals:\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\beta}} \\; \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n",
    "\\quad\\text{with}\\quad\n",
    "\\hat{y}_i = \\beta_0 + \\sum_{j=1}^{p-1} \\beta_j x_{ij}.\n",
    "$$\n",
    "\n",
    "In matrix notation, the OLS estimator $\\hat{\\boldsymbol{\\beta}}$ has the closed-form solution:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y},\n",
    "$$\n",
    "\n",
    "provided that $X^\\top X$ is invertible. This formula shows that multiple linear regression is still a linear model in the parameters: even if we add more predictors, or transformations of the original predictors, the estimation procedure remains the same.\n",
    "\n",
    "The interpretation of the coefficients in multiple linear regression is **conditional** on the other variables being held fixed. The coefficient $\\beta_j$ measures the expected change in the target variable $y$ for a one-unit increase in the predictor $x_j$, assuming all other predictors remain constant. This is a key difference from simple linear regression, where there is only one predictor and no need to condition on other variables.\n",
    "\n",
    "Multiple linear regression is also the natural framework to introduce issues such as multicollinearity, interaction terms, and polynomial features. For example, a polynomial regression in one variable $x$,\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_k x_i^k + \\epsilon_i,\n",
    "$$\n",
    "\n",
    "can be seen as a multiple linear regression where the predictors are the transformed features $x_i, x_i^2, \\dots, x_i^k$. This perspective emphasizes that, although the relationship between $x$ and $y$ may be non-linear, the model remains linear in the parameters $\\beta_j$ and can still be estimated using the same OLS machinery.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4a8a3",
   "metadata": {
    "papermill": {
     "duration": 0.004897,
     "end_time": "2025-12-17T12:20:17.593124",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.588227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Implementation in R\n",
    "\n",
    "In practice, linear regression models are often fitted using the R function `lm()`, which implements Ordinary Least Squares (OLS) estimation for a wide range of model specifications. A multiple linear regression can be defined as:\n",
    "\n",
    "```\n",
    "model <- lm(y ~ x1 + x2 + x3, data = df)\n",
    "```\n",
    "\n",
    "Here, `y` is the target variable, `x1`, `x2`, and `x3` are the predictors, and `df` is a data frame containing the data. The formula `y ~ x1 + x2 + x3` specifies that `y` is modeled as a linear combination of the predictors plus an intercept term, and `lm()` internally estimates the coefficients by minimizing the sum of squared residuals, exactly as described in the OLS formulation.\n",
    "\n",
    "Before fitting the model, it is common practice to inspect the data and, when appropriate, split it into training and test sets or use cross-validation to obtain an unbiased estimate of the model’s generalization performance. In many real-world applications, predictors are also standardized (e.g., using `scale()`) to make coefficients comparable in magnitude and to prepare the data for regularized regression methods.\n",
    "\n",
    "Once the model is fitted, the `summary()` function provides a detailed overview of the results:\n",
    "\n",
    "```\n",
    "summary(model)\n",
    "```\n",
    "\n",
    "This output includes the estimated coefficients, their standard errors, t-statistics, and p-values, which are used to assess the statistical significance of each predictor under the usual linear regression assumptions. For each coefficient $\\hat{\\beta}_j$, the standard error $\\text{SE}(\\hat{\\beta}_j)$ measures the estimated variability of $\\hat{\\beta}_j$ across hypothetical repeated samples. The **t-statistic** is defined as\n",
    "\n",
    "$$\n",
    "t_j = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)},\n",
    "$$\n",
    "\n",
    "and, under the null hypothesis $H_0 : \\beta_j = 0$ and assuming the linear model assumptions hold, $t_j$ approximately follows a t-distribution with $m - p$ degrees of freedom (where $m$ is the number of observations and $p$ the number of parameters, including the intercept). The corresponding **p-value** is the probability, under $H_0$, of observing a $t$-value as extreme or more extreme than the one computed from the data. A small p-value (typically below a chosen significance level, such as $0.05$) provides evidence against $H_0$, suggesting that the predictor $x_j$ is significantly associated with the response after controlling for the other variables in the model.\n",
    "\n",
    "The `summary()` output also reports the Residual Standard Error, the **R-squared** and **Adjusted R-squared**, and an overall F-statistic that tests whether the model explains a significant amount of variance compared to a null model with no predictors.\n",
    "\n",
    "The **Adjusted R-squared** is particularly important when comparing models with different numbers of predictors. Unlike the regular R-squared, which never decreases when new variables are added, the Adjusted R-squared penalizes unnecessary complexity and increases only if a new predictor improves the model more than would be expected by chance. In addition to R-squared, model selection in linear regression often relies on information criteria such as the **Akaike Information Criterion (AIC)** and the **Bayesian Information Criterion (BIC)**, which balance goodness of fit and model complexity: for both AIC and BIC, **lower values indicate a better trade-off** between fit and parsimony.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c063a7ac",
   "metadata": {
    "papermill": {
     "duration": 0.004774,
     "end_time": "2025-12-17T12:20:17.604658",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.599884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Matrix Form\n",
    "\n",
    "Linear regression is most naturally expressed in matrix notation. This allows us to write the model for all observations and all predictors in a single compact equation.\n",
    "\n",
    "Let:\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}$ be the vector of target values (one entry per observation),\n",
    "- $X \\in \\mathbb{R}^{m \\times p}$ be the design matrix, where:\n",
    "  - the first column is a column of ones (for the intercept),\n",
    "  - the remaining $p - 1$ columns contain the predictor values,\n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p \\times 1}$ be the vector of parameters (intercept and coefficients),\n",
    "- $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^{m \\times 1}$ be the vector of error terms.\n",
    "\n",
    "The multiple linear regression model can then be written compactly as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "For a given parameter vector $\\boldsymbol{\\beta}$, the predictions for all $m$ observations are:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = X \\hat{\\boldsymbol{\\beta}}\n",
    "$$\n",
    "\n",
    "The Ordinary Least Squares (OLS) estimator $\\hat{\\boldsymbol{\\beta}}$ is obtained by minimizing the sum of squared residuals $\\|\\mathbf{y} - X\\boldsymbol{\\beta}\\|^2$. Under standard conditions (e.g., $X^\\top X$ invertible), the closed-form solution is:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "This shows that, regardless of how many predictors we include, the estimation procedure remains the same: only the design matrix $X$ changes when we add, remove, or transform features.\n",
    "\n",
    "#### Example (Two Predictors)\n",
    "\n",
    "Consider a small example with two predictors $x_1$ and $x_2$ and three observations. We model:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}\n",
    "$$\n",
    "\n",
    "with data:\n",
    "\n",
    "| i | $x_{i1}$ | $x_{i2}$ | $y_i$ |\n",
    "|---|---------|---------|------|\n",
    "| 1 | 1       | 0       | 2    |\n",
    "| 2 | 2       | 1       | 4    |\n",
    "| 3 | 3       | 2       | 7    |\n",
    "\n",
    "In matrix form, we define:\n",
    "\n",
    "- Target vector:\n",
    "  $$\n",
    "  \\mathbf{y} =\n",
    "  \\begin{bmatrix}\n",
    "  2 \\\\\n",
    "  4 \\\\\n",
    "  7\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- Design matrix (first column of ones for the intercept, then $x_1$ and $x_2$):\n",
    "  $$\n",
    "  X =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 1 & 0 \\\\\n",
    "  1 & 2 & 1 \\\\\n",
    "  1 & 3 & 2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- Parameter vector:\n",
    "  $$\n",
    "  \\boldsymbol{\\beta} =\n",
    "  \\begin{bmatrix}\n",
    "  \\beta_0 \\\\\n",
    "  \\beta_1 \\\\\n",
    "  \\beta_2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  \n",
    "The model is:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "Using OLS in matrix form, the estimated coefficients are:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "For this example:\n",
    "\n",
    "$$\n",
    "X^\\top X =\n",
    "\\begin{bmatrix}\n",
    "3 & 6 & 3 \\\\\n",
    "6 & 14 & 8 \\\\\n",
    "3 & 8 & 5\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "X^\\top \\mathbf{y} =\n",
    "\\begin{bmatrix}\n",
    "13 \\\\\n",
    "31 \\\\\n",
    "18\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and $(X^\\top X)^{-1}$ exists (details of the inversion can be omitted in practice, since it is handled numerically by libraries). By multiplying $(X^\\top X)^{-1}$ and $X^\\top \\mathbf{y}$ we obtain:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "so the fitted model is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = 1 + 1 \\cdot x_1 + 1 \\cdot x_2.\n",
    "$$\n",
    "\n",
    "This example shows how the compact matrix formula $(X^\\top X)^{-1} X^\\top \\mathbf{y}$ directly produces the intercept and coefficients for a multiple linear regression model with more than one predictor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694f850",
   "metadata": {
    "papermill": {
     "duration": 0.006202,
     "end_time": "2025-12-17T12:20:17.615463",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.609261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Categorical Variables\n",
    "\n",
    "In many real-world regression problems, not all predictors are numerical. We often have **categorical variables**, such as `city`, `education_level`, or `product_type`. Linear regression, however, operates on numerical inputs, so categorical variables must be encoded into numeric form before they can be included in the model.\n",
    "\n",
    "A common strategy is **one-hot encoding** (or dummy encoding). Suppose we have a categorical variable `color` with three possible values: `red`, `green`, `blue`. We create three binary indicators:\n",
    "\n",
    "- $x_{\\text{red}} = 1$ if `color = red`, otherwise $0$  \n",
    "- $x_{\\text{green}} = 1$ if `color = green`, otherwise $0$  \n",
    "- $x_{\\text{blue}} = 1$ if `color = blue`, otherwise $0$\n",
    "\n",
    "If we include all three dummies together with an intercept, we introduce perfect multicollinearity, because:\n",
    "\n",
    "$$\n",
    "x_{\\text{red}} + x_{\\text{green}} + x_{\\text{blue}} = 1 \\quad \\text{for every observation.}\n",
    "$$\n",
    "\n",
    "To avoid this, one category is taken as a **reference level** and its dummy is omitted (e.g., drop `blue`). The model becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\beta_0\n",
    "          + \\beta_{\\text{red}} x_{i,\\text{red}}\n",
    "          + \\beta_{\\text{green}} x_{i,\\text{green}}\n",
    "          + \\text{(other numeric predictors)}.\n",
    "$$\n",
    "\n",
    "In this parameterization:\n",
    "\n",
    "- $\\beta_0$ is the expected value of $y$ for the reference category (`blue`) when all other predictors are zero.  \n",
    "- $\\beta_{\\text{red}}$ is the expected difference in $y$ between `red` and `blue`, holding other predictors fixed.  \n",
    "- $\\beta_{\\text{green}}$ is the expected difference in $y$ between `green` and `blue`, holding other predictors fixed.\n",
    "\n",
    "From the matrix point of view, each dummy variable is just another column of the design matrix $X$. The only difference is that these columns contain 0/1 values instead of continuous numbers. As long as we avoid including all dummy columns for a categorical variable (i.e., we drop one category), the usual OLS machinery and the closed-form solution still apply without modification.\n",
    "\n",
    "#### Example: Model Formula with Dummy Variables\n",
    "\n",
    "Suppose we have one numerical predictor `size` and one categorical predictor `color` with three levels: `red`, `green`, `blue`. After dummy encoding (and dropping `blue` as reference), our predictors are:\n",
    "\n",
    "- $x_{\\text{size}}$ (numeric)\n",
    "- $x_{\\text{red}}$ (dummy: 1 if `color = red`, 0 otherwise)\n",
    "- $x_{\\text{green}}$ (dummy: 1 if `color = green`, 0 otherwise)\n",
    "\n",
    "The regression model can be written as:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i\n",
    "= \\beta_0\n",
    "+ \\beta_{\\text{size}} \\, x_{i,\\text{size}}\n",
    "+ \\beta_{\\text{red}} \\, x_{i,\\text{red}}\n",
    "+ \\beta_{\\text{green}} \\, x_{i,\\text{green}}.\n",
    "$$\n",
    "\n",
    "If we look at each color separately:\n",
    "\n",
    "- For `blue` (reference): $x_{i,\\text{red}} = 0$, $x_{i,\\text{green}} = 0$  \n",
    "  $$\n",
    "  \\hat{y}_i \\mid \\text{blue}\n",
    "  = \\beta_0 + \\beta_{\\text{size}} \\, x_{i,\\text{size}}.\n",
    "  $$\n",
    "\n",
    "- For `red`: $x_{i,\\text{red}} = 1$, $x_{i,\\text{green}} = 0$  \n",
    "  $$\n",
    "  \\hat{y}_i \\mid \\text{red}\n",
    "  = (\\beta_0 + \\beta_{\\text{red}})\n",
    "  + \\beta_{\\text{size}} \\, x_{i,\\text{size}}.\n",
    "  $$\n",
    "\n",
    "- For `green`: $x_{i,\\text{red}} = 0$, $x_{i,\\text{green}} = 1$  \n",
    "  $$\n",
    "  \\hat{y}_i \\mid \\text{green}\n",
    "  = (\\beta_0 + \\beta_{\\text{green}})\n",
    "  + \\beta_{\\text{size}} \\, x_{i,\\text{size}}.\n",
    "  $$\n",
    "\n",
    "This makes the interpretation clear:\n",
    "\n",
    "- $\\beta_0$ is the intercept for the reference category `blue`.  \n",
    "- $\\beta_{\\text{red}}$ and $\\beta_{\\text{green}}$ shift the intercept up or down for `red` and `green` relative to `blue`, while the slope with respect to `size` remains $\\beta_{\\text{size}}$ for all colors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcadab4",
   "metadata": {
    "papermill": {
     "duration": 0.0047,
     "end_time": "2025-12-17T12:20:17.624850",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.620150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Interaction Terms\n",
    "\n",
    "So far, the regression models have assumed that each predictor has an effect on the target that does not depend on the values of the other predictors. In many real-world problems, however, the effect of one variable can **change** depending on another variable. Interaction terms allow the model to capture these context-dependent effects.\n",
    "\n",
    "In a multiple linear regression without interactions, a model like\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}\n",
    "$$\n",
    "\n",
    "assumes that:\n",
    "- the effect of $x_1$ on $y$ is always $\\beta_1$, regardless of the value of $x_2$  \n",
    "- the effect of $x_2$ on $y$ is always $\\beta_2$, regardless of the value of $x_1$.\n",
    "\n",
    "To allow the effect of $x_1$ to depend on $x_2$, we add an **interaction term** $x_{i1} x_{i2}$:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\gamma\\, x_{i1} x_{i2}.\n",
    "$$\n",
    "\n",
    "Now the partial effect of $x_1$ on $\\hat{y}$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial x_{i1}} = \\beta_1 + \\gamma x_{i2},\n",
    "$$\n",
    "\n",
    "so it changes with $x_{i2}$. Similarly, the effect of $x_2$ depends on $x_1$.\n",
    "\n",
    "From a design-matrix perspective, the interaction term is simply a **new column** in $X$, whose entries are the products $x_{i1} x_{i2}$.\n",
    "\n",
    "#### Example: Numeric × Categorical\n",
    "\n",
    "Consider the earlier example with one numerical predictor `size` and one categorical predictor `color` with levels `red`, `green`, `blue` (with `blue` as reference). After dummy encoding, we have:\n",
    "\n",
    "- $x_{\\text{size}}$ (numeric)\n",
    "- $x_{\\text{red}}$ (dummy)\n",
    "- $x_{\\text{green}}$ (dummy)\n",
    "\n",
    "A model **without** interactions is:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i\n",
    "= \\beta_0\n",
    "+ \\beta_{\\text{size}} \\, x_{i,\\text{size}}\n",
    "+ \\beta_{\\text{red}} \\, x_{i,\\text{red}}\n",
    "+ \\beta_{\\text{green}} \\, x_{i,\\text{green}}.\n",
    "$$\n",
    "\n",
    "In this model, the slope with respect to `size` is $\\beta_{\\text{size}}$ for all colors; the dummies shift only the intercept.\n",
    "\n",
    "To allow the slope of `size` to depend on `color`, we include interaction terms between `size` and the dummies:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i =\n",
    "\\beta_0\n",
    "+ \\beta_{\\text{size}} \\, x_{i,\\text{size}}\n",
    "+ \\beta_{\\text{red}} \\, x_{i,\\text{red}}\n",
    "+ \\beta_{\\text{green}} \\, x_{i,\\text{green}}\n",
    "+ \\gamma_{\\text{red}} \\, (x_{i,\\text{size}} \\cdot x_{i,\\text{red}})\n",
    "+ \\gamma_{\\text{green}} \\, (x_{i,\\text{size}} \\cdot x_{i,\\text{green}}).\n",
    "$$\n",
    "\n",
    "Now the model implies different lines for each color:\n",
    "\n",
    "- `blue` (reference): $x_{i,\\text{red}} = x_{i,\\text{green}} = 0$\n",
    "  $$\n",
    "  \\hat{y}_i \\mid \\text{blue}\n",
    "  = \\beta_0 + \\beta_{\\text{size}} \\, x_{i,\\text{size}}.\n",
    "  $$\n",
    "\n",
    "- `red`: $x_{i,\\text{red}} = 1$, $x_{i,\\text{green}} = 0$\n",
    "  $$\n",
    "  \\hat{y}_i \\mid \\text{red}\n",
    "  = (\\beta_0 + \\beta_{\\text{red}})\n",
    "  + (\\beta_{\\text{size}} + \\gamma_{\\text{red}})\\, x_{i,\\text{size}}.\n",
    "  $$\n",
    "\n",
    "- `green`: $x_{i,\\text{red}} = 0$, $x_{i,\\text{green}} = 1$\n",
    "  $$\n",
    "  \\hat{y}_i \\mid \\text{green}\n",
    "  = (\\beta_0 + \\beta_{\\text{green}})\n",
    "  + (\\beta_{\\text{size}} + \\gamma_{\\text{green}})\\, x_{i,\\text{size}}.\n",
    "  $$\n",
    "\n",
    "In this parameterization:\n",
    "- the intercept changes across colors (through $\\beta_{\\text{red}}$ and $\\beta_{\\text{green}}$),\n",
    "- the slope with respect to `size` also changes across colors (through $\\gamma_{\\text{red}}$ and $\\gamma_{\\text{green}}$).\n",
    "\n",
    "Interaction terms are useful when:\n",
    "\n",
    "- domain knowledge suggests that the effect of one variable should depend on another (e.g., effect of a treatment depending on age, effect of price depending on segment),  \n",
    "- plots or exploratory analysis show that the relationship between a numeric predictor and $y$ differs across groups.\n",
    "\n",
    "In practice, interactions are implemented by adding product columns to the design matrix $X$. The model remains linear in the parameters and can still be estimated with the usual OLS machinery.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe0847",
   "metadata": {
    "papermill": {
     "duration": 0.004376,
     "end_time": "2025-12-17T12:20:17.634100",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.629724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### ANOVA\n",
    "\n",
    "In the context of multiple linear regression, Analysis of Variance (ANOVA) decomposes the total variability of the response variable into a component explained jointly by all predictors in the model and a residual (unexplained) component. The starting point is the decomposition of the total sum of squares:\n",
    "\n",
    "$$\n",
    "\\text{TSS} = \\sum_{i=1}^{m} (y_i - \\bar{y})^2\n",
    "= \\underbrace{\\sum_{i=1}^{m} (\\hat{y}_i - \\bar{y})^2}_{\\text{Model sum of squares (MSS)}}\n",
    "+ \\underbrace{\\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2}_{\\text{Residual sum of squares (RSS)}}\n",
    "$$\n",
    "\n",
    "Here, $\\text{TSS}$ (Total Sum of Squares) measures the overall variability of $y$ around its mean, $\\text{MSS}$ measures the variability explained by the multiple regression model (i.e., by all predictors taken together), and $\\text{RSS}$ measures the variability that remains in the residuals. Note that the coefficient of determination can be written as $R^2 = \\text{MSS} / \\text{TSS}$.\n",
    "\n",
    "In a multiple linear regression with $p$ parameters (including the intercept), the degrees of freedom associated with each sum of squares are:\n",
    "\n",
    "- Model degrees of freedom: $\\text{df}_{\\text{model}} = p - 1$ (number of slope coefficients/predictors)\n",
    "- Residual degrees of freedom: $\\text{df}_{\\text{res}} = m - p$\n",
    "- Total degrees of freedom: $\\text{df}_{\\text{tot}} = m - 1$\n",
    "\n",
    "From these, the mean squares are defined as:\n",
    "\n",
    "$$\n",
    "\\text{MS}_{\\text{model}} = \\frac{\\text{MSS}}{\\text{df}_{\\text{model}}}, \\quad\n",
    "\\text{MS}_{\\text{res}}   = \\frac{\\text{RSS}}{\\text{df}_{\\text{res}}}\n",
    "$$\n",
    "\n",
    "The overall F-test for the multiple regression model compares the full model (with all predictors) against a null model with only the intercept. The null and alternative hypotheses are:\n",
    "\n",
    "- $H_0$: all slope coefficients are zero (the predictors, taken together, do not explain more variability than the mean), i.e. $\\beta_1 = \\beta_2 = \\dots = \\beta_{p-1} = 0$\n",
    "- $H_1$: at least one slope coefficient is non-zero (the predictors, taken together, explain a non-negligible part of the variability)\n",
    "\n",
    "The F-statistic is defined as:\n",
    "\n",
    "$$\n",
    "F = \\frac{\\text{MS}_{\\text{model}}}{\\text{MS}_{\\text{res}}}\n",
    "$$\n",
    "\n",
    "Under $H_0$, this statistic approximately follows an F-distribution with $(\\text{df}_{\\text{model}}, \\text{df}_{\\text{res}})$ degrees of freedom. A large value of $F$ (with a small p-value) provides evidence against $H_0$, indicating that the multiple regression model explains a significant portion of the variability in the response compared to using only the mean.\n",
    "\n",
    "In R, the ANOVA table for a single multiple regression model is obtained via:\n",
    "\n",
    "```\n",
    "anova(model)\n",
    "```\n",
    "\n",
    "which reports, for each source of variation, the sum of squares (SS), degrees of freedom (Df), mean squares (MS), F-statistics, and p-values.\n",
    "\n",
    "ANOVA can also be used to compare **nested multiple regression models**, for example, a reduced model containing only a subset of predictors and a more complex full model with additional predictors:\n",
    "\n",
    "```\n",
    "model_reduced <- lm(y ~ x1 + x2, data = df)\n",
    "model_full    <- lm(y ~ x1 + x2 + x3, data = df)\n",
    "\n",
    "anova(model_reduced, model_full)\n",
    "```\n",
    "\n",
    "In this case, the hypotheses are:\n",
    "\n",
    "- $H_0$: the reduced model is sufficient (the additional predictor(s) in `model_full` do not improve the fit)\n",
    "- $H_1$: the full model provides a better fit (at least one of the additional predictors contributes significantly)\n",
    "\n",
    "The F-statistic is constructed from the reduction in RSS and the loss of degrees of freedom when moving from the reduced to the full model. A significant p-value indicates that the additional predictors in the full multiple regression model lead to a statistically significant improvement in explained variance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c4074",
   "metadata": {
    "papermill": {
     "duration": 0.004283,
     "end_time": "2025-12-17T12:20:17.642867",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.638584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### AIC and BIC\n",
    "\n",
    "While ANOVA focuses on hypothesis testing for nested models, information criteria such as the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) provide a way to compare multiple models (not necessarily nested) by balancing goodness of fit and model complexity.\n",
    "\n",
    "For a fitted model with maximized log-likelihood $\\ell$ and $k$ free parameters, the AIC is defined as:\n",
    "\n",
    "$$\n",
    "\\text{AIC} = 2k - 2\\ell\n",
    "$$\n",
    "\n",
    "and the BIC is defined as:\n",
    "\n",
    "$$\n",
    "\\text{BIC} = k \\log(m) - 2\\ell\n",
    "$$\n",
    "\n",
    "where $m$ is the number of observations. Both criteria penalize models with more parameters:\n",
    "\n",
    "- The term $2k$ in AIC and $k \\log(m)$ in BIC act as penalties for model complexity.\n",
    "- The term $-2\\ell$ decreases as the model fits the data better (higher log-likelihood).\n",
    "\n",
    "In linear regression with Gaussian errors, the log-likelihood $\\ell$ is, up to an additive constant, a function of the residual sum of squares (RSS):\n",
    "\n",
    "$$\n",
    "\\ell \\propto -\\frac{m}{2} \\log\\left(\\frac{\\text{RSS}}{m}\\right)\n",
    "$$\n",
    "\n",
    "so models with smaller RSS (better fit) tend to have larger $\\ell$ and therefore smaller AIC and BIC, all else being equal.\n",
    "\n",
    "The general rule for model comparison is:\n",
    "\n",
    "- Prefer the model with **lower AIC**.\n",
    "- Prefer the model with **lower BIC**.\n",
    "\n",
    "Because BIC uses $\\log(m)$ instead of 2 as penalty weight, it penalizes complexity more strongly than AIC, especially for large sample sizes. As a result:\n",
    "\n",
    "- AIC tends to favor models with better predictive performance, possibly with slightly more parameters.\n",
    "- BIC tends to favor more parsimonious models and is often used in a more “conservative” model selection perspective.\n",
    "\n",
    "In R, AIC and BIC can be computed directly:\n",
    "\n",
    "```\n",
    "AIC(model_reduced, model_full)\n",
    "BIC(model_reduced, model_full)\n",
    "```\n",
    "\n",
    "These functions return the AIC/BIC values for each model, allowing direct comparison. When combined with ANOVA and adjusted R-squared, AIC and BIC provide a comprehensive toolkit to decide which linear regression specification is most appropriate in terms of both explanatory power and complexity.\n",
    "\n",
    "\n",
    "#### Residuals vs Fitted plot\n",
    "\n",
    "A key diagnostic tool for linear regression is the residuals vs fitted plot, which visualizes the relationship between the residuals $e_i = y_i - \\hat{y}_i$ and the fitted values $\\hat{y}_i$. Under the standard linear regression assumptions, the residuals should be centered around zero and display no systematic pattern as a function of the fitted values.\n",
    "\n",
    "If the model is appropriate and the assumptions of linearity and homoscedasticity hold, the residuals vs fitted plot should show a random cloud of points with roughly constant vertical spread along the horizontal axis. In contrast, visible structures (for example, a curved pattern or a “funnel” shape with increasing spread) suggest model misspecification: curvature indicates that the linearity assumption may be violated, while a funnel-shaped pattern indicates heteroscedasticity (non-constant variance of the errors).\n",
    "\n",
    "In R, the residuals vs fitted plot can be obtained directly from a fitted `lm` object:\n",
    "\n",
    "```\n",
    "plot(model, which = 1)\n",
    "```\n",
    "\n",
    "Here, `which = 1` selects the first standard diagnostic plot, where the x-axis shows the fitted values $\\hat{y}_i$ and the y-axis shows the standardized residuals. This visualization is particularly useful to decide whether transformations of the response or predictors, or a more flexible model (e.g. adding polynomial terms or interactions), might be necessary to better capture the underlying relationship.\n",
    "\n",
    "\n",
    "#### Q-Q plot of residuals\n",
    "\n",
    "Another important assumption of the classical linear regression model is that the errors (and therefore the residuals, as their estimates) are approximately normally distributed. This assumption is crucial for the validity of t-tests, F-tests, and confidence intervals for the regression coefficients. A Q-Q (quantile–quantile) plot provides a visual check of the normality of the residuals.\n",
    "\n",
    "The idea of the Q-Q plot is to compare the empirical quantiles of the residuals with the theoretical quantiles of a standard normal distribution. If the residuals follow a normal distribution, the points in the Q-Q plot should lie approximately on a straight line. Systematic deviations from the line indicate departures from normality: heavy tails (points bending away at the ends), skewness (points systematically above or below the line on one side), or other irregular patterns.\n",
    "\n",
    "In R, the Q-Q plot for the residuals of a linear model can be produced with:\n",
    "\n",
    "```\n",
    "plot(model, which = 2)\n",
    "```\n",
    "\n",
    "This generates a normal Q-Q plot of the standardized residuals. Mild deviations from the line are often acceptable in practice, especially for large samples where the central limit theorem mitigates moderate non-normality. However, strong deviations may suggest the presence of outliers, skewness, or other distributional issues that can affect inference, and might motivate transformations of the response or the use of more robust modeling approaches.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b723ea",
   "metadata": {
    "papermill": {
     "duration": 0.004408,
     "end_time": "2025-12-17T12:20:17.652024",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.647616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Bias–Variance Tradeoff\n",
    "\n",
    "When building regression models, there is a fundamental tradeoff between bias and variance that determines how well the model will generalize to unseen data. Intuitively, increasing model complexity (for example, by adding more predictors or using higher-degree polynomials) tends to reduce bias but increase variance, while simplifying the model has the opposite effect.\n",
    "\n",
    "In the standard regression setting, the expected prediction error at a point $x$ can be decomposed as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\big[(Y - \\hat{f}(x))^2\\big]\n",
    "= \\underbrace{\\big(\\mathbb{E}[\\hat{f}(x)] - f(x)\\big)^2}_{\\text{Bias}^2}\n",
    "+ \\underbrace{\\mathbb{V}[\\hat{f}(x)]}_{\\text{Variance}}\n",
    "+ \\underbrace{\\sigma^2}_{\\text{Irreducible error}}\n",
    "$$\n",
    "\n",
    "Here, $f(x)$ is the true regression function, $\\hat{f}(x)$ is the model’s prediction, and $\\sigma^2$ is the variance of the noise in the data. The **bias** term measures how far, on average, the model’s predictions are from the true function (systematic error), while the **variance** term measures how sensitive the model is to fluctuations in the training data (how much the predictions change if we fit the model on different samples).\n",
    "\n",
    "Simple models, such as a linear regression with very few predictors or low-degree polynomials, typically have **high bias and low variance**: they may underfit the data and fail to capture important structure, but their predictions are relatively stable across different training sets. In contrast, very flexible models, such as high-degree polynomial regressions or models with many interaction terms, tend to have **low bias and high variance**: they can closely follow the training data, including noise, but their predictions may change drastically if the training data change slightly, leading to overfitting. \n",
    "\n",
    "The bias–variance tradeoff implies that there is an intermediate level of model complexity that minimizes the overall expected prediction error. In practice, techniques such as **cross-validation** are used to select model complexity (for example, the degree of a polynomial or the amount of regularization) that achieves a good balance between bias and variance. Regularization methods like Ridge, Lasso, and Elastic Net explicitly control model flexibility by shrinking coefficients towards zero, thereby reducing variance at the cost of introducing some additional bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af822d",
   "metadata": {
    "papermill": {
     "duration": 0.004285,
     "end_time": "2025-12-17T12:20:17.660783",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.656498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Polynomial regression is an extension of linear regression that allows the model to capture non‑linear relationships between the input features and the target variable by introducing polynomial terms of the original features. Although the relationship between the original feature $x$ and the target $y$ can be curved, the model remains **linear in the parameters** because it is a linear combination of transformed features.\n",
    "\n",
    "\n",
    "In the simplest case with a single predictor $x$, a polynomial regression of degree $k$ can be written as:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_k x_i^k + \\epsilon_i,\n",
    "\\quad i = 1, \\dots, m\n",
    "$$\n",
    "\n",
    "If we define new features $z_{i1} = x_i, \\; z_{i2} = x_i^2, \\dots, z_{ik} = x_i^k$, this model is just a multiple linear regression in the transformed feature vector $(z_{i1}, \\dots, z_{ik})$.\n",
    "\n",
    "Let $Z$ be the design matrix whose columns are $\\mathbf{1}, x, x^2, \\dots, x^k$. Then the model is:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = Z \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "and the coefficients can still be estimated by Ordinary Least Squares using:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = (Z^\\top Z)^{-1} Z^\\top \\mathbf{y},\n",
    "$$\n",
    "\n",
    "provided that $Z^\\top Z$ is invertible. This shows that polynomial regression does not change the estimation machinery; it only changes the feature space in which linear regression is applied.\n",
    "\n",
    "### Implementing polynomial regression in R\n",
    "\n",
    "There are two common ways to specify polynomial terms in R:\n",
    "\n",
    "1. **Using explicit powers with `I()`** (raw polynomial):\n",
    "\n",
    "```r\n",
    "# quadratic\n",
    "fit_quad <- lm(y ~ x + I(x^2), data = df)\n",
    "\n",
    "# cubic\n",
    "fit_cubic <- lm(y ~ x + I(x^2) + I(x^3), data = df)\n",
    "```\n",
    "\n",
    "The `I()` function tells R to interpret `x^2` as “x squared” and not as part of the formula language.\n",
    "\n",
    "2. **Using `poly()`**:\n",
    "\n",
    "```r\n",
    "# degree 3, orthogonal polynomials (default)\n",
    "fit_poly3 <- lm(y ~ poly(x, 3), data = df)\n",
    "\n",
    "# degree 3, raw powers\n",
    "fit_poly3_raw <- lm(y ~ poly(x, 3, raw = TRUE), data = df)\n",
    "```\n",
    "\n",
    "- `poly(x, 3, raw = TRUE)` generates the columns $x, x^2, x^3$ (like manual `I(x^2)`, etc.).\n",
    "- `poly(x, 3)` (default `raw = FALSE`) generates **orthogonal polynomials**, which are linear combinations of $x, x^2, x^3$ but decorrelated; this improves numerical stability. The fitted values are the same up to numerical precision, only the coefficients differ in interpretation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb24b9",
   "metadata": {
    "papermill": {
     "duration": 0.004295,
     "end_time": "2025-12-17T12:20:17.669452",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.665157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Box–Cox Transformation\n",
    "\n",
    "The Box–Cox transformation is a parametric family of power transformations for strictly positive responses $y > 0$, defined as:\n",
    "\n",
    "$$\n",
    "y^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    "\\dfrac{y^\\lambda - 1}{\\lambda}, & \\lambda \\neq 0 \\\\\n",
    "\\log(y), & \\lambda = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The parameter $\\lambda$ controls the shape of the transformation: $\\lambda = 1$ corresponds to no transformation, $\\lambda = 0$ to a log transform, values like $\\lambda \\approx 0.5$ approximate a square‑root transform, and $\\lambda = -1$ is close to a reciprocal transform. The main goals are to reduce skewness and heteroscedasticity and to make the relationship between predictors and response closer to linear.\n",
    "\n",
    "In the context of linear regression, the Box–Cox transformation is typically applied to the **response** $y$ rather than the predictors. One considers a model of the form\n",
    "$$\n",
    "y_i^{(\\lambda)} = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i,\n",
    "$$\n",
    "where $y_i^{(\\lambda)}$ is the transformed response. The parameter $\\lambda$ is chosen (for example) by maximizing the profile log‑likelihood under the assumption of normal, constant‑variance errors for $y^{(\\lambda)}$. After selecting $\\lambda$, the linear model is fitted on the transformed scale using ordinary least squares, and standard diagnostic tools (residual plots, Q–Q plots) are used to assess whether normality and homoscedasticity have improved.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4630d",
   "metadata": {
    "papermill": {
     "duration": 0.004166,
     "end_time": "2025-12-17T12:20:17.678184",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.674018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Regularized Regression\n",
    "\n",
    "Regularization techniques are introduced to prevent overfitting in regression models by adding a penalty term to the loss function. These methods discourage the model from becoming too complex and help improve its ability to generalize. The two most common regularization techniques are **Ridge Regression** and **Lasso Regression**, both of which are extensions of linear regression.\n",
    "\n",
    "**Ridge Regression (L2 Regularization)**\n",
    "\n",
    "Ridge regression adds a penalty proportional to the **square of the magnitude** of the coefficients to the loss function. This penalty term prevents the coefficients from growing too large, ensuring that the model remains relatively simple, even if there are many features.\n",
    "\n",
    "The loss function for Ridge regression is:\n",
    "\n",
    "$$\n",
    "\\text{Loss function} = \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^n \\beta_j^2\n",
    "$$\n",
    "\n",
    "Here, **$\\lambda$** is the **regularization parameter**, which controls the strength of the penalty. Increasing **$\\lambda$** shrinks the coefficients further towards zero, resulting in a simpler model.\n",
    "\n",
    "**Lasso Regression (L1 Regularization)**\n",
    "\n",
    "Lasso regression adds a penalty proportional to the **absolute values** of the coefficients. The key difference from Ridge regression is that Lasso can **shrink some coefficients to exactly zero**, effectively performing feature selection by eliminating irrelevant or redundant variables.\n",
    "\n",
    "The loss function for Lasso regression is:\n",
    "\n",
    "$$\n",
    "\\text{Loss function} = \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^n |\\beta_j|\n",
    "$$\n",
    "\n",
    "Lasso is particularly useful when we expect that only a subset of the features is relevant, as it can help reduce the dimensionality of the model by setting unnecessary features to zero.\n",
    "\n",
    "**Elastic Net**\n",
    "\n",
    "The **Elastic Net** is a hybrid of Ridge and Lasso regression that combines both **L1** and **L2 regularization**. This method is especially useful when there are many correlated features in the dataset. The Elastic Net is powerful when there is a mix of relevant features and collinear features, as it can help overcome some of the limitations of both Ridge and Lasso individually.\n",
    "\n",
    "The Elastic Net penalty is controlled by two parameters: **$\\lambda$** (regularization strength) and **$\\alpha$** (mixing parameter). The loss function for Elastic Net is:\n",
    "\n",
    "$$\n",
    "\\text{Loss function} = \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 + \\lambda \\left( \\alpha \\sum_{j=1}^n |\\beta_j| + \\frac{1-\\alpha}{2} \\sum_{j=1}^n \\beta_j^2 \\right)\n",
    "$$\n",
    "\n",
    "By adjusting **$\\alpha$**, you can control the balance between **L1** and **L2** regularization, making Elastic Net an effective tool for both feature selection and handling multicollinearity in regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f42a6",
   "metadata": {
    "papermill": {
     "duration": 0.004206,
     "end_time": "2025-12-17T12:20:17.686716",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.682510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Classification\n",
    "\n",
    "Classification is a type of supervised learning where the goal is to predict the categorical label (or class) of an input based on its features. It involves assigning input data to one of several predefined categories or classes. Unlike regression, where the output is continuous, classification models output a discrete label.\n",
    "\n",
    "Binary classification refers to tasks where the output variable has two possible outcomes, often represented as 0 and 1. These models aim to predict which of the two classes an instance belongs to based on input features.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Logistic regression is a popular algorithm for binary classification tasks, where the goal is to predict whether an instance belongs to one of two classes. For example, it can be used to predict whether a customer will buy a product (class 1) or not (class 0), or whether an email is spam (class 1) or not (class 0).\n",
    "\n",
    "Unlike linear regression, which predicts continuous numerical values, logistic regression predicts probabilities. It outputs a probability that the instance belongs to class 1, with values between 0 and 1. This probability can then be thresholded (usually at 0.5) to classify the instance as either class 1 or class 0.\n",
    "\n",
    "At its core, logistic regression is based on the **logistic function** (or **sigmoid function**), which transforms the output of a linear equation into a probability. The logistic function is an S-shaped curve that maps any real-valued number to a value between 0 and 1. This is crucial because we want the model's output to represent the probability of an instance belonging to class 1.\n",
    "\n",
    "The mathematical formulation of logistic regression is:\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$\\hat{p}$** is the predicted probability that the instance belongs to class 1.\n",
    "- **$x_1, x_2, \\dots, x_n$** are the input features (independent variables).\n",
    "- **$\\beta_0$** is the intercept term, and **$\\beta_1, \\dots, \\beta_n$** are the coefficients (weights) for each feature.\n",
    "- **$e$** is the base of the natural logarithm (approximately 2.718).\n",
    "\n",
    "In this equation, the term **$(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)$** is a linear combination of the input features, similar to linear regression. This part of the model computes the log-odds of the event occurring, which is often called the **logit**. The logistic (sigmoid) function then maps the log-odds to a probability between 0 and 1.\n",
    "\n",
    "The key difference between logistic regression and linear regression lies in the transformation applied to the output. In linear regression, the model directly predicts a continuous value (without restrictions), whereas logistic regression uses the sigmoid function to compress the prediction to the [0, 1] range, making it suitable for classification problems.\n",
    "\n",
    "The output of logistic regression, **$\\hat{p}$**, is interpreted as the probability that the instance belongs to class 1. For example, if the model predicts **$\\hat{p} = 0.8$**, it means there is an 80% probability that the instance belongs to class 1, and a 20% probability that it belongs to class 0. By applying a decision threshold (commonly 0.5), we can classify the instance as either class 1 (if **$\\hat{p} \\geq 0.5$**) or class 0 (if **$\\hat{p} < 0.5$**).\n",
    "\n",
    "The coefficients **$\\beta_1, \\dots, \\beta_n$** in logistic regression are interpreted similarly to linear regression, as the change in the log-odds of the outcome for a one-unit increase in the corresponding feature, holding other features constant. For example, if **$\\beta_1 = 0.5$**, a one-unit increase in **$x_1$** will increase the log-odds of the outcome by 0.5, thus increasing the probability of class 1.\n",
    "\n",
    "One important characteristic of logistic regression is that, although it models probabilities, it can be extended to more complex scenarios. For instance, it can be adapted for **multiclass classification** tasks using methods like **One-vs-Rest** or **Softmax** for situations where there are more than two possible outcomes.\n",
    "\n",
    "The **decision boundary** is the threshold that separates the two classes (class 0 and class 1) based on the predicted probability. In logistic regression, the decision boundary corresponds to the point where the model predicts a 50% probability of class 1 (i.e., **$\\hat{p} = 0.5$**).\n",
    "\n",
    "The decision boundary is determined by setting the logistic regression equation equal to 0.5:\n",
    "\n",
    "$$\n",
    "\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}} = 0.5\n",
    "$$\n",
    "\n",
    "Solving this equation for the log-odds:\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n = 0\n",
    "$$\n",
    "\n",
    "This equation defines a hyperplane in the feature space, which separates the instances predicted to belong to class 1 (on one side of the boundary) from those predicted to belong to class 0 (on the other side). In two-dimensional space (two features), this boundary would be a straight line.\n",
    "\n",
    "By adjusting the threshold (e.g., using a value other than 0.5), the decision boundary can shift, which is particularly useful for tuning the model when handling imbalanced datasets or when you want to adjust the sensitivity of the model to one class over the other.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Multiclass Classification\n",
    "\n",
    "Multiclass classification involves tasks where the output variable has more than two classes. Several methods extend binary classification techniques to handle multiple classes.\n",
    "\n",
    "**One-vs-Rest**\n",
    "\n",
    "The One-vs-Rest (OvR) approach, also known as One-vs-All (OvA), is a strategy used to extend binary classification algorithms to multiclass problems. In this approach, a separate binary classifier is trained for each class in the dataset. \n",
    "\n",
    "For each classifier, the current class is treated as the positive class, and all other classes are grouped together as the negative class. This results in multiple binary classifiers, one for each class. After all classifiers are trained, the model makes a prediction by evaluating all classifiers on the test data and selecting the class with the highest confidence score.\n",
    "\n",
    "During prediction, each classifier will output a confidence score, and the class with the highest score will be chosen as the final prediction. \n",
    "\n",
    "This method is simple and works well when the classes are separable. However, it can become inefficient if there are many classes, as it requires training a separate classifier for each class. Additionally, since each classifier is independent, the final predictions may not take into account the relationships between the different classes.\n",
    "\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "Softmax is a function used to extend logistic regression to handle multiclass classification problems, where there are more than two possible outcomes. Unlike binary classification, where logistic regression assigns a probability to one of two classes, softmax is designed to compute the probability distribution over multiple classes.\n",
    "\n",
    "The softmax function works by first calculating a score for each class. These scores are essentially the output of the model's linear equations for each class, which are then transformed into probabilities. The probability for each class is proportional to the exponential of its score, and the sum of all class probabilities is normalized to 1, ensuring that they form a valid probability distribution.\n",
    "\n",
    "Mathematically, for a given class **$i$**, the probability of the instance belonging to that class is calculated as:\n",
    "\n",
    "$$\n",
    "P(y = i) = \\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}\n",
    "$$\n",
    "\n",
    "Here, **$z_i$** represents the raw score (or logit) for class $i$, which is the output of the model before applying softmax. The denominator is the sum of the exponentiated scores for all classes, $k$ being the total number of classes.\n",
    "\n",
    "What softmax essentially does is take the raw model outputs (which can be any real number) and squashes them into a range between 0 and 1, such that all probabilities sum to 1. This transformation makes it possible to interpret the outputs as probabilities, with each class having a score that reflects how likely the instance belongs to that class relative to the others.\n",
    "\n",
    "Softmax is commonly used in multiclass classification tasks, such as image classification or multi-category text classification, where the model needs to select one class out of many possible classes based on the input features. The class with the highest probability is typically selected as the predicted class.\n",
    "\n",
    "\n",
    "**Multinomial Logistic Regression**\n",
    "\n",
    "Multinomial logistic regression is an extension of logistic regression that handles multiclass classification problems. While logistic regression is designed for binary classification (two classes), multinomial logistic regression generalizes this to cases where there are more than two possible outcomes.\n",
    "\n",
    "Instead of predicting a single probability for one class (as in binary logistic regression), multinomial logistic regression predicts a probability for each possible class. The model works by comparing the probabilities of each class to a reference class, typically the first class. This is done using the **softmax function**, which is an extension of the sigmoid function used in binary classification.\n",
    "\n",
    "The softmax function transforms the outputs of the linear equations for each class into probabilities that sum to 1. Given a set of classes, the model calculates the unnormalized score (logits) for each class, and then applies the softmax to convert these logits into probabilities. The formula for the softmax function is:\n",
    "\n",
    "$$\n",
    "P(y = k | x) = \\frac{e^{\\beta_k x}}{\\sum_{j=1}^{K} e^{\\beta_j x}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **P(y = k | x)** is the predicted probability that the instance belongs to class **k** given the input **x**.\n",
    "- **$\\beta_k$** represents the model parameters for class **k**.\n",
    "- **K** is the total number of classes.\n",
    "\n",
    "The output of this function is a vector of probabilities, one for each class, which can be interpreted as the likelihood of each class given the input data. The class with the highest probability is typically chosen as the predicted class.\n",
    "\n",
    "Multinomial logistic regression can be used in various applications, such as classifying images into multiple categories (e.g., identifying different objects in a picture), or predicting the brand preference of customers from a set of options.\n",
    "\n",
    "---\n",
    "\n",
    "### K-Nearest Neighbors (KNN)\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple, instance-based learning algorithm used for both classification and regression tasks. The idea behind KNN is to make predictions based on the proximity to the \"K\" nearest neighbors in the feature space.\n",
    "\n",
    "For classification, the algorithm works by finding the \"K\" training samples that are closest to the query point (using a distance metric, typically Euclidean distance) and assigning the class label that is most common among those neighbors. For regression, the predicted value is usually the average of the values of the nearest neighbors.\n",
    "\n",
    "The algorithm doesn't build an explicit model; instead, it stores the entire training dataset. When a new instance needs to be classified or predicted, it searches for the nearest neighbors and uses their labels or values to make the decision. \n",
    "\n",
    "The key parameters in KNN are:\n",
    "- **K**: the number of nearest neighbors to consider. A small value of K might be sensitive to noise, while a large value might smooth out the decision boundary too much.\n",
    "- **Distance metric**: the function used to measure the distance between points. Commonly, Euclidean distance is used, but other metrics like Manhattan or Minkowski can also be employed.\n",
    "- **Weighting**: sometimes, neighbors are weighted differently, with closer neighbors having more influence on the prediction.\n",
    "\n",
    "One advantage of KNN is its simplicity and effectiveness, especially in low-dimensional spaces. However, it can become computationally expensive as the dataset grows because the algorithm requires checking every training instance to compute distances. KNN is also sensitive to the scale of the data, so feature scaling (such as normalization or standardization) is often essential before applying the algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### Support Vector Machine (SVM)\n",
    "\n",
    "Support Vector Machine (SVM) is a powerful supervised learning algorithm used for both classification and regression tasks, though it is most commonly used for classification. The goal of SVM is to find the optimal decision boundary (also called a hyperplane) that best separates the classes in the feature space.\n",
    "\n",
    "For a binary classification problem, SVM attempts to find a hyperplane that maximizes the margin between the two classes. The margin is defined as the distance between the hyperplane and the closest data points from each class, which are known as **support vectors**. These support vectors are the critical elements of the dataset that influence the position of the decision boundary.\n",
    "\n",
    "Mathematically, SVM aims to solve the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\text{maximize } \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "where \\( w \\) is the weight vector that defines the hyperplane. The larger the margin, the better the classifier generalizes to unseen data.\n",
    "\n",
    "One of the strengths of SVM is its ability to handle non-linearly separable data through the use of **kernel functions**. Kernels map the data into a higher-dimensional feature space where a linear separation might be possible. Common kernel functions include:\n",
    "- **Linear kernel**: No transformation, used for linearly separable data.\n",
    "- **Polynomial kernel**: Maps data to a higher-dimensional space using polynomial functions.\n",
    "- **Radial Basis Function (RBF) kernel**: A popular choice that uses the Gaussian function to project data into an infinite-dimensional space.\n",
    "\n",
    "The SVM decision boundary is controlled by two main parameters:\n",
    "- **C**: the regularization parameter that determines the trade-off between maximizing the margin and minimizing classification errors. A larger value of C leads to fewer misclassifications but can cause overfitting.\n",
    "- **Kernel**: the function used to transform the data into higher-dimensional space. The choice of kernel affects how the decision boundary is learned.\n",
    "\n",
    "SVM is effective in high-dimensional spaces and is versatile because of its ability to handle non-linear relationships through kernels. However, it can be computationally expensive for large datasets, especially with complex kernel functions. Additionally, SVM requires careful tuning of hyperparameters (C and kernel choice) for optimal performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "**Naive Bayes** is a simple and efficient classification algorithm based on Bayes' Theorem, with the \"naive\" assumption that the features are conditionally independent given the class label. Despite this simplifying assumption, Naive Bayes often performs well in practice, especially with high-dimensional datasets.\n",
    "\n",
    "The core idea behind Naive Bayes is to predict the probability that a given instance belongs to a particular class, based on the likelihood of the features given that class, and the prior probability of the class.\n",
    "\n",
    "**Bayes' Theorem** provides a way to update the probability estimate for a hypothesis (in this case, a class label) based on new evidence (the features). The formula for Bayes' Theorem is:\n",
    "\n",
    "$$\n",
    "P(C_k | X) = \\frac{P(X | C_k) P(C_k)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$P(C_k | X)$** is the posterior probability of class **$C_k$** given the features **X**.\n",
    "- **$P(X | C_k)$** is the likelihood, or the probability of observing the features **X** given the class **$C_k$**.\n",
    "- **$P(C_k)$** is the prior probability of class **$C_k$**.\n",
    "- **$P(X)$** is the evidence, or the probability of observing the features **X** (often ignored during classification because it is constant for all classes).\n",
    "\n",
    "The \"naive\" assumption is that the features are conditionally independent given the class label. This means that:\n",
    "\n",
    "$$\n",
    "P(X | C_k) = \\prod_{i=1}^n P(x_i | C_k)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$x_i$** are the individual features.\n",
    "- **$P(x_i | C_k)$** is the probability of observing feature **$x_i$** given the class **$C_k$**.\n",
    "\n",
    "There are several variations of Naive Bayes, depending on the type of data and distribution assumptions:\n",
    "\n",
    "- **Gaussian Naive Bayes**: Assumes that the features are normally distributed (Gaussian distribution) for each class. This is useful for continuous data.\n",
    "- **Multinomial Naive Bayes**: Assumes that the features are drawn from a multinomial distribution, typically used for discrete data such as word counts in text classification.\n",
    "- **Bernoulli Naive Bayes**: Assumes binary features (0 or 1), often used for binary classification problems like spam detection.\n",
    "\n",
    "Naive Bayes is computationally efficient and easy to implement, especially for high-dimensional datasets. Despite the strong independence assumption, it can perform surprisingly well even when the assumption is violated. It is particularly effective for text classification tasks, where features are often words or tokens that are treated as conditionally independent. Additionally, it handles both categorical and numerical data, depending on the variant used (such as Gaussian Naive Bayes for continuous data).\n",
    "\n",
    "However, the strong independence assumption can lead to poor performance when the features are highly correlated. Naive Bayes may struggle to capture complex relationships between features, as it only looks at individual feature distributions. The model can also be biased towards the dominant class in highly imbalanced datasets, leading to poor performance on minority classes.\n",
    "\n",
    "\n",
    "Naive Bayes is commonly used in text classification tasks, such as:\n",
    "- **Spam filtering**: Classifying emails as spam or not spam based on the content.\n",
    "- **Sentiment analysis**: Classifying the sentiment of text data (positive, negative, neutral).\n",
    "- **Document categorization**: Assigning documents to predefined categories.\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "A **Decision Tree** is a popular supervised machine learning algorithm used for both classification and regression tasks. It builds a model that makes decisions based on a series of simple, binary rules (questions) derived from the input features. The tree-like structure consists of **nodes** (questions or decisions) and **branches** (outcomes of the questions).\n",
    "\n",
    "A decision tree works by splitting the data into subsets based on feature values, aiming to reduce the impurity or uncertainty about the target variable. It recursively divides the data set into smaller subsets by asking a series of binary questions. This continues until each leaf node contains data that all belongs to the same class or has minimal variance (in case of regression).\n",
    "\n",
    "1. **Root Node**: The topmost node that represents the entire dataset. It is split based on the feature that provides the best separation of data.\n",
    "2. **Decision Nodes**: These nodes represent features and corresponding split points.\n",
    "3. **Leaf Nodes**: Terminal nodes that provide the final output (class label for classification or predicted value for regression).\n",
    "\n",
    "### Splitting Criteria\n",
    "\n",
    "The goal at each decision node is to find the best feature and the best split point that separates the data most effectively. This is done by evaluating a splitting criterion, such as **Gini Impurity**, **Entropy**, or **Variance Reduction**.\n",
    "\n",
    "**Gini Impurity** measures the \"impurity\" of a node in terms of the distribution of class labels. The Gini index for a node is calculated as:\n",
    "\n",
    "$$\n",
    "Gini(t) = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$p_i$** is the proportion of instances belonging to class $i$ at node $t$.\n",
    "- **$k$** is the number of unique classes.\n",
    "\n",
    "The algorithm will aim to minimize the Gini index when making splits. A Gini index of 0 indicates perfect purity (all instances in the node belong to the same class).\n",
    "\n",
    "**Entropy** measures the uncertainty or randomness of a system. For classification, the entropy of a node is calculated as:\n",
    "\n",
    "$$\n",
    "Entropy(t) = - \\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$p_i$** is the proportion of instances belonging to class $i$.\n",
    "\n",
    "**Information gain** is the reduction in entropy when a dataset is split based on a certain feature. The goal of the decision tree algorithm is to maximize information gain by finding the feature that minimizes entropy after the split.\n",
    "\n",
    "For regression problems, the splitting criterion is typically based on reducing the variance within each split. The **variance reduction** measures how much the variance of the target variable decreases when the dataset is split based on a certain feature. The decision tree algorithm will aim to choose the split that minimizes the variance within each child node.\n",
    "\n",
    "### Building a Decision Tree\n",
    "\n",
    "1. **Selecting the Root Node**: The root node is the feature that best splits the data. It is chosen based on the splitting criterion (e.g., minimizing Gini impurity or maximizing information gain).\n",
    "2. **Recursively Split**: For each child node, the process is repeated, selecting the best feature and split point that further reduces impurity or variance. This continues until stopping conditions are met.\n",
    "3. **Stopping Criteria**:\n",
    "   - **Maximum Depth**: A limit on the number of levels (depth) of the tree.\n",
    "   - **Minimum Samples per Leaf**: A threshold on the minimum number of samples required to create a split at a node.\n",
    "   - **Maximum Number of Leaf Nodes**: A limit on the number of leaf nodes in the tree.\n",
    "   - **Purity Threshold**: When a node's impurity is below a certain threshold, no further splitting occurs.\n",
    "\n",
    "### Overfitting and Pruning\n",
    "\n",
    "Decision trees are prone to **overfitting** if they grow too deep, capturing noise or irrelevant patterns in the training data. To prevent this, **pruning** is applied to remove unnecessary nodes or branches that do not contribute to the model's performance.\n",
    "\n",
    "There are two main types of pruning:\n",
    "\n",
    "1. **Pre-pruning (early stopping)**: This involves stopping the tree construction process before it becomes too complex. It can be done by setting parameters like maximum depth, minimum samples per leaf, or the maximum number of splits.\n",
    "2. **Post-pruning**: This involves growing the tree fully and then removing branches that add little predictive power. Post-pruning can be done using techniques like **cost-complexity pruning**, which removes branches based on a trade-off between complexity and accuracy.\n",
    "\n",
    "### Advantages of Decision Trees\n",
    "\n",
    "- **Interpretability**: Decision trees are easy to interpret and visualize. The model can be represented as a tree structure, making it clear how decisions are being made.\n",
    "- **Non-linear Relationships**: Unlike linear models, decision trees can model non-linear relationships between features and the target variable.\n",
    "- **Handles Both Numerical and Categorical Data**: Decision trees can handle both types of data without the need for scaling or encoding.\n",
    "- **No Need for Feature Engineering**: Decision trees automatically perform feature selection during the training process, identifying the most relevant features for splits.\n",
    "\n",
    "### Disadvantages of Decision Trees\n",
    "\n",
    "- **Overfitting**: Decision trees can easily overfit the training data, especially when they are too deep, capturing noise or irrelevant patterns.\n",
    "- **Instability**: Small changes in the data can lead to large changes in the tree structure, making decision trees sensitive to variations in the dataset.\n",
    "- **Bias towards Features with More Levels**: Decision trees tend to favor features with many levels (e.g., categorical features with many categories), as they can create many splits.\n",
    "- **Limited Performance on Complex Relationships**: Decision trees might struggle with tasks where the relationships between features are highly complex or require sophisticated patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Bagging\n",
    "\n",
    "**Bagging**, which stands for **Bootstrap Aggregating**, is an ensemble learning technique designed to improve the accuracy of machine learning models. It works by combining multiple models to create a more robust and stable prediction, reducing variance and helping to prevent overfitting.\n",
    "\n",
    "Bagging can be applied to many different base models, but it is particularly popular with models that are prone to overfitting, such as **decision trees**. The idea is to train multiple models independently on different subsets of the data and combine their predictions, typically by averaging (for regression) or by voting (for classification). \n",
    "\n",
    "### How Bagging Works\n",
    "\n",
    "The process of bagging involves the following steps:\n",
    "\n",
    "1. **Bootstrap Sampling**: Randomly sample from the training data with replacement to create several subsets of the original dataset. Each subset is used to train a separate base model. Since the sampling is done with replacement, some instances may appear more than once in a subset, while others may be left out.\n",
    "   \n",
    "2. **Model Training**: Train a base model (e.g., decision tree) independently on each subset of the data. Each model is trained on a slightly different view of the data, which helps introduce diversity in the ensemble.\n",
    "\n",
    "3. **Aggregation**: Once all models are trained, their predictions are combined to produce a final output. For classification tasks, a **majority vote** is used to determine the predicted class. For regression tasks, the predictions are **averaged**.\n",
    "\n",
    "The strength of bagging lies in the diversity of the models in the ensemble, which helps to reduce the model's overall variance, leading to better generalization on unseen data.\n",
    "\n",
    "### Bagging Algorithm\n",
    "\n",
    "Here’s a step-by-step overview of how bagging works:\n",
    "\n",
    "1. **Bootstrap Sampling**: \n",
    "   - From the original training dataset, create multiple bootstrap samples (subsets of the data).\n",
    "   - Each subset is created by randomly selecting data points with replacement.\n",
    "\n",
    "2. **Train Models**: \n",
    "   - Train an independent model (typically of the same type, like decision trees) on each bootstrap sample.\n",
    "   - The models may overfit the data due to high variance, but because each model is trained on slightly different data, the overfitting can be reduced when the models are aggregated.\n",
    "\n",
    "3. **Prediction Aggregation**:\n",
    "   - For classification, take the **mode** (majority vote) of the predictions across all models.\n",
    "   - For regression, take the **mean** of the predicted values across all models.\n",
    "\n",
    "\n",
    "\n",
    "### Advantages of Bagging\n",
    "\n",
    "- **Reduces Variance**: Bagging helps to reduce the variance of models, making it less likely that the model will overfit. By aggregating multiple models, the predictions become more stable and less sensitive to the noise in the data.\n",
    "  \n",
    "- **Improves Model Robustness**: By training multiple models on different subsets of the data, bagging reduces the impact of outliers and noise, resulting in a more robust and accurate prediction.\n",
    "\n",
    "- **Parallelizable**: Since the models are trained independently, bagging algorithms are highly parallelizable, making them suitable for implementation on multiple processors or distributed computing systems.\n",
    "\n",
    "- **Works Well with High-Variance Models**: Bagging is particularly effective when applied to models with high variance, such as decision trees, which are prone to overfitting. By averaging over multiple trees, bagging stabilizes the predictions.\n",
    "\n",
    "### Disadvantages of Bagging\n",
    "\n",
    "- **Increased Computational Cost**: Since bagging involves training multiple models independently, it can be computationally expensive, especially with large datasets or complex base models.\n",
    "\n",
    "- **Lack of Interpretability**: The ensemble nature of bagging makes the model less interpretable compared to individual models, such as a single decision tree.\n",
    "\n",
    "- **Less Effective for Low-Variance Models**: Bagging is most effective when applied to high-variance models. If the base model already has low variance (e.g., linear regression), bagging may not provide a significant improvement.\n",
    "\n",
    "### Applications of Bagging\n",
    "\n",
    "- **Classification Problems**: Bagging is widely used in classification tasks where the goal is to predict a categorical label. For example, bagging can be used for **spam classification** or **image recognition**.\n",
    "\n",
    "- **Regression Problems**: In regression, bagging can help to improve the accuracy of models like decision trees by reducing variance and preventing overfitting. It’s particularly useful in tasks like **predicting house prices** or **forecasting sales**.\n",
    "\n",
    "- **Random Forests**: The Random Forest algorithm is one of the most popular and successful examples of bagging. It is highly effective for both classification and regression tasks and is often the go-to algorithm for many machine learning problems.\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method based on the concept of **bagging** (Bootstrap Aggregating), which is used to improve the accuracy and stability of machine learning models. It combines multiple decision trees to create a robust and accurate model, where each tree is trained on a different random subset of the data and its final prediction is made by aggregating the predictions of all trees.\n",
    "\n",
    "The fundamental idea behind Random Forest is that, by combining many decision trees, the model reduces the risk of overfitting, which is common in individual decision trees, while maintaining the model's ability to capture complex patterns in the data. Unlike a single decision tree, which can be very sensitive to the data it is trained on, Random Forest averages out the noise and variance across its ensemble of trees, resulting in more stable and generalized predictions.\n",
    "\n",
    "In Random Forest, each decision tree is trained on a **bootstrap sample** (a random subset of the original dataset, drawn with replacement), meaning each tree sees a slightly different version of the data. Additionally, when splitting a node in a tree, only a random subset of the features is considered, which further promotes diversity among the trees and prevents them from becoming too similar to each other.\n",
    "\n",
    "Once all trees are trained, the predictions are aggregated:\n",
    "- **For classification tasks**, the output of each tree is a class prediction, and the final output is determined by majority voting — the class predicted by the most trees is the final class.\n",
    "- **For regression tasks**, the prediction of each tree is averaged to produce the final output.\n",
    "\n",
    "This combination of multiple, diverse trees leads to a model that is less likely to overfit and can generalize better to unseen data. The randomness introduced by both bootstrapping the data and selecting a random subset of features at each split helps to ensure that the trees in the forest are uncorrelated, thus reducing the overall model variance.\n",
    "\n",
    "One of the key strengths of Random Forest is its ability to handle a large number of features without requiring extensive tuning or feature selection. It can also handle both **numerical** and **categorical** data, making it versatile for many different types of datasets.\n",
    "\n",
    "In addition, Random Forest provides useful information about feature importance. It can be used to identify which features are the most influential in making predictions. This can be helpful for understanding the underlying patterns in the data, as well as for feature selection in other models.\n",
    "\n",
    "However, Random Forest models can be computationally expensive and slow to train, particularly with large datasets and a large number of trees. Moreover, the ensemble nature of the model makes it less interpretable compared to a single decision tree, which can be easily visualized and understood.\n",
    "\n",
    "Random Forest is widely used for both **classification** and **regression** tasks across various domains, including:\n",
    "- **Classification tasks**: such as **spam detection**, **customer segmentation**, and **image classification**.\n",
    "- **Regression tasks**: such as **predicting house prices**, **sales forecasting**, and **medical diagnosis**.\n",
    "\n",
    "In practice, Random Forest is often a go-to algorithm because of its accuracy, ease of use, and ability to work well with minimal parameter tuning. It is robust to overfitting, even with a large number of trees, and performs well with both small and large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting is an ensemble learning technique that aims to improve the performance of a weak learner by combining multiple models to create a strong learner. Unlike bagging, which builds multiple independent models and combines their predictions, boosting builds models sequentially. Each new model is trained to correct the errors made by the previous model, giving more weight to the misclassified instances.\n",
    "\n",
    "### Key Concepts of Boosting\n",
    "\n",
    "- **Sequential Learning**: In boosting, models are trained sequentially, with each new model focusing on the mistakes made by the previous ones. This means that the training process is dependent on the results of the previous iterations, as opposed to bagging, where models are trained independently.\n",
    "  \n",
    "- **Weight Adjustment**: Boosting adjusts the weights of the data points during training. The algorithm gives higher weights to misclassified instances, making them more important for the next model in the sequence. This allows boosting to focus on hard-to-classify instances and improve overall accuracy.\n",
    "\n",
    "- **Weak Learners**: The base models used in boosting are usually weak learners, which are models that perform slightly better than random guessing. The goal is to combine many weak learners to create a strong model. In practice, decision trees with a shallow depth (such as stumps or small trees) are commonly used as weak learners.\n",
    "\n",
    "- **Model Combination**: After the sequential training of multiple models, boosting combines their predictions through a weighted sum or voting. In the case of regression, the predictions of individual models are averaged, while in classification, the final class prediction is often made based on a weighted majority vote of all models.\n",
    "\n",
    "### Advantages of Boosting\n",
    "\n",
    "- **Improved Accuracy**: Boosting generally leads to better predictive accuracy compared to individual models, as it focuses on correcting the errors made by previous models.\n",
    "  \n",
    "- **Handles Complex Data**: Boosting can capture complex patterns in the data, making it effective for a wide range of problems, including both classification and regression tasks.\n",
    "\n",
    "- **Low Bias**: Since boosting corrects errors iteratively, it can significantly reduce the bias of a weak learner and often results in better performance than single models.\n",
    "\n",
    "### Disadvantages of Boosting\n",
    "\n",
    "- **Overfitting**: While boosting can reduce bias, it is prone to overfitting if the model is allowed to train for too many iterations, especially with noisy data. Regularization techniques, such as early stopping or pruning, are often used to prevent this.\n",
    "\n",
    "- **Computationally Expensive**: Boosting is computationally intensive since models are trained sequentially. This can lead to longer training times compared to parallelized methods like bagging.\n",
    "\n",
    "- **Sensitivity to Noisy Data**: Boosting can be sensitive to noisy or outlier data, as it tends to focus on correcting misclassified instances, which may be outliers in the dataset. Proper data preprocessing is essential to mitigate this risk.\n",
    "\n",
    "Boosting has become a popular approach in machine learning due to its ability to produce highly accurate models, but it requires careful tuning to avoid overfitting and to maximize its effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Boosting Machine (GBM)\n",
    "\n",
    "Gradient Boosting Machine (GBM) is a powerful ensemble learning algorithm that combines multiple weak learners (typically decision trees) to create a strong predictive model. It is a type of boosting algorithm, where each subsequent model corrects the errors made by the previous model.\n",
    "\n",
    "The key idea behind GBM is to iteratively add decision trees, each trained to predict the residual errors (or gradients) of the predictions made by the previous trees. This helps to reduce the bias of the model, and as more trees are added, the model becomes progressively more accurate.\n",
    "\n",
    "Key Concepts in GBM:\n",
    "\n",
    "- **Boosting**: GBM is a boosting algorithm, which means it combines multiple weak models (often decision trees) in a sequential manner, where each tree is trained to correct the mistakes of the previous ones. Unlike bagging methods, where trees are built independently, boosting involves learning from the residuals or errors of prior trees.\n",
    "  \n",
    "- **Residuals/Gradients**: In each iteration, GBM computes the residuals or errors (the difference between the true and predicted values) and fits a new decision tree to predict these residuals. This is called gradient boosting because it uses gradient descent to minimize the loss function by fitting the trees to the gradients of the error.\n",
    "\n",
    "- **Loss Function**: GBM optimizes a predefined loss function (such as mean squared error for regression tasks or log loss for classification tasks) during each iteration. It tries to minimize this loss by adding trees that reduce the overall error.\n",
    "\n",
    "- **Learning Rate**: GBM introduces a learning rate (often denoted as **η**) to scale the contribution of each tree added to the model. A lower learning rate generally requires more trees to fit the model, but it can improve generalization and prevent overfitting. The learning rate is a hyperparameter that can be tuned to balance training time and model performance.\n",
    "\n",
    "- **Shrinkage**: Shrinkage is a regularization technique used in GBM to prevent overfitting. It involves scaling the output of each tree by the learning rate. This makes each individual tree less influential, which can improve the generalization of the model.\n",
    "\n",
    "- **Early Stopping**: GBM models are prone to overfitting, especially with too many iterations. Early stopping is a technique where the model training process is stopped once the performance on a validation set no longer improves. This helps to avoid overfitting to the training data.\n",
    "\n",
    "Advantages of GBM:\n",
    "- **High Predictive Power**: GBM tends to provide strong performance in terms of predictive accuracy, especially in structured/tabular datasets.\n",
    "- **Flexibility**: It can be used for both regression and classification tasks, and it allows the optimization of different loss functions (e.g., regression loss, classification loss, etc.).\n",
    "- **Handles Different Types of Data**: GBM works well with both continuous and categorical features (though categorical features may need to be preprocessed, such as through one-hot encoding).\n",
    "- **Handles Complex Relationships**: GBM is capable of modeling complex non-linear relationships between features.\n",
    "\n",
    "Disadvantages of GBM:\n",
    "- **Training Time**: GBM can be computationally expensive, especially on large datasets, as it requires the sequential addition of many trees. It can be slower compared to some other algorithms, like random forests, due to the sequential nature of boosting.\n",
    "- **Overfitting**: GBM can be prone to overfitting if not tuned properly, especially when the number of trees is large or the learning rate is too high. Regularization techniques like shrinkage, learning rate tuning, and early stopping are crucial to prevent this.\n",
    "- **Sensitive to Noisy Data**: GBM can be sensitive to noisy data or outliers, as it tries to fit the residuals of the previous models. This means that if the data has noise or outliers, it may lead to poor generalization.\n",
    "\n",
    "GBM is often used as a baseline for many machine learning tasks and is widely used in practice for structured data tasks. However, due to its sequential nature, it can be slower than some other methods and may require careful tuning to avoid overfitting.\n",
    "\n",
    "It is one of the foundational algorithms for ensemble learning and is the basis for more advanced algorithms like **XGBoost**, **LightGBM**, and **CatBoost**, which introduce optimizations to make the training process faster and more efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is one of the most popular and powerful algorithms for supervised learning tasks, particularly for structured/tabular data. It is an optimized and efficient implementation of gradient boosting, designed to be highly scalable and capable of handling large datasets with high performance. XGBoost has become a go-to algorithm in many machine learning competitions due to its accuracy, speed, and flexibility.\n",
    "\n",
    "The algorithm is based on gradient boosting, which combines the predictions of several weak learners (usually decision trees) to create a strong learner. Each tree is trained to correct the errors made by the previous tree, and the final prediction is made by aggregating the outputs of all the trees.\n",
    "\n",
    "Some key features of XGBoost include:\n",
    "\n",
    "- **Gradient Boosting Framework**: Like other boosting algorithms, XGBoost builds trees sequentially, where each tree corrects the errors made by the previous one. The goal is to minimize a loss function, often the mean squared error (MSE) for regression or log loss for classification.\n",
    "\n",
    "- **Regularization**: XGBoost includes L1 (Lasso) and L2 (Ridge) regularization terms in the objective function, which helps to control model complexity, reduce overfitting, and improve generalization.\n",
    "\n",
    "- **Handling Missing Data**: XGBoost has built-in capabilities to handle missing values during training, so you don’t need to explicitly impute missing data beforehand.\n",
    "\n",
    "- **Parallel and Distributed Computing**: XGBoost is highly optimized for speed, and it leverages parallel processing to speed up the training process. This makes it ideal for large datasets and distributed environments.\n",
    "\n",
    "- **Tree Pruning**: XGBoost uses a technique called \"max depth\" for tree pruning to optimize the growth of decision trees and avoid overfitting. It uses a depth-first approach to grow trees, unlike other boosting algorithms that grow trees in a level-wise manner.\n",
    "\n",
    "- **Sparsity-Aware**: XGBoost has a built-in feature to handle sparse data, making it suitable for datasets with missing or sparse features.\n",
    "\n",
    "- **Early Stopping**: XGBoost supports early stopping during model training, where the training stops if the model performance on a validation set stops improving after a specified number of rounds.\n",
    "\n",
    "XGBoost is widely used in both regression and classification tasks, and it excels in handling imbalanced datasets, feature interactions, and large datasets. It is available in multiple programming languages, including Python, R, and Julia, and is supported by most major machine learning libraries. \n",
    "\n",
    "---\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is one of the earliest and most popular boosting algorithms. It is an ensemble learning method that combines the predictions of several weak learners to create a strong learner. The weak learners are typically decision trees with a shallow depth, also known as decision stumps.\n",
    "\n",
    "The key idea behind AdaBoost is to give more weight to the misclassified instances from the previous iteration so that the model focuses more on difficult-to-predict examples in subsequent rounds. It uses a weighted combination of weak classifiers to create a strong final classifier.\n",
    "\n",
    "How AdaBoost works:\n",
    "\n",
    "- **Initial Weights**: In the first iteration, all training instances are assigned equal weights.\n",
    "- **Training Weak Learners**: A weak classifier (usually a decision stump) is trained on the data. The classifier will perform poorly on some instances but better on others.\n",
    "- **Weight Adjustment**: After the first weak classifier is trained, the weights of the misclassified instances are increased. This encourages the next classifier to focus more on the misclassified instances. Instances that were classified correctly will have their weights reduced.\n",
    "- **Weighted Majority Voting**: Each weak classifier is assigned a weight based on its accuracy. The final prediction is made by combining the weighted predictions of all the weak classifiers. Misclassified instances will have more influence on the final result, as they are given higher weights.\n",
    "\n",
    "AdaBoost combines the weak learners in such a way that the final model becomes much more powerful than any individual weak learner. The final model is a weighted sum of the individual weak classifiers, where classifiers with better performance on the training data are given higher weights.\n",
    "\n",
    "Key Features of AdaBoost:\n",
    "- **Boosts weak classifiers**: AdaBoost converts weak learners into a strong classifier by focusing on the errors made by previous models.\n",
    "- **Robust to Overfitting**: While the model can overfit with noisy data, AdaBoost is generally less prone to overfitting compared to other machine learning algorithms, especially when combined with weak classifiers.\n",
    "- **Simple to implement**: AdaBoost is easy to implement and computationally less expensive than some other ensemble methods.\n",
    "- **Works well with binary classification**: AdaBoost is particularly effective for binary classification problems.\n",
    "- **Sensitive to noisy data and outliers**: Since AdaBoost increases the weights of misclassified instances, it can be sensitive to noisy data or outliers that might receive higher weights, potentially leading to overfitting.\n",
    "\n",
    "While AdaBoost is a powerful boosting technique, it has some limitations:\n",
    "- It can be sensitive to noisy data and outliers because it increases the weight of misclassified instances, which could be outliers.\n",
    "- It may struggle with datasets that have many irrelevant features or classes.\n",
    "\n",
    "Despite these limitations, AdaBoost is still a widely used algorithm, especially for binary classification tasks, and is highly effective for simple, interpretable models. It has been used successfully in applications such as face detection, text classification, and fraud detection.\n",
    "\n",
    "---\n",
    "\n",
    "### LightGBM\n",
    "\n",
    "LightGBM (Light Gradient Boosting Machine) is an efficient and scalable gradient boosting framework developed by Microsoft. It is designed to be faster and more memory-efficient than traditional gradient boosting methods, making it particularly useful for large datasets and high-dimensional problems.\n",
    "\n",
    "LightGBM is based on decision tree algorithms, and like other gradient boosting methods, it combines multiple weak learners (usually decision trees) to create a strong predictive model. However, LightGBM introduces several optimizations that set it apart from traditional gradient boosting algorithms.\n",
    "\n",
    "Key Features of LightGBM:\n",
    "\n",
    "- **Histogram-based Training**: Unlike traditional decision tree algorithms that split data continuously, LightGBM uses a histogram-based approach to bucket continuous feature values into discrete bins. This reduces the computational cost by reducing the number of comparisons needed for finding the best split.\n",
    "- **Leaf-wise Tree Growth**: While traditional boosting methods grow trees level by level (i.e., breadth-first), LightGBM grows trees leaf-wise (i.e., depth-first). This allows it to find more optimal splits and results in a more accurate model. However, it can lead to overfitting if not tuned properly, especially for small datasets.\n",
    "- **Support for Categorical Features**: LightGBM can handle categorical features natively without the need for one-hot encoding. This helps save memory and reduces the complexity of the model.\n",
    "- **Efficiency**: LightGBM is designed to be highly efficient both in terms of computation and memory usage. It supports multi-threading and distributed learning, making it suitable for large-scale datasets. The use of histogram-based algorithms reduces memory consumption and speeds up the training process.\n",
    "- **Regularization**: LightGBM includes built-in support for regularization through parameters such as lambda and alpha, which help prevent overfitting. It also provides other hyperparameters to fine-tune the model for optimal performance.\n",
    "\n",
    "LightGBM’s strengths:\n",
    "- **Speed and Efficiency**: LightGBM is significantly faster than other gradient boosting methods like XGBoost, especially on large datasets.\n",
    "- **Scalability**: It is capable of handling very large datasets with billions of rows and high-dimensional feature spaces.\n",
    "- **Better Handling of Categorical Data**: Unlike other boosting methods, LightGBM can handle categorical features natively, which reduces the need for extensive pre-processing.\n",
    "- **High Accuracy**: The leaf-wise tree growth strategy can result in better accuracy compared to other gradient boosting implementations, especially for complex datasets.\n",
    "\n",
    "However, there are some challenges with LightGBM:\n",
    "- **Overfitting**: Due to the leaf-wise growth strategy, LightGBM can be prone to overfitting on smaller datasets if not tuned properly.\n",
    "- **Sensitive to Parameters**: Like other gradient boosting methods, LightGBM requires careful hyperparameter tuning. The default parameters may not work well for all types of data, and achieving optimal performance may require experimentation.\n",
    "\n",
    "LightGBM has become one of the most popular gradient boosting frameworks due to its speed, efficiency, and scalability. It is widely used in machine learning competitions (such as Kaggle) and in industrial applications where large-scale data is common.\n",
    "\n",
    "LightGBM is often preferred when dealing with very large datasets or when speed and memory efficiency are critical factors. However, like all machine learning models, it should be carefully tuned and validated to ensure that it performs well on the specific problem at hand.\n",
    "\n",
    "---\n",
    "\n",
    "### CatBoost (Categorical Boosting)\n",
    "\n",
    "CatBoost is a state-of-the-art gradient boosting algorithm developed by Yandex, designed specifically to handle categorical features efficiently. Unlike traditional gradient boosting algorithms, such as XGBoost and LightGBM, CatBoost incorporates special techniques to handle categorical data without requiring extensive preprocessing (like one-hot encoding), making it particularly effective for datasets with many categorical variables.\n",
    "\n",
    "CatBoost combines the strength of boosting with unique features that allow it to deal with categorical variables directly, making it one of the most powerful tools for structured data.\n",
    "\n",
    "Key Features and Concepts:\n",
    "\n",
    "- **Categorical Feature Handling**: One of the main advantages of CatBoost is its ability to handle categorical features natively. Unlike other gradient boosting models that require preprocessing (such as one-hot encoding), CatBoost transforms categorical features into numeric representations by using techniques like **ordered target encoding** and **count encoding**. This reduces memory usage and ensures that the model can handle large numbers of categorical features without compromising performance.\n",
    "\n",
    "- **Ordered Target Encoding**: In traditional target encoding, categorical variables are replaced with the mean of the target variable for each category. However, this method can introduce data leakage, leading to overfitting. CatBoost avoids this issue by using **ordered target encoding**, where the categories are encoded using information from earlier in the training process. This prevents future information from leaking into the encoding process, improving generalization.\n",
    "\n",
    "- **Gradient Boosting Algorithm**: Like other gradient boosting models, CatBoost builds an ensemble of decision trees in a sequential manner. In each iteration, a new tree is trained to predict the residual errors (or gradients) of the previous ensemble of trees. The model’s objective is to minimize a loss function through boosting. This iterative approach leads to a strong, predictive model by combining many weak learners (decision trees).\n",
    "\n",
    "- **Efficient Computation**: CatBoost optimizes the gradient boosting process by using advanced techniques such as **symmetric tree building** and **efficient computation of gradients**. This makes the algorithm faster and more memory-efficient, especially when handling large datasets. The implementation is highly optimized for parallelization and multi-core processing, resulting in faster training times compared to other gradient boosting frameworks.\n",
    "\n",
    "- **Handling Missing Values**: CatBoost can handle missing values automatically by treating them as a separate category. This is particularly useful when working with real-world data, where missing values are common. The algorithm determines the best way to split the data based on the missing value distribution, which improves the model's performance without needing imputation or preprocessing.\n",
    "\n",
    "- **Overfitting Control**: CatBoost has several built-in regularization techniques that help prevent overfitting. Some of these include:\n",
    "  - **Depth of Trees**: Controlling the depth of trees prevents the model from becoming too complex and overfitting the training data.\n",
    "  - **Learning Rate**: CatBoost allows for the use of a lower learning rate, which requires more trees but can improve generalization.\n",
    "  - **L2 Regularization**: This regularization term helps to reduce the complexity of the model and prevent overfitting.\n",
    "\n",
    "- **Fast and Robust**: Compared to other boosting algorithms, CatBoost has been shown to provide faster training times and better accuracy on a wide range of datasets. It is designed to work efficiently with large datasets and high-cardinality categorical features, making it a powerful tool in many machine learning tasks.\n",
    "\n",
    "Advantages of CatBoost:\n",
    "- **Native Support for Categorical Data**: CatBoost can handle categorical features without the need for manual preprocessing like one-hot encoding or label encoding.\n",
    "- **Efficient and Fast**: The algorithm is optimized for speed and memory usage, making it suitable for large-scale datasets.\n",
    "- **Improved Generalization**: The ordered target encoding and regularization techniques help improve generalization and prevent overfitting.\n",
    "- **High Predictive Power**: CatBoost often outperforms other gradient boosting models (such as XGBoost and LightGBM) on a wide variety of tasks, especially when categorical data is involved.\n",
    "\n",
    "Disadvantages of CatBoost:\n",
    "- **Longer Training Time on Small Datasets**: While CatBoost is efficient for large datasets, it can sometimes be slower than other algorithms (like XGBoost or LightGBM) on smaller datasets due to its more complex handling of categorical data.\n",
    "- **Memory Usage**: The model may require more memory, especially for datasets with high-cardinality categorical features, although this can be mitigated by adjusting hyperparameters.\n",
    "- **Less Intuitive Parameter Tuning**: Like other boosting models, CatBoost requires careful hyperparameter tuning for optimal performance, which can be time-consuming.\n",
    "\n",
    "Use Cases for CatBoost:\n",
    "- **Structured Data**: CatBoost excels in tasks with structured/tabular data, particularly when there are many categorical features. It has been widely used in fields such as:\n",
    "  - **Financial modeling**: Predicting credit risk or fraud detection.\n",
    "  - **Marketing**: Customer segmentation, recommendation systems.\n",
    "  - **E-commerce**: Product recommendation, sales forecasting.\n",
    "- **Competitions**: CatBoost has gained popularity in machine learning competitions (like Kaggle) due to its high accuracy and ease of use.\n",
    "\n",
    "---\n",
    "\n",
    "### Stochastic Gradient Boosting\n",
    "\n",
    "Stochastic Gradient Boosting is a variation of the traditional gradient boosting algorithm where randomness is introduced into the training process in order to improve model generalization and reduce overfitting. In contrast to the standard gradient boosting approach, which uses all available data to train each tree, stochastic gradient boosting trains each tree using a random subset of the data, thereby introducing a form of \"noise\" or randomness into the model.\n",
    "\n",
    "This randomness can improve the diversity of the trees in the ensemble, helping the model avoid overfitting to noise or patterns that are only present in the training data. The general idea behind stochastic gradient boosting is to make the model more robust by not relying too heavily on any single subset of the data.\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "- **Random Sampling of Data**: In stochastic gradient boosting, instead of using the entire dataset to build each decision tree, only a random subset of the data (often referred to as a \"mini-batch\") is used to build each tree. This is done by randomly sampling a fixed percentage of the training set (e.g., 80%) before fitting each new tree. This process introduces stochasticity into the model, which can prevent overfitting by reducing the model's reliance on particular subsets of the data.\n",
    "\n",
    "- **Bootstrap Sampling**: Like in bagging methods (e.g., Random Forest), stochastic gradient boosting can also use bootstrap sampling, where a random subset of the training data is sampled with replacement. This further diversifies the trees in the ensemble, improving the robustness and generalization of the model.\n",
    "\n",
    "- **Model Regularization**: The randomness introduced by stochastic gradient boosting acts as a regularizer, reducing the model's variance and helping it generalize better on unseen data. By building trees on different random subsets of the training data, the model reduces the chance of overfitting compared to using the entire dataset for each tree.\n",
    "\n",
    "- **Learning Rate and Subsampling Fraction**: In stochastic gradient boosting, the learning rate (or shrinkage) and subsample fraction (the proportion of data used for each tree) are key hyperparameters that influence the model's performance. A smaller learning rate can increase the number of iterations needed to converge, but it often leads to better generalization. Similarly, a smaller subsample fraction can lead to a more regularized model, though if set too low, it may impair the model's ability to learn from the data.\n",
    "\n",
    "Advantages:\n",
    "- **Reduced Overfitting**: By adding randomness to the training process, stochastic gradient boosting can reduce overfitting compared to traditional gradient boosting methods.\n",
    "- **Better Generalization**: The model's ability to generalize to new, unseen data is improved due to the diversity of trees trained on different subsets of data.\n",
    "- **Improved Performance on Noisy Data**: Stochastic gradient boosting is more robust when dealing with noisy datasets, as the randomness helps to prevent the model from fitting to noise or outliers.\n",
    "\n",
    "Disadvantages:\n",
    "- **Slower Training**: Stochastic gradient boosting can be slower to converge than traditional gradient boosting because more trees may be required to compensate for the randomness introduced.\n",
    "- **Hyperparameter Tuning**: The introduction of randomness introduces additional hyperparameters (e.g., subsample fraction) that need to be carefully tuned for optimal performance.\n",
    "- **Requires Careful Tuning**: As with all boosting algorithms, stochastic gradient boosting requires careful tuning of the learning rate, number of iterations, subsample fraction, and other parameters to avoid overfitting or underfitting.\n",
    "\n",
    "Applications:\n",
    "- **High-Variance Datasets**: Stochastic gradient boosting is especially useful in applications where the data is highly variable or noisy. The randomness introduced helps the model generalize better and avoid fitting to irrelevant noise.\n",
    "- **Kaggle Competitions**: Like other boosting algorithms, stochastic gradient boosting is widely used in machine learning competitions for tasks such as classification, regression, and ranking problems, where generalization is crucial.\n",
    "- **Financial Modeling**: It can be applied to high-dimensional datasets in finance, such as stock price prediction or risk modeling, where the model benefits from being more resistant to overfitting on noisy data.\n",
    "\n",
    "\n",
    "\n",
    "## Model Evaluation\n",
    "### Regression Metrics\n",
    "- MSE, RMSE, MAE\n",
    "- R² (Coefficient of determination)\n",
    "- Adjusted R²\n",
    "\n",
    "### Classification Metrics\n",
    "- Accuracy\n",
    "- Precision, Recall, F1-score\n",
    "- Confusion Matrix\n",
    "- ROC curve and AUC\n",
    "- Log-loss\n",
    "- Matthews Correlation Coefficient (MCC)\n",
    "- Cohen’s Kappa\n",
    "- Balanced accuracy\n",
    "- F-beta score\n",
    "\n",
    "### Cross-validation\n",
    "- K-Fold Cross Validation\n",
    "- Leave-One-Out Cross Validation\n",
    "- Stratified K-Fold\n",
    "\n",
    "## Advanced Techniques\n",
    "- Ensemble methods\n",
    "  - Bagging (Random Forest)\n",
    "  - Boosting (AdaBoost, Gradient Boosting)\n",
    "  - Stacking\n",
    "- Feature importance\n",
    "- Hyperparameter tuning\n",
    "  - Grid Search\n",
    "  - Random Search\n",
    "  - Bayesian Optimization\n",
    "- Learning curves and validation curves\n",
    "\n",
    "## Advanced Topics / Algorithms\n",
    "- Probabilistic models:\n",
    "  - Bayesian regression\n",
    "  - Gaussian Naive Bayes\n",
    "- Distance-based methods beyond KNN:\n",
    "  - Metric learning concepts\n",
    "- Tree-based advanced techniques:\n",
    "  - Extra Trees\n",
    "  - Gradient Boosting variants (CatBoost specifics)\n",
    "- Neural networks for tabular data (basic MLP)\n",
    "- Calibration of probabilistic classifiers\n",
    "\n",
    "## Practical Data Handling\n",
    "- Feature transformation:\n",
    "  - Log transformation, polynomial features\n",
    "  - Interaction terms\n",
    "- Categorical variable encoding advanced:\n",
    "  - Target encoding\n",
    "  - Frequency encoding\n",
    "- Handling missing data advanced:\n",
    "  - Imputation techniques (mean, median, k-NN, MICE)\n",
    "- Data leakage prevention\n",
    "- Pipeline automation (scikit-learn pipelines)\n",
    "\n",
    "## Evaluation / Metrics\n",
    "- Learning curves (training vs validation performance)\n",
    "- Validation curves (hyperparameter impact)\n",
    "- Bootstrapping and Monte Carlo evaluation\n",
    "- Nested Cross-validation\n",
    "- Confounding variables and collinearity\n",
    "- Concept drift handling\n",
    "\n",
    "## Practical Considerations\n",
    "- Imbalanced datasets\n",
    "  - Oversampling (SMOTE)\n",
    "  - Undersampling\n",
    "- Handling outliers\n",
    "- Model interpretability\n",
    "- Model deployment\n",
    "- Scalability and optimization\n",
    "- \n",
    "\n",
    "## Applications\n",
    "- Sales prediction (regression)\n",
    "- Image or text classification\n",
    "- Fraud detection\n",
    "- Churn prediction\n",
    "- Medical diagnosis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84efd7d7",
   "metadata": {
    "papermill": {
     "duration": 0.004297,
     "end_time": "2025-12-17T12:20:17.695350",
     "exception": false,
     "start_time": "2025-12-17T12:20:17.691053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "## Introduction to Unsupervised Learning\n",
    "- Definition of unsupervised learning\n",
    "- Difference between supervised, unsupervised, and reinforcement learning\n",
    "- Goal: find patterns, structures, or groupings in data\n",
    "- No target variable (y)\n",
    "- Common tasks:\n",
    "  - Clustering\n",
    "  - Dimensionality reduction\n",
    "  - Anomaly detection\n",
    "\n",
    "## Fundamental Concepts\n",
    "- Dataset and features\n",
    "- Distance and similarity measures\n",
    "  - Euclidean distance\n",
    "  - Manhattan distance\n",
    "  - Cosine similarity\n",
    "  - Correlation\n",
    "- Overfitting and underfitting in unsupervised learning\n",
    "- Evaluation challenges (lack of ground truth)\n",
    "\n",
    "## Data Preprocessing\n",
    "- Data cleaning\n",
    "- Handling missing values\n",
    "- Normalization and standardization\n",
    "- Encoding categorical variables\n",
    "- Feature scaling\n",
    "- Feature selection and feature engineering\n",
    "- Dimensionality reduction before clustering (optional)\n",
    "\n",
    "## Clustering\n",
    "### Partitioning Methods\n",
    "- K-Means\n",
    "  - Algorithm overview\n",
    "  - Choosing number of clusters (elbow method, silhouette score)\n",
    "  - Limitations: sensitive to initialization, outliers\n",
    "- K-Medoids / PAM\n",
    "- Mini-Batch K-Means\n",
    "\n",
    "### Hierarchical Clustering\n",
    "- Agglomerative clustering\n",
    "  - Linkage methods: single, complete, average, ward\n",
    "- Divisive clustering\n",
    "- Dendrogram visualization\n",
    "\n",
    "### Density-Based Clustering\n",
    "- DBSCAN\n",
    "- OPTICS\n",
    "- HDBSCAN\n",
    "\n",
    "### Model-Based Clustering\n",
    "- Gaussian Mixture Models (GMM)\n",
    "- Expectation-Maximization algorithm\n",
    "- Choosing number of components (BIC/AIC)\n",
    "\n",
    "### Other Clustering Techniques\n",
    "- Spectral Clustering\n",
    "- Self-Organizing Maps (SOM)\n",
    "- Mean-Shift clustering\n",
    "\n",
    "## Dimensionality Reduction\n",
    "- Principal Component Analysis (PCA)\n",
    "- Kernel PCA\n",
    "- Independent Component Analysis (ICA)\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- Uniform Manifold Approximation and Projection (UMAP)\n",
    "- Linear Discriminant Analysis (LDA, supervised variant)\n",
    "\n",
    "## Anomaly Detection\n",
    "- Z-score and statistical methods\n",
    "- Isolation Forest\n",
    "- One-Class SVM\n",
    "- Local Outlier Factor (LOF)\n",
    "- Autoencoder-based anomaly detection\n",
    "\n",
    "## Evaluation of Unsupervised Models\n",
    "- Internal metrics:\n",
    "  - Silhouette score\n",
    "  - Davies-Bouldin index\n",
    "  - Calinski-Harabasz index\n",
    "- External metrics (if ground truth available):\n",
    "  - Adjusted Rand Index (ARI)\n",
    "  - Normalized Mutual Information (NMI)\n",
    "  - Fowlkes-Mallows score\n",
    "- Visual inspection:\n",
    "  - Scatter plots, cluster plots\n",
    "  - Heatmaps\n",
    "\n",
    "## Advanced Techniques\n",
    "- Ensemble clustering\n",
    "- Consensus clustering\n",
    "- Subspace clustering\n",
    "- Feature learning with autoencoders\n",
    "- Self-supervised learning (modern approach)\n",
    "\n",
    "## Practical Considerations\n",
    "- Choosing the right number of clusters/components\n",
    "- Handling high-dimensional data\n",
    "- Handling categorical features\n",
    "- Handling outliers\n",
    "- Scaling for large datasets\n",
    "- Interpretability of clusters or latent features\n",
    "\n",
    "## Applications\n",
    "- Customer segmentation\n",
    "- Market basket analysis\n",
    "- Anomaly/fraud detection\n",
    "- Image compression or embedding\n",
    "- Topic modeling in text\n",
    "- Dimensionality reduction for visualization\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.089512,
   "end_time": "2025-12-17T12:20:18.018027",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-17T12:20:12.928515",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
